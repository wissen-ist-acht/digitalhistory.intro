[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Digital History",
    "section": "",
    "text": "Welcome\nDer vorliegende Guide, erstellt im Herbstsemester 2022, begleitet die Einführungskurse im Fach Geschichte an der Universität Basel und soll einen ersten Einblick in den Bereich Digital History geben. Es ist ein living document, das regelmäßig aktualisiert wird und dabei auch die epochen- und areaspezifischen Inhalte der verschiedenen Einführungskurse berücksichtigt, indem Verweise auf verschiedene Digital-History-Projekte aus unterschiedlichen Bereichen mit der Zeit in den Guide einfließen. Für die Teilnehmer:innen der Einführungskurse wird der Guide von einer Präsenzsitzung begleitet, bietet aber hoffentlich auch unabhängig davon einen Mehrwert. Für Kommentare, Anregungen oder Beschwerden freue ich mich über eine Nachricht.\nDer Guide ist in zwei Teile gegliedert: Die Kapitel 1–5 sollen eine erste Übersicht über Digital History bieten und den Blick auf Neuerungen und Veränderungen richten, die sich in den Geschichtswissenschaften aus der Nutzung digitaler Methoden ergeben. Der anschließende praktische Teil zeigt an einem konkreten Beispiel die Anwendung verschiedener Techniken auf, die sich (nicht nur) für Historiker:innen bei der Arbeit mit Quellenmaterial anbieten. Der Praxisteil verfolgt dabei zwei Ziele: Zum einen sollen Hemmungen bei der Arbeit mit dem Computer, die über die Nutzung als elektronische Schreibmaschine hinausgeht, abgebaut werden. Zum anderen soll ein grundlegendes Verständnis dafür hergestellt werden, welche Möglichkeiten computergestützte Analysen bieten und wie diese in der historischen Arbeit eingesetzt werden können.\nDie Übersicht soll möglichst knapp gehalten werden – es gibt zahlreiche ausführliche Grundlagenwerke, weswegen viele Themen nur kurz angeschnitten, dafür aber mit weiterführenden Verweisen versehen werden. Dasselbe gilt für den Praxisteil: Weiterreichende Anleitungen, Tutorials oder Onlinekurse werden an entsprechender Stelle verlinkt. Vollständigkeit wird an keiner Stelle beansprucht; Hinweise auf weitere Online-Angebote nehme ich gerne auf.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "01_intro.html#footnotes",
    "href": "01_intro.html#footnotes",
    "title": "1  Was ist Digital History?",
    "section": "",
    "text": "Blaney, Jonathan; Winters, Jane; Milligan, Sarah et al.: Doing digital history: A beginner’s guide to working with text as data, Manchester 2021 (IHR research guides), S. 6.↩︎\nEntgegen einer häufig zitierten Aussage von Emmanuel Le Roy Ladurie (*1929), der Historiker von morgen werde Programmierer sein, oder er werde nicht sein: “L’historien de demain sera programmeur ou il ne sera pas.” Le Roy Ladurie, Emmanuel: La fin des érudits, in: Le Nouvel Observateur, 08/1968.↩︎\nRomein, C. Annemieke; Kemman, Max; Birkholz, Julie M. et al.: State of the Field: Digital History, in: History 105 (365), 04/2020, pp. 291–312. Online: &lt;https://doi.org/10.1111/1468-229X.12969&gt;, accessed: 09/15/2022.↩︎",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Was ist Digital History?</span>"
    ]
  },
  {
    "objectID": "02_forschung_lehre.html",
    "href": "02_forschung_lehre.html",
    "title": "2  Forschung und Lehre",
    "section": "",
    "text": "2.1 Digitalisierte Quellen, digitale Quellen\nAls Historiker:innen steht die Arbeit mit Quellen im Mittelpunkt unserer Analysen. Das bedeutet gleichzeitig, dass der Zugang bzw. die Verfügbarkeit von Dokumenten einen Einfluss darauf hat, welche Fragen wir beantworten oder welche Analysen wir vornehmen können. Zugangsbeschränkungen, die die Größe und Zusammensetzung unseres Untersuchungskorpus beeinflussen, können dabei von Gedächtnisinstitutionen – also Museen, Archiven, Bibliotheken – ausgehen, beispielsweise wenn bei zeitgenössischen Akten eine Schutzfrist festgesetzt wird oder wenn ein Objekt zu fragil für die Benutzung ist. Auch kann es aus finanziellen und/oder organisatorischen Gründen schwierig sein, bestimmte Archive an weiter entfernten Orten aufzusuchen, um weitere Dokumente für die Untersuchung zu berücksichtigen. Groß angelegte Digitalisierungsprojekte in Bibliotheken und Archiven bergen damit die Möglichkeit, zusätzliche Quellen nicht nur über einen Eintrag im Bibliothekskatalog zu finden, sondern die entsprechenden Dokumente in digitaler Form auf den eigenen Rechner zu laden. Gerade auch für wertvolle historische Bestände – antike Papyri, Handschriften aus dem Frühmittelalter, einzelüberlieferte Frühdrucke usw. – entsteht hier die Möglichkeit, diese einem größeren Kreis verfügbar zu machen, ohne das Objekt zu großer Belastung durch häufige Benutzung auszusetzen, und ohne dass die Benutzer:innen lange Reisen auf sich nehmen müssten. Für mittelalterliche und frühneuzeitliche Handschriften und Drucke beispielsweise existieren mittlerweile mehrere (meist nationale) Portale, die eine zentrale Suche über alle Bestände ermöglichen; eine Auswahl findet sich unter Kapitel 2.4.2.\nNeben der Digitalisierung vorhandener Quellen (Retrodigitalisierung) steht die unaufhörliche Entstehung neuer Quellen in rein digitaler Form (born digital data). Der relativen Knappheit von Quellen – und damit Daten –, die Vormodernehistoriker:innen oftmals zu beklagen haben, steht eine Überfülle an zeitgenössischem Material gegenüber, und beide Situationen – zu wenig/zu unvollständige und zu viele/zu unübersichtliche Datenmengen – bergen methodische Probleme: Wie stellt man ein Korpus, also eine Sammlung von Quellen zusammen, das ausreichend Dokumente beinhaltet, um Fragestellungen zu beantworten, Thesen zu stützen, neue Erkenntnisse zu erhalten, das aber gleichzeitig in einem Forscher:innenleben bewältigbar bleibt? Historiker:innen müssen neue Kompetenzen erwerben, um mit solchen Fragen reflektiert umzugehen. Zur klassischen Quellenkritik kommt die digitale Quellenkritik, zur Fähigkeit, analoge Quellen zu lesen und zu verstehen, ein Äquivalent für den digitalen Bereich. Etwas ausführlicher geht es in Kapitel 3 um Digital Literacy und Digital Criticism.",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Forschung und Lehre</span>"
    ]
  },
  {
    "objectID": "02_forschung_lehre.html#sec-digitaletools",
    "href": "02_forschung_lehre.html#sec-digitaletools",
    "title": "2  Forschung und Lehre",
    "section": "2.2 Digitale Tools zur Analyse",
    "text": "2.2 Digitale Tools zur Analyse\nDie hier bereits zitierte Definition, die aktive und kritische Nutzung digitaler Werkzeuge in Forschung, Lehre oder Studium sei es, was Digital Humanities ausmachten, wirft die Frage auf, was genau unter digitalen Werkzeugen, unter digital tools zu verstehen ist, und zu welchem Zweck man sie einsetzt. Allein schon das Lesen dieses Guides ist ohne digitale Hilfsmittel nicht möglich – es existiert kein gedrucktes Exemplar davon. Lesen am Bildschirm allein macht noch keinen digital humanist, aber man muss nicht erst eine Programmiersprache lernen, um den Computer für die eigene Arbeit zu nutzen und zu Ergebnissen zu kommen, die mit klassischen Methoden – im Bereich der Geschichtswissenschaften etwa papierbasiertes close reading von Quellen und Forschungsliteratur – nicht im selben Ausmaß erzielt werden könnten.\nUntersuchungen, die digitale Methoden einsetzen, sind im Normalfall skalierbar – wenn man eine Software benutzt, die die Häufigkeit von Begriffen in einem Dokument zählt, sollte es keinen Unterschied in der Anwendung machen, ob man eines oder einhundert Dokumente auswerten will. Würde man dasselbe per Hand tun, wäre man analog zum Anwachsen der Dokumente mit der Auszählung beschäftigt. Digitale Werkzeuge ermöglichen es also unter anderem, Untersuchungen auf größere Mengen von Dokumenten auszuweiten. Sie ermöglichen es auch, an ein so erweitertes Korpus andere Fragen zu stellen, als dies mit einer kleineren Quellen-/Datengrundlage möglich wäre. Die vorherrschende Überlieferung historischer Quellen besteht aus Text, handgeschrieben, gemeißelt oder gedruckt – und durch die Möglichkeit, diesen mittels Texterkennung in computerlesbare Daten umzuwandeln, ergeben sich neue Perspektiven für die Arbeit von Historiker:innen: Wenn Texte als Daten verstanden werden, lassen sich aus Textquellen Datenbestände erstellen, die mithilfe quantitativer Methoden untersucht und ausgewertet werden können.1\nFür die Literaturwissenschaften beispielsweise ist ein wichtiges Anwendungsfeld die Überprüfung von Autor:innenschaft: Ob ein anonym überliefertes Werk einem:r namentlich bekannten Autor:in zugeschrieben werden kann, lässt sich entweder durch close reading von Literaturwissenschaftler:innen überprüfen, oder durch die Suche nach patterns, Mustern, nach quantifizierbaren Eigenschaften eines Textes, wie beispielsweise die Häufigkeit von Funktionswörtern, Partikeln, Satzzeichen usw. Der unter dem Pseudonym Robert Galbraith veröffentlichte Kriminalroman The Cuckoo’s Calling konnte mit entsprechender Software Joanne K. Rowling zugeschrieben werden – damit dauerte die Untersuchung dreißig Minuten, was etwa dem Lesen von zwanzig Romanseiten entspricht. Zu einem Artikel, der diesen Fall thematisiert und in das Feld der linguistischen Forensik einbettet, die Straftäter:innen mithilfe quantitativer Textanalyse ermittelt, geht es hier. Ein Video zur Entwicklung und Anwendung von Software zur Zuschreibung von Autor:innenschaft finden Sie hier. Die genutzte Software, JGGAP,2 lässt sich offensichtlich auch für historische Analysen nutzen – man denke nur an Herrschaftssysteme, in denen strenge Zensur geübt wird/wurde und viele Autor:innen daher nicht unter ihrem Klarnamen publizier(t)en. Durch eine Identifikation anonymer Schreiber:innen lassen sich weitere Aspekte rund um die Thematik Zensur untersuchen – welche Akteur:innen waren öffentlich bekannt, wer publizierte gleichzeitig anonym und unter Klarnamen, welche Autor:innen schrieben aus dem Exil, welche Netzwerke lassen sich rekonstruieren usw. Dadurch, dass ein Programm durch quantitative Auswertungen die Kärrnerarbeit der Identifikation abnehmen kann – um einen reflektierten Umgang mit Daten und Algorithmen geht es in Kapitel 3 –, bleibt mehr Zeit für die qualitative Arbeit; gleichzeitig fußt die Analyse auf einem aussagekräftigen Datensatz, anstatt nur Einzelbeispiele beleuchten zu können.\nQuantitative und qualitative Methoden sollen hier keinesfalls gegeneinander ausgespielt werden; vielmehr soll verdeutlicht werden, dass beide Herangehensweisen Vor- und Nachteile haben, und dass sie im besten Fall gewinnbringend miteinander kombiniert werden können – quantitative Auswertungen nur um ihrer selbst willen und ohne eine spezifische historische Fragestellung generieren kaum je einen Mehrwert.\nJe nach Datengrundlage, Analysezweck und Forschungsfrage bieten sich unterschiedliche Tools zur Nutzung an; für die meisten Forschungsvorhaben bis zum Ende des Studiums dürfte existierende Software ausreichen, sei es für die Akquise und Aufbereitung von Daten(-sätzen), für verschiedene Arten von Textanalysen, statistische Auswertungen, Netzwerkanalysen, Geomapping oder Visualisierungen. Eine Auswahl an Tools – alle kostenfrei/open source – für spezifische Analysen findet sich unter Literatur, Tools, Tutorials. Für gewisse Analysen bietet es sich an, Programmierkenntnisse zu erwerben – das Erstellen eigener Skripts, also kleiner Programme, beinhaltet die umfassende Kontrolle darüber, wie Daten eingelesen, aufbereitet, angereichert, analysiert und visualisiert werden; bei wiederkehrenden Prozessen, die händisch einige Arbeitszeit in Anspruch nehmen würden, lässt sich so zusätzlich Zeit sparen.\nFür geisteswissenschaftliche Projekte werden zurzeit vor allem zwei Programmiersprachen genutzt, R und Python. Da sich beide großer Beliebtheit in den Humanities erfreuen, existieren mittlerweile zahlreiche Packages, die Data und Text Mining, also groß angelegte Daten- und Textanalysen, sehr einfach machen. Solche Packages für Programmiersprachen kann man sich wie Plug-Ins für Programme vorstellen, beispielsweise ein AdBlocker für den Browser. So etwas war von den Entwickler:innen ursprünglich nicht vorgesehen, aber jemand hatte Bedarf, Werbeanzeigen zu blockieren, hat hierzu ein Programm geschrieben und es der Allgemeinheit zur Verfügung gestellt. Der Unterschied zu einem Package ist, dass dieses verschiedene Funktionen zur Verfügung stellt – auswählen und ausführen müssen die Anwender:innen. Wer in Schule und Studium keine Berührungspunkte mit Programmieren hatte, wird zu Beginn vielleicht größere Berührungsängste haben – aber noch einmal: Sie müssen nicht programmieren können, um quantitativ zu arbeiten. Speziell an Historiker:innen ohne Programmier-Vorkenntnisse richtet sich das Projekt “The Programming Historian”, das seit 2008 zahlreiche Tutorials veröffentlicht, um verschiedene Tools, Techniken und Workflows für die geschichtswissenschaftliche Forschung und Lehre vorzustellen.",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Forschung und Lehre</span>"
    ]
  },
  {
    "objectID": "02_forschung_lehre.html#digitale-tools-zur-kommunikation",
    "href": "02_forschung_lehre.html#digitale-tools-zur-kommunikation",
    "title": "2  Forschung und Lehre",
    "section": "2.3 Digitale Tools zur Kommunikation",
    "text": "2.3 Digitale Tools zur Kommunikation\nAuch ohne quantitative/computergestützte Analysen bei der historischen Arbeit selbst bieten sich mithilfe digitaler Unterstützung verschiedene Möglichkeiten an, um Forschungsergebnisse zu kommunizieren. Dies betrifft zum einen unterschiedliche Formen der digitalen Publikation, zum anderen die Nutzung digitaler Tools zur Visualisierung von Ergebnissen. Gedruckte wissenschaftliche Zeitschriften und Buchverlage haben gewisse Kriterien bei der Annahme eines Textes zur Publikation – inhaltliche Qualität ist bei jeder wissenschaftlichen Arbeit wichtig; aber Formalia wie Länge/Umfang/Bebilderung sind für Onlineformate weniger zentral. So können erste Resultate aus einem neuen Forschungsprojekt oder auch Ergebnisse aus einer Seminararbeit einem interessierten Publikum in Form eines Blogbeitrags ohne viel organisatorischen Vorlauf und Rücksicht auf die Auslastung von Druckerpressen präsentiert werden.\nDie Zahl der wissenschaftlichen Blogs ist im letzten Jahrzehnt kontinuierlich gestiegen, sodass für verschiedenste Fächer und Themengebiete eine Auswahl an passenden Publikationsorten besteht. Sie verfügen über eine Redaktion, also verantwortlich zeichnende Wissenschaftler:innen, die eine Qualitätssicherung der Beiträge leistet. Auch viele Promotionsprojekte werden von Blogs begleitet – das liefert Sichtbarkeit für die Forschung der Doktorand:innen und bietet eine Plattform für Themen, die in der Dissertation keinen Platz finden, aber doch erwähnenswert sind.\nHypothèses hat sich als eine wichtige Plattform etabliert, die zahlreiche Wissenschaftsblogs hostet; ein Katalog aller Blogs, filterbar nach Themen und Epochen, findet sich hier.\nAus Lehrveranstaltungen und Forschungsprojekten am Departement Geschichte der Universität Basel sind u.a. folgende Blogs hervorgegangen:\n\nExilland Schweiz. Kulturschaffende und Intellektuelle im Schweizer Exil, 2021–\n\nGlobal Health Africa. Circulating Knowledge and Innovations, 2019–\n\nMaterialized Histories. Materielle Kultur und digitale Forschung, 2021–\n\nMaterialized Identities. Objects, Affects and Effects in Early Modern Culture (1450–1750), 2016–2021.\n\nStadt.Geschichte.Basel. Blog zum Forschungsprojekt, 2019–\n\nThe Color Line. Race Relations in Schlüsseltexten amerikanischer Autor:innen (1881–1953), 2022–\n\nAls relativ junges Format haben sich Data Stories etabliert – Narrative, die auf der Basis von (quantitativen oder qualitativen) Daten und Analysen einen Sachverhalt darstellen. Diese Art von (Daten-)Publikation wird u.a. im journalistischen Bereich genutzt, auch, um Interaktivität und Aktualität von Daten zu gewährleisten. Es gibt verschiedene Tools, um solche Stories zu erstellen, zum Teil bereits mit integrierter Publikationsmöglichkeit; eine Auswahl findet sich unter Literatur, Tools, Tutorials.\nEin Beispiel, das auch Input durch die Nutzer:innen ermöglicht, ist Darüber spricht der Bundestag, eine Data Story der “ZEIT”, die alle Reden im deutschen Bundestag seit 1949 analysierbar macht. Ein Beispiel für eine kartenzentrierte Darstellung mit Einbettung verschiedener Medien ist die Reise von Arya aus Game of Thrones. Ein Beispiel für die Nutzung von Volkszählungsdaten stammt vom Schweizer Bundesamt für Statistik, Die Schweiz (er)zählen.",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Forschung und Lehre</span>"
    ]
  },
  {
    "objectID": "02_forschung_lehre.html#sec-projects",
    "href": "02_forschung_lehre.html#sec-projects",
    "title": "2  Forschung und Lehre",
    "section": "2.4 Projekte und Ressourcen für Forschung und Lehre",
    "text": "2.4 Projekte und Ressourcen für Forschung und Lehre\n\n2.4.1 Alte Geschichte\nProjekte:\n\nD-Scribes: Projekt zur Erkennung antiker Schreiber:innen griechischer und koptischer Papyri\n\nRessourcen/Portale:\n\nPapyprus Portal: digitale Papyrussammlung\nThe Arabic Papyrology Database: Datenbank für vormoderne arabische Schriften auf Papyrus, Pergament oder Papier vom 7. bis zum 16. Jahrhundert\n\n\n\n2.4.2 Mittelalter und Frühe Neuzeit\nProjekte:\n\nBurchards Dekret Digital: Digitale Edition, die die handschriftliche Überlieferung des Decretum Burchardi erschließt.\nhallerNet: Editions- und Forschungsplattform mit mehreren Textsammlungen von Gelehrten aus der Übergangszeit zwischen 1700 und 1850, mit Strukturdaten verknüpft\nPrinted Markets: Projekt zur Digitalisierung und Anreicherung der frühneuzeitlichen Basler Anzeigenzeitung “Avisblatt” (1729–1844)\nRepertorium Academicum: Projekt zur Erfassung europäischer Gelehrter zwischen 1250 und 1550\n\nRessourcen/Portale:\n\ndMGH: Monumenta Germaniae Historica online (Beta-Version)\ne-codices: Virtuelle Handschriftenbibliothek der Schweiz\nFragmentarium: Laboratory for Medieval Manuscript Fragments\nHandschriftenportal: Zentraler nationaler Nachweis für Buchhandschriften in deutschen Bibliotheken und in deutscher Sprache (Entwicklungsstadium)\nInnovating Knowledge: Datenbank und digitale Edition zu Isidors von Sevilla “Etymologiae”\ne-manuscripta: Digitalisierte handschriftliche Quellen aus Schweizer Bibliotheken und Archiven\ne-rara: Plattform für digitalisierte Drucke aus Schweizer Institutionen\nGallica: Digitalisierte Quellen aus französischen Bibliotheken\nStapfer Enquête: Edition der Helvetischen Schulumfrage von 1799\nswisscollections: Suchplattform für historische Schweizer Bestände\ntranscriptiones: Plattform zum Erstellen, Teilen und Nutzen von Transkriptionen historischer Manuskripte\n\n\n\n2.4.3 Moderne und Zeitgeschichte\nProjekte:\n\nimpresso. Media Monitoring of the Past: Projekt zur Aufbereitung, semantischen Anreicherung, Repräsentation, Exploration und Recherche in historischen Medien (Zeitungen und Radio) über Zeit, Sprachen und Nationalgrenzen hinweg\nLiving with Machines: Forschungsprojekt über die Auswirkungen der Mechanisierung der Arbeit während der Industrialisierung\nRefugee History: Wissenschaftliches Blog und interaktives Netzwerk zu aktuellen Debatten um das Thema “Flüchtlinge”\n\nRessourcen/Portale:\n\nDatenbank Bild + Ton zur Geschichte (Schweizer) sozialer Bewegungen\nDodis: Wissenschaftliche Edition von Dokumenten zur Schweizer Außenpolitik\nGallica: Digitalisierte Quellen aus französischen Bibliotheken\ne-newspaperarchives.ch: Schweizer Zeitungen online\ne-periodica: Schweizer Zeitschriften online\nHistorische Statistik der Schweiz (HSSO)\nhistat: Zeitreihen zur Historischen Statistik\n\n\n\n2.4.4 Jüdische Geschichte\nProjekte:\n\nDigital Jewish Studies Online, Stroum Center for Jewish Studies, University of Washington\n\nRessourcen/Portale:\n\nBlavatnik Archive: Archiv zur Erhaltung und Verfügbarmachung von Material zur (jüdischen) Geschichte des 20. Jahrhunderts mit Fokus auf die zwei Weltkriege und Sowjetrussland.\nMenny, Anna; Rürup, Miriam; Siegel, Björn: Jüdische Geschichte im deutschsprachigen Raum, in: Busse, Laura u. a. (Hg.): Clio-Guide. Ein Handbuch zu digitalen Ressourcen für die Geschichtswissenschaften, Berlin 2018, S. E.2-1–E.2-56. Online: https://doi.org/10.18452/19244.\n\n\n\n2.4.5 Geschichte Afrikas\nProjekte:\n\nEmandulo: Digitales Archiv, das archivalische/museale Sammlungen und Präsentationen über präkoloniale südafrikanische Geschichte zusammenführt und neu zusammenstellt.\nLegacies of British Slavery: Forschungsprojekt zum britischen Sklavenhandel und -besitz\n\nRessourcen/Portale:\n\nFHYA: Experimentelle digitale Forschungsplattform über präkoloniale südafrikanische Geschichte\nLegacies of British Slavery: Datenbank zum britischen Sklavenhandel und -besitz\nSlave Voyages: Datenbanken zum transatlantischen und interamerikanischen Sklavenhandel und Personendatenbank\n\n\n\n2.4.6 Osteuropäische Geschichte\nProjekte:\n\nGulag: Many Days, Many Lives: Archiv und Präsentationsplattform zu den sowjetischen Gulags\nGulag Online: Virtuelles Museum mit Präsentationen und Quellen zum Leben im Gulag, zu Personen und Objekten\nSeventeen Moments in Soviet History: Multimediales Online-Archiv mit ausgewählten Quellen zu Ereignissen in der sowjetischen Geschichte anhand 17 verschiedener Jahre zwischen 1917 und 1991\nThe Imperiia Project: Eine räumliche Geschichte des Russischen Reichs\n\nRessourcen/Portale:\n\nBlavatnik Archive: Archiv zur Erhaltung und Verfügbarmachung von Material zur (jüdischen) Geschichte des 20. Jahrhunderts mit Fokus auf die zwei Weltkriege und Sowjetrussland.\nThe Other Side: Webarchiv von Interviews ehemaliger Ostarbeiter:innen, Kriegsgefangener und Insassen deutscher Lager; Publikationsplattform\n\n\n\n2.4.7 Epochen-/Areaübergreifend:\nProjekte:\n\nLord of the Rings Project: Interaktive Analyse der Werke J. R. R. Tolkiens\n\nRessourcen/Portale:\n\nAround DH in 80 days: Portal zur Vorstellung achtzig verschiedener Digital-Humanities-Projekte weltweit und aus verschiedenen Disziplinen\nInternet Archive: digitale Bibliothek zur Archivierung von Büchern, Bildern, Filmen, Software, Musik und Webseiten",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Forschung und Lehre</span>"
    ]
  },
  {
    "objectID": "02_forschung_lehre.html#footnotes",
    "href": "02_forschung_lehre.html#footnotes",
    "title": "2  Forschung und Lehre",
    "section": "",
    "text": "In der Zeitgeschichte ist dies einfacher möglich, weil viele Texte hier schon digital vorliegen; für Epochen bzw. Forschungsbereiche, die eher eine Quellenarmut zu beklagen haben, ist eine Generierung von Daten aus Texten umso interessanter.↩︎\nJava Graphical Authorship Attribution Program, http://evllabs.github.io/JGAAP/.↩︎",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Forschung und Lehre</span>"
    ]
  },
  {
    "objectID": "03_digital_literacy_criticism.html",
    "href": "03_digital_literacy_criticism.html",
    "title": "3  Digital Literacy, Digital Criticism",
    "section": "",
    "text": "3.1 Digital Literacy, Data Literacy\nUnter Data Literacy wird die Kompetenz verstanden, Daten zu sammeln, zu managen, zu evaluieren und zu nutzen,1 eine Kompetenz, die jede:r für den mittlerweile unvermeidlichen Umgang mit Daten verschiedenster Art im eigenen Alltag entwickeln sollte. Je nach Forschungsdisziplin ergeben sich weiter gewisse Spezifika, wobei Studierenden der Geisteswissenschaften ein Thema wie Algorithmenkritik nicht als erstes in den Sinn kommt, wenn es um die im Studium zu erwerbenden Kompetenzen geht.2 Aber auch ohne den Quellcode von machine-learning-Software im Detail zu verstehen, ermöglicht ein grundlegendes Verständnis von und ein Wissen über die Funktionsweisen solcher Anwendungen einen reflektierten Umgang mit diesen. Eine solche Art von Digital bzw. Data Literacy ist vor allem dann relevant, wenn es um die Interpretation von Ergebnissen geht, die scheinbar objektiv sind, bzw. scheinbar objektiv entstanden. Ein gutes Beispiel hierfür sind die Ergebnislisten bei Suchanfragen in einer Suchmaschine. Je nachdem, welchen Anbieter Sie nutzen, spielen verschiedene Umstände in die Generierung von Trefferlisten hinein, beispielsweise Ihre Suchhistorie, sodass search neutrality nicht mehr gewährleistet ist.3\nGehen Sie auf die Bilder-Suche von Google und suchen Sie nach “historian”. Was sehen Sie?\nWüsste ich nichts über Geschichtswissenschaftler:innen, würde ich aufgrund der Ergebnisse meiner Suche davon ausgehen, “a historian” wäre meist ein alter, weißer Mann mit Brille, Bart und einem großen Bücherregal; wenn Sie sich am Departement Geschichte der Uni Basel umsehen, dürfte ein etwas anderer Eindruck entstehen. Die Ergebnisse von Suchmaschinen, die für ihr Funktionieren Algorithmen anwenden, sind biased, verzerrt: Sie beruhen auf vorangegangenen Suchen, Vorlieben, geographischem Standort – und auf von Menschen eingegebenen Metadaten, also Daten mit Informationen über andere Daten. Ein Bewusstsein hierfür und das Hinterfragen von Datensätzen gehören also mit zur Arbeit in einer digitalisierten Welt.",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Digital Literacy, Digital Criticism</span>"
    ]
  },
  {
    "objectID": "03_digital_literacy_criticism.html#digital-criticism-data-criticism",
    "href": "03_digital_literacy_criticism.html#digital-criticism-data-criticism",
    "title": "3  Digital Literacy, Digital Criticism",
    "section": "3.2 Digital Criticism, Data Criticism",
    "text": "3.2 Digital Criticism, Data Criticism\nDigitalisierte Quellen ebenso wie rein digitale erfordern eine erweiterte Art von Quellenkritik – im Einführungskurs an der Universität Basel lernen Sie die Grundlagen klassischer Quellenkritik:\nWoher kommt eine Quelle, wer hat sie unter welchen Umständen und zu welchem Zweck erstellt? Welche Absichten können darin verborgen sein, und welche Verzerrungen können sich durch sie ergeben? Welche Tendenzen könnten sich in hochmittelalterlichen Herrscherchroniken verstecken, wenn der Verfasser in direkter Abhängigkeit des Auftraggebers stand? Wie sind Zeug:innenaussagen in Hexenprozessen zu bewerten, die unter Anwendung körperlicher Strafe entstanden sind? Mit wie viel Vorsicht sind die Inhalte eines Tagebuchs zu bewerten, das allem Anschein nach mit Blick auf eine spätere Veröffentlichung verfasst wurde?\nNeben der inneren Kritik geht es bei der Arbeit mit Quellen immer auch um Fragen der Korpusbildung: Wie kann eine Quellengrundlage erstellt werden, die für Beantwortung einer spezifischen historischen Fragestellung belastbar und aussagekräftig genug ist und gleichzeitig in angemessener Zeit bearbeitet werden kann? Hinzu kommen Spezifika bei der Arbeit mit unterschiedlichen Quellenformen bzw. -formaten: Bei analogen Quellen, die auch in digitaler Form zur Verfügung stehen, besteht die Gefahr, dass ein Thema, ein Bereich, ein Aspekt vernachlässigt wird, wenn nur die unmittelbar verfügbaren, digitalisierten Bestände zur Korpusbidlung genutzt werden. Wenn Sie sich beispielsweise für die Schweizer Historikerin und Frauenrechtlerin Meta von Salis (1855–1929) und deren briefliche Korrespondenz – Friedrich Nietzsche war einer ihrer Brieffreunde – interessieren und über die Suchplattform für historische Schweizer Bestände, swisscollections, in nationalen Bibliotheken und Archiven nach entsprechenden Dokumenten suchen, erhalten Sie 361 Treffer:\n\n\n\n\n\n\n\n\n\nErweiterte Suchmaske von swisscollections\n\n\n\n\n \n\n\n\n\n\nSuchergebnisse für “Meta von Salis” + “Brief”\n\n\n\n\n\nDigital verfügbar waren hiervon im Oktober 2022 lediglich drei Einträge, wobei der erste ein Brief von Nietzsche an Meta von Salis ist, der zweite Eintrag umfasst sieben Briefe von Caroline Farner, und der dritte Eintrag ist weder an noch von Meta von Salis, sondern hat sie nur zum Thema:\n\n\n\nSuchergebnisse für “Meta von Salis” + “Brief” + “Digitalisat verfügbar”\n\n\nIhnen würde bei einer Korpuserstellung vom Schreibtisch aus, also nur mit den angezeigten Digitalisaten, also der Großteil der Überlieferung fehlen, und Ihre Untersuchungsergebnisse wären wohl sehr verzerrt, würden Sie statistische Aussagen treffen wollen: Meta von Salis unterhielt brieflichen Kontakt zu einem Mann und einer Frau, das Geschlechterverhältnis wäre also ausgeglichen; und Frauen schreiben im Schnitt mehr Briefe an Meta von Salis als Männer. Beim Blick auf alle Suchergebnisse würden sich Ihre Aussagen aber sehr ändern, und es würde sich lohnen, diese Verzerrung, diesen Bias aus Ihrer Datengrundlage zu entfernen.\n\nHinzu kommt natürlich immer das grundlegende Problem bei der Suche nach Quellen: swisscollections und ähnliche Portale können nur anzeigen, was die Kooperationspartner:innen zur Verfügung stellen. Hat eine Bibliothek Briefe von Meta von Salis in ihrem Bestand, diese aber noch nicht als Datensatz erfasst, wissen Sie im Gegensatz zum obigen Beispiel nicht einmal, dass Ihnen etwas entgehen würde, dass in Ihrem Korpus überhaupt ein Bias vorhanden ist.\n\nÄhnliche Vorsicht zur Vermeidung von Verzerrungen in der Datengrundlage gilt bei der Arbeit mit rein digitalen Daten, beispielweise bei der Auswertung von Datensätzen aus Befragungen. Wenn Sie sich am 27.10.2022 vor die Universitätsbibliothek in Basel stellen und einen Tag lang mithilfe eines kurzen Fragebogens und einer Tabellendatei erfassen, wie zufrieden die befragten Personen mit dem Essen in der Unimensa sind, werden Sie am Ende einen Datensatz erhalten, in dem sich vermutlich über 80% der Befragten für besseres und nahezu 100% für günstigeres Essen in der Mensa aussprechen – eine gute Schlagzeile für die BZ, die sich auf die neuesten Ergebnisse einer wissenschaftlichen Studie berufen kann. Führen Sie die gleiche Umfrage eine Woche später, mitten während der Herbstmesse durch, werden die Ergebnisse wohl erheblich anders aussehen. Die Wahrscheinlichkeit, dass die Mensa infolge der BZ-Schlagzeile innerhalb weniger Tage den Menüplan überarbeitet und die Preise herabgesetzt hat, ist dabei wohl geringer als diejenige, dass sich Ihr Sample, die Auswahl an Datenpunkten, also befragten Personen, durch die Messe stark verändert hat: Im Umkreis der Bibliothek treffen Sie nun nicht mehr vor allem Studierende und andere Uni-Angehörige an, sondern auch Messebesucher:innen vom Petersplatz. Auch hier sind Verzerrungen entstanden, ähnlich wie beim vorherigen Beispiel mit den Briefen: Wenn aus einer Gesamtheit nur eine spezifische Untermenge beobachtet wird, die sich durch ein gemeinsames Merkmal von der Gesamtheit unterscheidet – digitalisierte Quelle oder Besucher:in der Universitätsbibliothek –, ist die Datengrundlage und damit die Untersuchungsergebnisse biased. Um bei Daten, die Sie nachnutzen, eventuell vorhandene Verzerrungen nicht weiterzutransportieren, ist das Üben von Datenkritik eine essentielle Kompetenz.\nZur Tatsache, dass Daten eben nicht “gegeben” sind (lat. dare, datum: geben, gegeben), sondern gemacht, und daher entsprechend interpretiert werden müssen, finden Sie ein gutes Interview von Roopika Risam (2020);4 zur Zementierung von Klischees durch Übersetzungsalgorithmen gibt es einen Artikel in der Republik von Marie-José Kolly und Simon Schmid (2021);5 und über die Macht von Data Science und dem Änderungspotential von Data Feminism haben Catherine D’Ignazio und Lauren F. Klein 2020 ein ganzes Buch veröffentlich.6\nZur Frage, wie sich die digitale Wende, der digital turn, auf die Quellenkritik auswirkt, sehen Sie sich dieses kurze Video des Projekts Ranke.2 – Quellenkritik im digitalen Zeitalter an:7\n\n\nEine Handreichung zum Umgang mit digitalisierten und digitalen Daten, das im selben Projekt erarbeitet wurde, finden Sie hier.",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Digital Literacy, Digital Criticism</span>"
    ]
  },
  {
    "objectID": "03_digital_literacy_criticism.html#künstliche-intelligenz-und-algorithmenkritik",
    "href": "03_digital_literacy_criticism.html#künstliche-intelligenz-und-algorithmenkritik",
    "title": "3  Digital Literacy, Digital Criticism",
    "section": "3.3 Künstliche Intelligenz und Algorithmenkritik",
    "text": "3.3 Künstliche Intelligenz und Algorithmenkritik\nMit dem Release von ChatGPT-3 der Firma OpenAI im Winter 2022 hat sich die Nutzung künstlicher Intelligenz (KI) auf einen Schlag für einen sehr großen Personenkreis geöffnet: Ein Chatbot steht, nach einmaliger Registrierung, über eine URL den unterschiedlichsten Fragen Rede und Antwort, und erklärt wahlweise Fermats letzten Satz, produziert Sonette im Stil Shakespeares über die Liebe einer Emmentalers zu einer Büroklammer oder fasst den in den 1980er-Jahren in Deutschland geführten Historiker:innenstreit über die Singularität des Holocaust zusammen. Technisch handelt es sich bei ChatGPT um ein generatives Sprachmodell (Generative Pre-trained Transformer), ein sog. Large Language Model (LLM), das mit einem sehr großen Datensatz trainiert wurde – etwa 1 Billion Wörter aus sechs Millionen Wikipedia-Artikeln, Millionen Webseiten und digitalisierten Büchern, insgesamt etwas 570 GB –, um auf dieser Grundlage neue, der Anfrage bzw. Eingabe der Nutzer:innen entsprechende Texte zu produzieren.8 Bittet man ChatGPT selbst, das Prinzip eines generativen Sprachmodells und seines Algorithmus zu erklären, erhält man folgende Antwort:\n\nHallo! Gerne erkläre ich dir das Prinzip hinter einem generativen Sprachmodell und meinem Algorithmus.\nEin generatives Sprachmodell ist ein künstlicher Intelligenz-Algorithmus, der darauf trainiert wird, Texte zu erzeugen, die einer bestimmten Sprache oder einem bestimmten Stil entsprechen. Das Modell nutzt dabei eine statistische Methode namens Wahrscheinlichkeitsverteilung, um die Wahrscheinlichkeit zu bestimmen, mit der ein bestimmtes Wort oder eine bestimmte Phrase in einem Text auftaucht.\nDas Ziel des Algorithmus besteht darin, Texte zu generieren, die natürliche Sprachstrukturen und -muster aufweisen, sodass sie für den Menschen verständlich und lesbar sind. Das generative Sprachmodell kann dabei verschiedene Ansätze verfolgen, wie zum Beispiel die Verwendung von rekurrenten neuronalen Netzen (RNN) oder Transformer-Modellen.\nAls ChatGPT, basiere ich auf der GPT-3.5 Architektur und nutze ebenfalls ein Transformer-Modell. Mein Algorithmus nutzt eine große Menge an Textdaten, um Sprachmuster und -strukturen zu erlernen und kann anschließend Texte generieren, die dieser Sprache ähnlich sind.\nUm Texte zu erzeugen, nutzt mein Algorithmus eine sogenannte autoregressive Methode, bei der er das zuvor erzeugte Wort oder die zuvor erzeugte Phrase als Input verwendet, um das nächste Wort oder die nächste Phrase vorherzusagen. Auf diese Weise wird ein Text Schritt für Schritt erzeugt, wobei jeder Schritt auf dem vorherigen basiert.\nDurch die ständige Anpassung der Gewichtungen im Modell, je nachdem wie gut es darin ist, den nächsten Textsatz vorherzusagen, wird das generative Sprachmodell im Laufe der Zeit immer besser darin, realistische und gut klingende Texte zu erzeugen.9\n\nDas bedeutet, dass die Antworten besonders gut bzw. sinnvoll für Fragen zu Themen sind, für die sehr viel Trainingsmaterial vorlag. Das bedeutet auch, dass alles, was nach Abschluss des Trainings im November 2021 online gestellt wurde, nicht mehr Eingang in das Modell 3.5 gefunden hat; hier wird zwar aktuell nachgeholt, und je nach Release und Modell gibt es hier Unterschiede, aber Ereignisse wie den Ukraine-Krieg kennt ChatGPT (noch) nicht.10 Auch ist es nicht immer einfach, die Ausgewogenheit und Qualität der Antworten richtig abzuschätzen – bei der Erstellung von Programmierfunktionen lässt sich der Vorschlag eines Chatbots direkt testen. Bei Fragen, die etwas offener gestellt sind, bieten die LLMs sehr überzeugend klingende Antworten – die aber natürlich geprägt sind vom zugrundeliegenden Trainingsmaterial, Modell und Trainingsmethoden, die nicht immer transparent sind.11 Auf die Frage nach den Namen von “ten medieval philosophers” gibt ChatGPT-4o zehn Männer aus, sieben davon aus West-/Zentraleuropa.12 Erst auf den Hinweis, dass die Liste recht männerlastig und weiß sei, bekommt man eine größere Auswahl arabischer und persischer Philosophen und (westlicher) Frauen. Es braucht noch eine weitere Nachfrage, um auch indische und chinesische Namen zu erhalten. Der Großteil der Texte, die ins Training einflossen, sind englisch und damit größtenteils aus dem angloamerikanischen Raum. Ein Ungleichgewicht zugunsten Wissensproduktion und Meinungen aus diesem (Sprach-)Raum ist daher unvermeidlich und muss bei der Nutzung stets mitbedacht werden.\nMit Blick auf die obige, tadellos formulierte Antwort zum eigenen Funktionieren liegt es natürlich nahe, LLMs für alle Arten von Schreibarbeiten zur Hilfe zu ziehen, und die Aufregung im (Hoch-)Schulbereich war mitunter groß13 – auch, weil die Programme erfundene Fakten im Brustton der Überzeugung formulieren oder auf nicht existierende Referenzen verweisen. Bei unkritischer Übernahme der Antworten und Ausgabe des Textes als eigenes Produkt kommt es also nicht nur zur Täuschung in Bezug auf die eigenen Fähigkeiten, sondern es werden auch falsche Informationen übernommen und mitunter verbreitet, beispielsweise in Referaten. Zahlreiche Hochschulleitungen haben auf die neue Technik reagiert und Codes of Conduct erstellt; an der Universität Basel gibt es eine erweiterte Eigenständigkeitserklärung, in der versichert wird, mit KI erstellte Passagen als solche zu kennzeichnen – Software, die mithilfe KI erstellte Texte als solche identifiziert, gibt es natürlich auch.[Das Vizerektorat Lehre hat einen Leitfaden “Aus KI zitieren” erstellt.]\nÜberlegungen zum konstruktiven Einsatz generativer Sprachmodelle, als Lernassistent,14 Schreibcoach oder Hilfe bei der Recherche, gibt es ebenso, und gelungene Beispiele, wie ChatGPT in der Lehre eingesetzt werden kann.15 Zentral für eine gewinnbringende und informierte Nutzung ist sicher ein ungefähres Wissen über die Funktionsweise des Algorithmus und die Bereitschaft, erhaltene Antworten kritisch zu prüfen und ggf. zu verwerfen.",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Digital Literacy, Digital Criticism</span>"
    ]
  },
  {
    "objectID": "03_digital_literacy_criticism.html#footnotes",
    "href": "03_digital_literacy_criticism.html#footnotes",
    "title": "3  Digital Literacy, Digital Criticism",
    "section": "",
    "text": "Ridsdale, Chantel; Rothwell, James; Smit, Mike u. a.: Strategies and Best Practices for Data Literacy Education Knowledge Synthesis Report, 2015, S. 8. Online: &lt;https://doi.org/10.13140/RG.2.1.1922.5044&gt;.↩︎\nVerschiedene Organisationen und Interessensverbände machen sich hierzu Gedanken; die deutsche “Gesellschaft für Informatik” hat ein Grundlagenpapier erstellt, in dem es um Data Literacy und Data Science Education als digitale Kompetenzen in der Hochschulausbildung geht: https://gi.de/fileadmin/GI/Hauptseite/Aktuelles/Aktionen/Data_Literacy/GI_DataScience_2018-04-20_FINAL.pdf.↩︎\nAlternative Suchmaschinen sind zum Beispiel Startpage oder DuckDuckGo.↩︎\nRisam, Roopika: “It’s Data, Not Reality”: On Situated Data With Jill Walker Rettberg, 06.2020. Online: &lt;https://medium.com/nightingale/its-data-not-reality-on-situated-data-with-jill-walker-rettberg-d27c71b0b451&gt;, Stand: 16.08.2022.↩︎\nKolly, Marie-José; Schmid, Simon: Sie ist hübsch. Er ist stark. Er ist Lehrer. Sie ist Kindergärtnerin, in: Republik, 04.2021. Online: &lt;https://www.republik.ch/2021/04/19/sie-ist-huebsch-er-ist-stark-er-ist-lehrer-sie-ist-kindergaertnerin&gt;, Stand: 23.08.2022.↩︎\nD’Ignazio, Catherine; Klein, Lauren F.: Data feminism, 2020. Online: &lt;https://direct.mit.edu/books/book/4660/Data-Feminism&gt;.↩︎\nEine Selbstlerneinheit zum Thema finden Sie hier: https://ranke2.uni.lu/u/archival-digital-turn/.↩︎\nEine ausführliche und anschauliche Erklärung von Helmut Linde findet sich unter https://www.golem.de/news/kuenstliche-intelligenz-so-funktioniert-chatgpt-2302-171644.html, ein Einführungsvortrag mit besonderem Blick auf Konsequenzen für die Hochschullehre von Thomas Arnold unter https://www.youtube.com/watch?v=-c8ogAwX6KI.↩︎\nChatGPT-3.5-Antwort, Anfrage zu „Wie funktioniert ein generatives Sprachmodell und der dazugehörige Algorithmus?“, 11. November 2022.↩︎\nDie neueste Version von ChatGPT, GPT-4.o, hat keine neueren Trainingsdaten als 3.5; perplexity gibt Juli 2023 als cut-off-Datum an, Googles Gemini den Zeitraum Ende 2023 bis Anfang 2024. Zwar fließen neueste Onlineartikel mit in Antworten ein, aber je nach Ereignis fehlen substantielle Hintergrundinformationen aus der Zeit zwischen dem Abschluss des Trainings und einer Online-Suche einige Monate später.↩︎\nDas “Open” im Firmennamen OpenAI, das hinter ChatGPT steht, ist also nicht Programm. Der vollständige technische Report, der viele grundlegende Parameter auslässt, ist hier abrufbar. Zur Kritik am Vorgehen von OpenAI siehe u.a. einen Artikel von Leigh Mc Gowran. Eine Open-source-Alternative mit offenem Trainingsset und Modell ist z.B. dolly. Das LLM perplexity wirbt damit, mit Fußnoten auf genutzte Onlinequellen zu verweisen und die Ergebnisse somit nachvollziehbarer zu machen.↩︎\nChatGPT-4o-Antwort, Anfrage zu „Name ten medieval philosophers“, 3. September 2024.↩︎\nSiehe z.B. in Basel jetzt, 19.01.2023; NDR, 09.02.2023; bajour, 20.03.2023; 20 minuten, 24.03.2023.↩︎\nSiehe z.B. Pedagogical Educational Tutor (PET), entwickelt an der Universität Hohenheim, mit Einführungsvideo.↩︎\nU.a von Andrea Klein oder Ethan Mollick.↩︎",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Digital Literacy, Digital Criticism</span>"
    ]
  },
  {
    "objectID": "04_datenerhebung_analyse.html",
    "href": "04_datenerhebung_analyse.html",
    "title": "4  Datenerhebung, -aufbereitung und -analyse",
    "section": "",
    "text": "4.1 Datenerhebung\nEs gibt verschiedene Möglichkeiten, Daten für die historische Forschung zu erheben bzw. zu erstellen, von denen einige im Folgenden kurz angesprochen werden.\nFür Zeiträume, in denen Quellen vergleichsweise knapp sind und keine seriellen Daten existieren, bietet sich die Digitalisierung von Texten und deren anschließende Analyse an. Digitalisierung beinhaltet dabei nicht nur die Transformation von einer physischen Quelle in ein digitales Bild, sondern auch die Anreicherung des Bilds mit Layout und Text: Erst durch eine Markierung von Bereichen, in denen Text vorkommt, ist es in einem zweiten Schritt möglich, diesen als solchen zu erkennen und damit maschinenlesbar und auswertbar zu machen. Eine solche Umwandlung vom Bild zum Text ist dabei sowohl für moderne Texte, die als Typoskript vorliegen, als auch für vormoderne Handschriften und Drucke möglich, in lateinischer ebenso wie in arabischer, chinesischer oder japanischer Schrift. Es gibt kostenpflichtige Programme wie den Abbyy FineReader, aber auch Open-Source-Tools mit und ohne Graphical User Interface (GUI). Weit verbreitet ist Transkribus, das viele Funktionalitäten bündelt; die Texterkennung ist ab einer gewissen Menge Seiten allerdings kostenpflichtig, wobei studentische Projekte auf Anfrage unterstützt werden können. Programme, die über die Kommandozeile laufen, gänzlich kostenfrei sind und ebenfalls zahlreiche Funktionalitäten bieten, sind beispielsweise Kraken, OCR4all, OCRopus oder Calamari.\nZur Extraktion von Daten aus digitalen/digitalisierten Texten existieren verschiedene Möglichkeiten mithilfe kleiner Kommandozeilenprogramme (eher mühsam und schwierig zu lesen) oder mit Packages für Programmiersprachen, für die Geisteswissenschaften vor allem R oder Python (siehe dazu auch Kapitel 2.2). So können beispielsweise aus digitalisierten Telefonbüchern Entitäten, also Einheiten, wie Personen, Straßennamen oder Berufe oder aus alten Theaterprogrammheften gespielte Stücke, beteiligte Schauspieler:innen und verantwortliche Regisseurinnen extrahiert und als Datensätze weitergenutzt werden.1\nDer anfängliche Aufwand, der einer automatisierten Datenextraktion vorangeht, und die steile Lernkurve bei der Bedienung mancher Programme können abschreckend wirken. Wenn Sie nur ein Theaterprogramm detaillierter auswerten wollen, sind Sie sicher schneller, wenn Sie die entsprechenden Daten in eine Tabellensoftware abtippen. Wenn Sie aber einen größeren Quellenbestand zur Verfügung haben, der in sich ähnlich strukturiert ist, wie das bei Telefonbüchern oder einer Serie von Theaterprogrammheften der Fall sein dürfte, macht es kaum einen Unterschied mehr, ob Sie zehn oder hundert Theaterprogramme mithilfe eines Skripts analysieren möchten. Zudem können Sie Ihr erstelltes Skript, Ihr kleines Computerprogramm, anderen zur Verfügung stellen oder für ähnlich strukturierte Quellen in einem anderen Projekt nachnutzen.\nWenn Sie mit bereits digitalisierten Beständen aus öffentlichen Institutionen wie Galerien, Bibliotheken, Museen oder Archiven arbeiten wollen (sog. GLAMs: Galleries, Libraries, Archives, Museums), besteht oft die Möglichkeit, Daten über Schnittstellen herunterzuladen.2 Solche Schnittstellen, engl. API (Application Programming Interface), ermöglichen eine Kommunikation zwischen zwei Computern, ohne dass hierfür der Umweg über eine graphische Oberfläche nötig ist. Anstatt also beispielsweise über die Suchmaske der Staatlichen Museen zu Berlin nach Objekten oder Dokumenten mithilfe verschiedener Schlagwörter zu suchen und die Ergebnisse dann einzeln herunterzuladen, kann Ihr Computer mit der Schnittstelle des Museums direkt kommunizieren und mit einfachen Befehlen ganze Ergebnislisten zur Weiterarbeit herunterladen. Für solche Abfragen können ein Kommandozeilenprogramm oder Programmiersprachen genutzt werden, die Abfrage besteht dabei im Wesentlichen aus einer Zeile, wie hier in der Programmiersprache R:\nlibrary(jsonlite)\ncats &lt;- fromJSON(\"https://smb.museum-digital.de/json/objects?&s=katze\")\nDas Ergebnis der Suchanfrage nach “katze” wird in der Variable cats gespeichert, und diese kann zur Weiterarbeit in ein Tabellenformat exportiert werden:\nwrite.csv(cats, \"docs/cats_smb.csv\")\nDie Funktion write.csv speichert den Inhalt der Variable cats als csv-Datei3 unter dem Dateipfad “docs/cats_smb.csv” auf der Festplatte.\nWenn Webseiten keine Schnittstellen zur Verfügung stellen, besteht die Möglichkeit, mit Web Scraping an gewünschte Daten zu kommen. Je nach Webseite bzw. Inhalten ist die Rechtslage allerdings nicht ganz klar. Zum Download von Webseiten mit der Programmiersprache Python gibt es eine Lektion im Programming Historian von William J. Turkel und Adam Crymble. Ein weiteres Tutorial zur Datenakquise, von Zach Coble, Liz Rodrigues, Erin Pappas, Chelcie Rowell, und Yasmeen Shorish, findet sich hier.",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Datenerhebung, -aufbereitung und -analyse</span>"
    ]
  },
  {
    "objectID": "04_datenerhebung_analyse.html#datenerhebung",
    "href": "04_datenerhebung_analyse.html#datenerhebung",
    "title": "4  Datenerhebung, -aufbereitung und -analyse",
    "section": "",
    "text": "Wenn Sie die Schritte nachvollziehen möchten, können Sie R hier herunterladen. Wenn Sie das Programm öffnen, müssen Sie zuerst das Paket jsonlite installieren: install.packages(\"jsonlite\")\nMit “Enter” wird das Paket installiert.  Dann können Sie die zwei Zeilen oben eintippen und ebenfalls mit “Enter” ausführen. Die Ergebnisse Ihrer Suche können Sie sich mit\ncats + “Enter”\nanzeigen lassen.\n\n\n\n\n\n\n\nBeginn der Trefferliste für “katze” über die API der Staatlichen Museen zu Berlin\n\n\n\nUm Abfragen zu vermeiden, die die Server überlasten, haben die meisten APIs entweder eine Authentifizierung oder eine maximale Trefferanzahl pro Abfrage eingebaut. Beim obigen Beispiel erhalten Sie dadurch nicht die gesamte Trefferanzahl (134, aus der Spalte “total” ersichtlich), sondern nur die ersten 24 – diese Einstellungen haben die Entwickler:innen der Schnittstelle gemacht. Um dennoch alle Treffer mit einer Abfrage zu erhalten, müssten Sie die Dokumentation der API lesen und die Abfrage etwas modifizieren. Wenn Sie das interessiert, finden Sie Details in der Fußnote.4",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Datenerhebung, -aufbereitung und -analyse</span>"
    ]
  },
  {
    "objectID": "04_datenerhebung_analyse.html#datenaufbereitung",
    "href": "04_datenerhebung_analyse.html#datenaufbereitung",
    "title": "4  Datenerhebung, -aufbereitung und -analyse",
    "section": "4.2 Datenaufbereitung5",
    "text": "4.2 Datenaufbereitung5\nBei der Arbeit mit Datensätzen, seien sie selbst erhoben oder von Dritten übernommen, ist es häufig der Fall, dass Informationen fehlen oder uneinheitlich erhoben wurden, was eine spätere Analyse erschwert.\nWenn in einer Umfrage unter Studierenden das Studienfach mit in eine Tabelle aufgenommen wurde, ohne zuvor Werte für diese Kategorie zu definieren, finden sich für “Geschichte” und “Deutsch” vielleicht auch folgende Varianten: “Gesch.”, “Geschichtswissenschaft”, “Geschichtswissenschaften”, “Geschihcte”, “Germanistik”, “Dt.”, “Germ.”. Anstatt zwei Werten für zwei Studienfächer gibt es neun – ohne, dass sich das Fächerspektrum erweitert hätte. Im besten Fall werden solche Varianten schon bei der Erhebung der Daten vermieden, indem eine feste Liste an Werten erstellt wird. Erhält man jedoch einen Datensatz mit verschiedenen Varianten für ein und denselben Wert, muss man diese zusammenführen, um eine saubere Datengrundlage zu erhalten. Sie können entweder mit Strg-R versuchen, verschiedene Schreibweisen zu finden und zu ersetzen; in Tabellenprogrammen wie Excel, Open Office oder Google Sheets können Sie sich einzigartige Werte einzelner Spalten anzeigen lassen und zusammengehörende Varianten zu einem Grundwert zusammenführen; am hilfreichsten, recht voraussetzungslos zu bedienen und dabei auch für große Datensätze nutzbar ist die Software OpenRefine, mit der Sie Daten extrahieren,6 säubern/vereinheitlichen7 und anreichern8 können, um eine für Ihre Forschungsfrage und dafür notwendige Analysen sinnvolle Datengrundlage zu erhalten.\nFür Textdaten sind verschiedene Schritte zur Aufbereitung notwendig, je nachdem, welche Methode bzw. Software Sie nutzen möchten. Für die meisten Analysen ist es sinnvoll, mit sogenannten Stopword-Listen zu arbeiten. Stopwords sind Wörter, die vor einer Analyse aus einem Korpus entfernt werden, um aussagekräftigere Ergebnisse zu erhalten, gerade, wenn es um rein quantitative Methoden zur inhaltlichen Erschließung geht. Stopwords sind Wörter mit grammatikalischen Funktionen, die in großer Zahl in Dokumenten vorkommen, jedoch wenig Bedeutung tragen. Wenn man den unbearbeiteten Text dieses Guides nach Worthäufigkeiten auswertet, hier mit Voyant-Tools, lässt sich nur schwerlich erahnen, worum es geht – “digital” steht auf Platz 12, viel häufiger sind Artikel und Präpositionen. Mit Hilfe einer Stopword-Liste, die die häufigsten nicht-sinntragenden Wörter aus dem Text entfernt, wird der Inhalt klarer:\n\n\n\n\n\n\n\n\n\nWorthäufigkeiten roher Text\n\n\n\n\n \n\n\n\n\n\nWorthäufigkeiten ohne Stopwords\n\n\n\n\n\nWeitere Schritte beinhalten oft eine Tokenisierung, also die Segmentierung in Einheiten der Wortebene, und eine Lemmatisierung, also die Rückführung von verschiedenene Formen eines Worts auf eine Grundform – aus “ist”, “war” und “sind” wird “sein”. Wie bei den Schreibvarianten der Studienfächer haben die verschiedenen Flexionsformen für die meisten Forschungsfragen keinen Mehrwert und können zur weiteren Analyse zusammengeführt werden. Für solche vorbereitenden Schritte gibt es existierende Software und Packages für Programmiersprachen, sodass hier das Rad nicht neu erfunden werden muss, vor allem für moderne, weit verbreitete Sprachen, siehe auch Kapitel B.3. Schwieriger wird es für nicht-standardisierte Sprachen bzw. Sprachformen, also dialektal geprägte oder vormoderne Texte. Zwar gibt es auch hierfür Programme, die tatsächlich erreichte Präzision muss dabei jedoch je nach Quelle beurteilt werden.",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Datenerhebung, -aufbereitung und -analyse</span>"
    ]
  },
  {
    "objectID": "04_datenerhebung_analyse.html#datenanalyse",
    "href": "04_datenerhebung_analyse.html#datenanalyse",
    "title": "4  Datenerhebung, -aufbereitung und -analyse",
    "section": "4.3 Datenanalyse",
    "text": "4.3 Datenanalyse\nWenn Sie einen Datensatz zur Analyse zur Verfügung haben, aus selbst erhobenen Daten oder durch Nachnutzung eines vorhandenen, und für Ihre Zwecke aufbereitet haben, folgt (endlich) auch die Analyse. Welche Software oder Methoden Sie verwenden, hängt dabei nicht nur von der Art und Menge der Daten, sondern auch dem Datenformat und vor allem auch Ihrer Forschungsfrage ab. Wenn Sie eine Personendatenbank haben, in der Briefschreiber:innen und Empfänger:innen aufgenommen sind und der Wohnort der Personen bekannt ist, Sie es jedoch versäumt haben, die Datierungen der Einzelbriefe zu verzeichnen, können Sie nur eine räumliche Verteilung, keine raum-zeitliche Entwicklung eines Briefschreiber:innennetzwerks darstellen.9 Wenn Sie aber nur an der örtlichen Verteilung weiblicher und männlicher Verfasser:innen interessiert sind und die zeitliche Komponente für Sie keine Rolle spielt, erübrigt sich auch ein raum-zeitliche Analyse. Bevor Sie sich also für eine Methode entscheiden, sollten Sie sich fragen, zu welchem Zweck Sie Ihren Datensatz nutzen wollen und welche Frage(n) er beantworten soll.\nIn einem nächsten Schritt sollte über die konkrete Art der Analyse nachgedacht werden, die mit den vorhandenen Daten möglich ist. Unter den zahlreichen Möglichkeiten für die Arbeit mit strukturellen Daten sind für die Geschichtswissenschaften u.a. die Netzwerkanalyse oder die Regressionsanalyse häufig genutzte Methoden. Für textuelle Daten bieten sich ebenfalls verschiedene Arten der Analyse an, darunter beispielsweise Auszählungen von Worthäufigkeiten als Teil der Stylometrie/Zuschreibung von Autor:innenschaft (siehe Kapitel 2.2), Topic Modeling als statistische Methode zur Identifizierung wiederkehrender Themen in größeren Textbeständen, oder Sentimentanalyse, um Stimmungen, Gefühle, Bewertungen aus Textpassagen zu extrahieren. Wenn Sie über georeferenzierte Daten verfügen, können Sie verschiedene Analysen mithilfe von GIS (Geographic Information System) durchführen und visualisieren.\nOb Sie für Topic Modeling ein eigenes Skript schreiben oder vorhandene Software nutzen, ob Sie Regressionsanalysen selbst durchführen oder auf Webseiten durchführen lassen, ist dabei Ihre Entscheidung; oftmals ist das Nutzen vorhandener Webangebote für erste kurze Analysen sinnvoll, um zu überlegen, ob die vorgesehene Methode überhaupt sinnvolle Ergebnisse liefern kann. Für größere Projekte, in denen komplexere Analysen über einen längeren Zeitraum durchgeführt werden sollen, bietet sich die Arbeit mit Programmiersprachen schon allein deswegen an, weil so ein sehr hohes Maß an Anpassungen von vorhandenen Funktionen für die eigenen Zwecke und die völlige Kontrolle über die eigenen Daten ermöglicht wird. Eine Auflistung häufig genutzter Tools für die historische Arbeit findet sich in Kapitel B.3.",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Datenerhebung, -aufbereitung und -analyse</span>"
    ]
  },
  {
    "objectID": "04_datenerhebung_analyse.html#sec-sicherung",
    "href": "04_datenerhebung_analyse.html#sec-sicherung",
    "title": "4  Datenerhebung, -aufbereitung und -analyse",
    "section": "4.4 Datensicherung",
    "text": "4.4 Datensicherung\nIn Kapitel 5 wird es um Fragen zur nachhaltigen Speicherung von Forschungsdaten gehen; an dieser Stelle sei darauf hingewiesen, dass die Sicherung von Daten am besten auch mit einer Versionierung und mit einer Dokumentation einhergeht. Datenversionierung hat den Vorteil, dass Schritte wieder rückgängig gemacht werden können, Datensätze in unterschiedlichen Stadien gespeichert und für eine spätere Weiterarbeit genutzt werden können und einzelne Schritte einzelnen Projektmitarbeiter:innen zugeschrieben werden können. Zusätzliche Versionierung geht dabei über die Funktionalitäten von Backup-Programmen oder Cloudspeichern wie Dropbox oder Switchdrive hinaus, und für Einzelprojekte wie auch für kollaboratives Arbeiten hat sich in der Wissenschaft wie in der Wirtschaft git etabliert, häufig in Kombination mit Daten-/Coderepositorien auf GitHub. Die meisten von Ihnen werden vermutlich keine eigenen GitHub-Repositorien anlegen, aber das System dennoch irgendwann nutzen, am ehesten durch den Download von dort zur Verfügung gestellten Daten – die Textdaten für diesen Guide liegen auch in einem GitHub-Repositorium. Die Dokumentation von gespeicherten Daten schließlich beinhaltet Informationen zur Entstehung des Datensatzes: Wie und von wem wurden die Daten erhoben? Wie wurden sie annotiert? In welchem Format sind die Daten vorhanden? Welche Software wurde an welcher Stelle benutzt? Was stellen die Daten dar? Die Sicherung von Daten an mehreren Orten, bspw. auf der lokalen Festplatte, in einem Cloudspeicher und auf einem USB-Stick, schützt sicher vor Datenverlust. Eine Dokumentation und die Sicherung in einem Repositorium, einem Langzeitspeicher für Daten, sorgt zusätzlich für Sichtbarkeit und die Möglichkeit zur Nachnutzung von Ergebnissen. Als Fachrepositorien für die Geisteswissenschaften existieren beispielsweise DARIAH-DE oder das DaSCH, es gibt spezialisiertere Repositorien wie AMAD (Mittelalter), oder für alle Disziplinen offene wie Zenodo (fächerübergreifend, betrieben durch das CERN). Sie können Ihre Forschungsdaten dort kostenfrei ablegen, Ihre Urheberschaft nachweisen und die Daten/Publikation mit einem Digital Object Identifier (DOI), also einem eindeutigen und dauerhaften digitalen Identifikator, nachhaltig zitierbar machen.",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Datenerhebung, -aufbereitung und -analyse</span>"
    ]
  },
  {
    "objectID": "04_datenerhebung_analyse.html#footnotes",
    "href": "04_datenerhebung_analyse.html#footnotes",
    "title": "4  Datenerhebung, -aufbereitung und -analyse",
    "section": "",
    "text": "Ein gut nachvollziehbares Tutorial zur Extraktion von Daten aus Telefonbüchern hat Lindsey Wieck für einen DH-Kurs an der St. Mary’s University in San Antonio erstellt: https://lindseywieck.com/fall_2016_sf/gatheringdatatutorial.html. Derek Miller arbeitet zu Broadway-Vorstellungen, Visualizing Broadway, ein Projekt, das hier beschrieben wird; hier gibt es dazu ein Video in Vorlesungslänge.↩︎\nUnter openglam.ch finden sich Informationen zu Schweizer GLAM-Einrichtungen, die offene Daten anbieten.↩︎\ncomma separated value ist ein Format, in dem einzelne Werte, values, über spezifische Trenner, meist commas, eindeutig abgrenzbar sind und somit in einem Tabellenformat angezeigt werden können, wobei jeder Wert in einer separaten Zelle steht. Tabellensoftware wie Excel, Google Sheets oder Numbers kann csv-Dateien öffnen.↩︎\nDie API aus dem Beispiel ist so konfiguriert, dass bei Abfragen mit Ergebnissen über 24 Treffern nur die ersten 24 ausgegeben werden; das ist etwas ungewöhnlich, aber wir können damit umgehen, indem wir die maximale Trefferausgabe pro Anfrage auf 10 setzen – diese Zahl ist nicht zu hoch, und wir können gut damit rechnen. Der Parameter für die maximale Trefferzahl kann mit &breitenat=10 eingestellt werden. Den Startpunkt der Ausgabe kann man mit dem Parameter &startwert= ändern. Um also alle Treffer für eine Abfrage zu erhalten, können wir die Ergebnisse in 10er-Schritten abfragen und anschließend zusammenfügen. Damit das nicht zu einer copy-paste-Aktion wird, müssen wir etwas ausführlicher formulieren bzw. mehrere Variablen verwenden. Das hat aber den Vorteil, dass man auf diese Art dann nach jedem Begriff suchen kann.\n\nbase_URL &lt;- \"https://smb.museum-digital.de/json/objects?&s=katze\"\ncats &lt;- fromJSON(base_URL)\nstart &lt;-  0\nbreite &lt;- 10\niterations &lt;- cats$total[1]%/%10 + 1\nendsize &lt;- cats$total[1]-(iterations-1) * 10\ncat_list &lt;- data.frame()\nfor (i in 1:iterations){\n  if(i &lt; iterations){\ncat_list &lt;- rbind(cat_list, fromJSON(paste(base_URL, \"&gbreitenat=10&startwert=\", start , sep=\"\")))\n  } else {\ncat_list &lt;- rbind(cat_list,fromJSON(paste(base_URL, \"&gbreitenat=\", \n                                              endsize, \"&startwert=\", start, sep=\"\")))\n  }\n  start &lt;- start + 10\n  write.csv(cat_list, \"Desktop/cat_list.csv\")\n}\nZuerst machen wir den Code übersichtlicher und speichern den Großteil der URL in base_URL:\nbase_URL &lt;- \"https://smb.museum-digital.de/json/objects?&s=katze\"\nDie Ergebnisse der Suchanfrage werden wieder im Objekt cats gespeichert:\ncats &lt;- fromJSON(base_URL)\nDie Anzahl Durchgänge für eine Abfrage ergibt sich aus der Anzahl der totalen Treffer/10 + 1; die Anzahl der Treffer lässt sich aus der Spalte “total” im Objekt cats entnehmen. In R formuliert man das so:\ncats$total[1]\nIm Katzenbeispiel sind es 134 Gesamttreffer, also (134/10 ohne Rest)+1, also 14 Durchgänge:\niterations &lt;- cats$total[1]%/%10 + 1\nDann setzt man den Startwert auf 0:\nstart &lt;-  0\nUnd die Maximaltreffer auf 10:\nbreite &lt;- 10\nDie letzte Iteration muss dabei nicht die nächsten 10 Treffer abfragen, sondern nur noch 4 (die letzten 4 nach 130):\nendsize &lt;- cats$total[1]-(iterations-1) * 10\nDann erstellen wir eine leere Tabelle, einen data frame, die wir mit unseren Anfragen nach und nach befüllen. (Bei kleinen Datenmengen kann die Funktion rbind zur Verbindung von Einzeltabellen genutzt werden; bei größeren Datenmengen ist das iterative Verlängern von data frames nicht empfohlen):\ncat_list &lt;- data.frame()\nWenn wir diese Variablen festgelegt haben, können wir einen Loop, eine Schleife bauen, die unter bestimmten Bedingungen verschiedene Aktionen ausführt:\nfor (i in 1:iterations){\nFalls die letzte Iteration noch nicht erreicht ist, wird die Abfrage in 10er-Schritten durchgeführt, wobei der Startwert bei jedem Durchgang um 10 verschoben wird und die Ergebnisse hintereinander in cat_list geschrieben werden.\nif(i &lt; iterations){\ncat_list &lt;- rbind(cat_list, fromJSON(paste(base_URL, \"&gbreitenat=10&startwert=\", start , sep=\"\")))\n} else {\nSobald die letzte Iteration erreicht ist, werden nicht mehr die nächsten 10, sondern so viel Treffer, wie in endsize gespeichert, abgefragt, in unserem Beispiel 4:\ncat_list &lt;- rbind(cat_list,fromJSON(paste(base_URL, \"&gbreitenat=\",  endsize, \"&startwert=\", start, sep=\"\")))\n}\nstart &lt;- start + 10\nZum Schluss, in diesem Fall nach 14 Iterationen, wird die Tabelle in eine Datei geschrieben:\nwrite.csv(cat_list, \"Desktop/cat_list.csv\")\n}↩︎\nEine häufige Aussage ist, zur Datenvorbereitung/Preprocessing würde 80% der Arbeitszeit verwendet, zur Analyse und Interpretation blieben nur 20%. In einem Blogartikel von 2020 geht Leigh Dodds diesen Zahlen nach – ganz so dramatisch ist das Verhältnis in Wahrheit wohl nicht.↩︎\nEvan Peter Williamson: Fetching and Parsing Data from the Web with OpenRefine, Programming Historian 6 (2017), https://doi.org/10.46430/phen0065.↩︎\nSeth van Hooland, Ruben Verborgh, Max De Wilde: Cleaning Data with OpenRefine, Programming Historian 2 (2013), https://doi.org/10.46430/phen0023.↩︎\nKaren Li-Lun Hwang: Enriching Reconciled Data with OpenRefine, The Bytegeist Blog 2018, https://medium.com/the-bytegeist-blog/enriching-reconciled-data-with-openrefine-89b885dcadbb↩︎\nEin Großprojekt an der Universität Stanford, “Mapping the Republic of Letters”, hat für das 18. Jahrhundert das Briefnetzwerk europäischer Gelehrter modelliert. Ein Fallbeispiel ist das Netzwerk Voltaires, in verschiedenen Visualisierungen: http://republicofletters.stanford.edu/publications/voltaire/letters/. Dan Edelstein. Interactive Visualization for Voltaire’s Correspondence Network. Letters in Voltaire’s Network [Created using Palladio, http://hdlab.stanford.edu/palladio].↩︎",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Datenerhebung, -aufbereitung und -analyse</span>"
    ]
  },
  {
    "objectID": "05_FAIR_CARE.html",
    "href": "05_FAIR_CARE.html",
    "title": "5  FAIR, CARE und LOUD",
    "section": "",
    "text": "5.1 FAIRe Daten\nDie Prinzipien FAIRer Daten wurden 2016 von einem Konsortium aus Wissenschaftler:innen und Organisationen wie folgt definiert:1 Findability, Accessibility, Interoperability, Reuse of digital assets.\nDaten sollen also auffindbar und zugänglich sein, zudem interoperabel, also mit verschiedenen Systemen nutzbar, und wiederverwendbar. Wenn Sie für eine Proseminararbeit zehn Testamente aus dem 18. Jahrhundert im Staatsarchiv Basel fotografieren, anschließend transkribieren, die vererbten Gegenstände identifizieren, zwischen den Erblasser:innen vergleichen und Ihre Ergebnisse ausgedruckt bei dem:r Dozierenden einreichen, sind Ihre Daten das genaue Gegenteil: Niemand weiß, dass Sie die Daten erhoben haben, sie sind über gängige Suchmethoden nicht auffindbar und nur über persönliche Kontakte zugänglich; und wenn Ihr:e Dozent:in Ihre Ergebnisse anderen Studierenden zur Verfügung stellen will, um weitere Forschung anzuregen, geht dies nur in Form von Kopien Ihrer gedruckten Arbeit; Papierkopien sind dabei weder interoperabel noch sind Ihre Daten vernünftig wiederverwendbar – sie müssten via Abtippen erst wieder maschinenlesebar gemacht werden, um damit weiterarbeiten zu können. Wenn Sie Ihre transkribierten Texte und die identifizierten Objekte in Standardformaten und mit offener Lizenz auf einem Repositorium veröffentlichen, machen Sie nicht nur wichtige Teile Ihrer eigenen Arbeit sichtbar, sondern erleichtern so auch anschließende Forschungen.2 Zudem kann so vermieden werden, dass geleistete Arbeit wie beispielsweise Transkriptionen doppelt gemacht wird.3",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>FAIR, CARE und LOUD</span>"
    ]
  },
  {
    "objectID": "05_FAIR_CARE.html#care-prinzipien",
    "href": "05_FAIR_CARE.html#care-prinzipien",
    "title": "5  FAIR, CARE und LOUD",
    "section": "5.2 CARE-Prinzipien",
    "text": "5.2 CARE-Prinzipien\nAnschließend an die FAIR-Prinzipien wurden 2019 von der Global Indigenous Data Alliance die CARE-Prinzipien für den Umgang mit indigenen Daten formuliert:4 Collective Benefit, Authority to Control, Responsibility, Ethics.\nDas Augenmerk liegt dabei darauf, nicht einfach offene Daten und Datenaustausch zu propagieren, sondern auch die Menschen und den Zweck zu berücksichtigen, um bestehende Machtunterschiede zwischen verschiedenen Akteur:innen nicht zu verstärken. Indigene Daten sollen dem kollektiven Nutzen dienen, ein Recht auf Kontrolle soll gegeben sein, Verantwortung für die Datennutzung übernommen und Ethische Prinzipien beachtet werden.5 Auch wenn diese Richtlinien speziell für die Arbeit mit indigenen Daten ausgearbeitet wurden, ergänzen sie den datenzentrierten Ansatz der FAIR-Prinzipien um eine Dimension, die den Entstehungskontext der Daten mitberücksichtigt und zur Reflexion über die (Weiter-)Arbeit mit Daten anregt.",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>FAIR, CARE und LOUD</span>"
    ]
  },
  {
    "objectID": "05_FAIR_CARE.html#loud",
    "href": "05_FAIR_CARE.html#loud",
    "title": "5  FAIR, CARE und LOUD",
    "section": "5.3 LO(U)D",
    "text": "5.3 LO(U)D\nTim Berners-Lee, der Erfinder des World Wide Web, propagierte früh die Verlinkbarkeit von standardisierten digitalen Daten und dadurch die Entstehung eines Semantic Web, in dem Daten mensch- und maschinenlesbar austauschbar sind:\n\nThe Semantic Web isn’t just about putting data on the web. It is about making links, so that a person or machine can explore the web of data. With linked data, when you have some of it, you can find other, related, data.6\n\nUm andere Daten nicht nur finden, sondern auch weiternutzen und kombinieren zu können, wurden auch für Linked Open Data Prinzipien formuliert, die bei der Erstellung bzw. Veröffentlichung von Daten beachtet werden sollten; bekannte Beispiele für LOD-Datensätze sind Wikibase, Wikidata oder GeoNames.\nBerners-Lee hat ein Fünf-Sterne-System zur Klassifizierung offener Datensätze vorgeschlagen, wobei 5 Sterne Linked Open Data entspricht:\n1 Stern: Ein Datensatz ist offen verfügbar, in einem beliebigen Format, etwa als PDF-Datei.\n2 Sterne: Ein Datensatz ist offen verfügbar in einem strukturierten Format, etwa im Dateiformat von Microsoft Excel (.xls).\n3 Sterne: Ein Datensatz ist offen verfügbar in einem nicht-proprietären strukturierten Format, etwa als Comma-separated values (.csv).\n4 Sterne: Ein Datensatz folgt Standards des World Wide Web Konsortiums (W3C), wie die Nutzung des Resource Description Frameworks (RDF) und die Vergabe von Uniform Resource Identifiern (URIs).\n5 Sterne: Ein Datensatz erfüllt alle obigen Bedingungen und enthält zudem Links zu anderer Linked Open Data.\nÜber das Fünf-Sterne-System hinaus wurden Grundsätze für Linked Open Usable Data entwickelt, die darauf abzielen, Daten nicht nur in offener und verlinkter Form zur Verfügung zu stellen, sondern sie auch verständlich zu strukturieren und zu dokumentieren, um ihre Nutzbarkeit zu verbessern.\nÜber Dinge wie das W3C, RDF und URIs müssen Sie sich im Verlauf Ihres Studiums vermutlich keine Gedanken machen, aber behalten Sie im Auge, dass Ihre Forschung(sdaten) sichtbarer und nachhaltig nachnutzbarer sind, wenn Sie nicht bereits bei der Erstellung einer Datei ein Format wählen, das nur auf einem bestimmten Betriebssystem läuft oder den Erwerb eines kostenpflichtigen Programms erfordert.",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>FAIR, CARE und LOUD</span>"
    ]
  },
  {
    "objectID": "05_FAIR_CARE.html#footnotes",
    "href": "05_FAIR_CARE.html#footnotes",
    "title": "5  FAIR, CARE und LOUD",
    "section": "",
    "text": "Wilkinson, Mark D.; Dumontier, Michel; Aalbersberg, IJsbrand Jan u. a.: The FAIR Guiding Principles for scientific data management and stewardship, in: Scientific Data 3 (1), 03.2016, S. 160018. Online: &lt;https://doi.org/10.1038/sdata.2016.18&gt;, Stand: 09.11.2022.↩︎\nZu verschiedenen Repositorien, also digitalen Speicherorten für Daten, vgl. das Unterkapitel Kapitel 4.4.↩︎\nDie Plattform transcriptiones ermöglicht niederschwelliges Teilen von Transkriptionen historischer Dokumente.↩︎\nCarroll, Stephanie Russo; Garba, Ibrahim; Figueroa-Rodríguez, Oscar L. u. a.: The CARE Principles for Indigenous Data Governance, in: Data Science Journal 19, 11.2020, S. 43. Online: &lt;https://doi.org/10.5334/dsj-2020-043&gt;, Stand: 28.11.2022.↩︎\nEine auf deutsch übersetzte Fassung der CARE-Prinzipien findet sich hier.↩︎\nTim Berners-Lee: Linked Data, 2009. Online: https://www.w3.org/DesignIssues/LinkedData.html.↩︎",
    "crumbs": [
      "Welcome",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>FAIR, CARE und LOUD</span>"
    ]
  },
  {
    "objectID": "Praxis.html#footnotes",
    "href": "Praxis.html#footnotes",
    "title": "Praxis",
    "section": "",
    "text": "DER STURM. Digitale Quellenedition zur Geschichte der internationalen Avantgarde, erarbeitet und herausgegeben von Marjam Trautmann und Torsten Schrade. Mainz, Akademie der Wissenschaften und der Literatur, 2018. Online: https://sturm-edition.de/, Lizenz: CC-BY-4.0.↩︎",
    "crumbs": [
      "Praxis"
    ]
  },
  {
    "objectID": "06_Briefedition_Sturm.html#footnotes",
    "href": "06_Briefedition_Sturm.html#footnotes",
    "title": "6  Briefedition ‘Der Sturm’",
    "section": "",
    "text": "Online unter https://sturm-edition.de/quellen/briefe/fma/Q.01.19140414.FMA.01.html↩︎",
    "crumbs": [
      "Praxis",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Briefedition 'Der Sturm'</span>"
    ]
  },
  {
    "objectID": "07_Vordereingang.html#footnotes",
    "href": "07_Vordereingang.html#footnotes",
    "title": "7  Durch den Vordereingang",
    "section": "",
    "text": "Lemercier, Claire; Zalc, Claire: Quantitative Methods in the Humanities. An Introduction, Charlottesville 2019, S. 57–60.↩︎",
    "crumbs": [
      "Praxis",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Durch den  Vordereingang</span>"
    ]
  },
  {
    "objectID": "08_Hintertuer.html",
    "href": "08_Hintertuer.html",
    "title": "8  Durch die Hintertür",
    "section": "",
    "text": "8.1 Shell 101\nIn einem Terminal/einer Shell – zur Unterscheidung siehe Fußnote 1 – können Kommandos bzw. Programme ausgeführt werden, die auf der Strukturebene stattfinden – wie beispielsweise das Löschen einer Datei, rm Dateiname.xyz (rm für remove), oder das Erstellen eines Ordners, mkdir NeuerOrdner (mkdir für make directory). Ebenso möglich sind Operationen auf der Inhaltsebene – wie beispielsweise das Suchen eines bestimmten Begriffs in einer Textdatei, grep 'Begriff' Textdatei.txt (Mac/Linux) bzw. Select-String -Path Textdatei.txt -Pattern 'Begriff' (Windows), oder das Auszählen mehrerer Begriffe und das Speichern des Ergebnisses in einer neuen Datei, grep -Ec '(Begriff1|Begriff2)' Textdatei.txt | wc -l &gt; Ergebnisse.txt (Mac/Linux) bzw. (Select-String -Path Textdatei.txt -Pattern '(Begriff1|Begriff2)'.Matches.Count &gt; Ergebnisse.txt (Windows) – die Kommandos werden weiter unten nochmals einzeln erklärt.\nWoher weiss Ihre Shell aber, was sie ausführen soll, wenn Sie rm oder grep/String-Select eintippen? Es gibt zahlreiche Shell-Programme, die bereits auf Ihrem System vorinstalliert sind, und mit denen Sie vieles tun können – öffnen Sie Ihre Shell, tippen Sie date ein und drücken ‘Enter’: Das aktuelle Datum mit Uhrzeit erscheint. (Ihre Shell sucht nach dem ersten Argument, dem Befehl date, im Filesystem des Computers, und wenn sie fündig wird, führt sie eine Aktion mit den entsprechenden Parametern aus.)\nFalls Sie einen Befehl eintippen, den es nicht gibt bzw. für den kein installiertes Programm auf Ihrem Computer existiert, bekommen Sie eine simple Fehlermeldung – kaputtgehen kann dabei nichts:\nDer Output auf Windows ist etwas ausführlicher:\nDas aktuelle Datum wird Ihnen wahrscheinlich auch in Ihrer Toolbar angezeigt, und einen neuen Ordner können Sie per Rechtsklick erstellen, dazu brauchen Sie das Terminal nicht unbedingt. Um einen Begriff in einem Textdokument zu finden und alle Vorkommen zu zählen, können Sie das Dokument öffnen, Strg-F drücken, den Begriff eingeben und das Ergebnis sehen. Wenn Sie nach mehreren Begriffen suchen wollen, müssen Sie dieselbe Aktion zweimal ausführen: Strg-F, Begriff 2. Und wenn Sie mehrere Dateien durchsuchen möchten, beispielsweise um herauszufinden, wie oft die Grußformel “Mit herzlichem Gruß” in einer Briefsammlung vorkommt, müssen Sie die Suche in jeder Datei einzeln ausführen. Wenn Sie dann noch nach der Variante “Mit herzlichen Grüßen” oder gar “Herzl. Gruß” suchen wollen, vervielfacht sich Ihre Arbeit.\nSie können dasselbe auch mit dem Terminal machen und einige der Built-in-Programme nutzen, um sich Zeit und Arbeit zu sparen.",
    "crumbs": [
      "Praxis",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Durch die Hintertür</span>"
    ]
  },
  {
    "objectID": "08_Hintertuer.html#shell-101",
    "href": "08_Hintertuer.html#shell-101",
    "title": "8  Durch die Hintertür",
    "section": "",
    "text": "Hinweis\n\n\n\ntmi: Wenn Sie echo $PATH im Terminal (Mac/Linux) bzw. $env:PATH (Windows) eingeben, sehen Sie eine Auflistung der Orte, an denen nach Befehlen gesucht wird. Tippen Sie which date ein und drücken Sie ‘Enter’, um zu sehen, wo das Programm “date” in Ihrem Computer liegt.\n\n\n\n(base) serina00@dg-19-mac-02 ~ % nonsense\ncommand not found: nonsense\n\nnonsense: The term 'nonsense' is not recognized as a name of a cmdlet, function, script file, or executable program.\nCheck the spelling of the name, or if a path was included, verify that the path is correct and try again.",
    "crumbs": [
      "Praxis",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Durch die Hintertür</span>"
    ]
  },
  {
    "objectID": "08_Hintertuer.html#strg-f-2.0",
    "href": "08_Hintertuer.html#strg-f-2.0",
    "title": "8  Durch die Hintertür",
    "section": "8.2 Strg-F 2.0",
    "text": "8.2 Strg-F 2.0\nWir arbeiten wie im vorangegangenen Kapitel mit einem Teilkorpus der Quellenedition “Der Sturm”, nämlich mit allen Briefen, die von Franz Marc verfasst wurden. Um die folgenden Schritte nachzuvollziehen, laden Sie sich den Ordner “letters_Der_Sturm” herunter. Sie können dazu entweder das vollständige GitHub-Repository zu diesem Guide als Zip-Datei herunterladen und entpacken, und im Ordner “docs” befindet sich der Ordner “letters_Der_Sturm”.\n\n\n\nGitHub-Repositorium mit Quellcode, Download via “Code” und “Download ZIP”.\n\n\nSie können das Repositorium auch über die Kommandozeile klonen\ngit clone https://github.com/wissen-ist-acht/digitalhistory.intro.git\noder als bequeme Variante diesen Direktlink nutzen.\nWenn Sie die Schnittstelle der Webseite ausprobieren wollen, kommen Sie mit wenigen Kommandos an die Dateien.\n\n\n\n\n\n\nDownload per Schnittstelle (für Mac/Linux)\n\n\n\n\n\nMit dem ersten Kommando erstellen wir eine Datei “briefe_marc.xml” mit den Dateinamen aller Briefe, die von Franz Marc geschrieben wurden – über das Register auf der Webseite wissen wir, dass er die Personen-ID P.0000003 hat; die URL zur Abfrage der Schnittstelle können wir der Dokumentation entnehmen:\ncurl https://sturm-edition.de/api/persons/P.0000003 --output briefe_marc.xml\nWenn Sie die Datei mit einem Editor öffnen, der XML-Dateien lesen kann, sehen Sie, dass neben den Dateinamen, die nach “target=” stehen, noch viel Beifang ist, den wir loswerden möchten:\n&lt;person xmlns=\"http://www.tei-c.org/ns/1.0\" source=\"http://d-nb.info/gnd/11857745X\" xml:id=\"P.0000003\"&gt;\n    &lt;persName type=\"pref\"&gt;Marc, Franz&lt;/persName&gt;\n    &lt;persName type=\"fn\"&gt;Franz Marc&lt;/persName&gt;\n    &lt;linkGrp type=\"files\"&gt;\n        &lt;ptr n=\"Bl.375\" target=\"Q.01.19191212.JVH.01.xml\"/&gt;\n        &lt;ptr n=\"Bl.377\" target=\"Q.01.19200114.JVH.01.xml\"/&gt;\n        &lt;ptr n=\"Bl.219\" target=\"Q.01.19160128.FMA.01.xml\"/&gt;\n        &lt;ptr n=\"Bl.222\" target=\"Q.01.19160205.FMA.01.xml\"/&gt;\n        &lt;ptr n=\"Bl.223\" target=\"Q.01.19160302.FMA.01.xml\"/&gt;\n        &lt;ptr n=\"Bl.218\" target=\"Q.01.19160101.FMA.01.xml\"/&gt;\n        &lt;ptr n=\"Bl.221\" target=\"Q.01.19160122.FMA.01.xml\"/&gt;\n        &lt;ptr n=\"Bl.220\" target=\"Q.01.19160115.FMA.01.xml\"/&gt;\n        &lt;ptr n=\"Bl.207\" target=\"Q.01.19150703.FMA.01.xml\"/&gt;\n        ...\nDenn eigentlich brauchen wir nur die Dateinamen, um die Dateien mit einem entsprechenden Befehl herunterladen zu können. Auch sehen wir, dass nicht nur Briefe mit dem Kürzel “FMA” für Franz Marc gelistet sind, sondern auch neun mit “JVH”, Jacoba van Heemskerck. Beim Blick in die betreffenden Dateien zeigt sich, dass in diesen Franz Marc jeweils erwähnt und im TEI-XML mit persName key=\"P.0000003\" getaggt wurde, daher Teil des Abfrageergebnisses ist. Mit dem zweiten Kommando erstellen wir eine neue Datei, in der die einzelnen extrahierten Dateinamen ohne diejenigen von Jacoba van Heemskerck mit dem Download-Kommando curl kombiniert und um die entsprechende URL zum Download ergänzt werden:\ncat briefe_marc.xml | grep -o 'Q.*FMA.*.xml\\b' | perl -nle 'print \"curl -o $_ https://sturm-edition.de/api/files/$_ \"' &gt; dateinamen_briefe_marc.txt\nDie Datei “dateinamen_briefe_marc.txt” sieht so aus:\ncurl -o Q.01.19160128.FMA.01.xml https://sturm-edition.de/api/files/Q.01.19160128.FMA.01.xml \ncurl -o Q.01.19160205.FMA.01.xml https://sturm-edition.de/api/files/Q.01.19160205.FMA.01.xml \ncurl -o Q.01.19160302.FMA.01.xml https://sturm-edition.de/api/files/Q.01.19160302.FMA.01.xml \ncurl -o Q.01.19160101.FMA.01.xml https://sturm-edition.de/api/files/Q.01.19160101.FMA.01.xml \ncurl -o Q.01.19160122.FMA.01.xml https://sturm-edition.de/api/files/Q.01.19160122.FMA.01.xml \ncurl -o Q.01.19160115.FMA.01.xml https://sturm-edition.de/api/files/Q.01.19160115.FMA.01.xml \ncurl -o Q.01.19150703.FMA.01.xml https://sturm-edition.de/api/files/Q.01.19150703.FMA.01.xml \ncurl -o Q.01.19150417.FMA.01.xml https://sturm-edition.de/api/files/Q.01.19150417.FMA.01.xml \ncurl -o Q.01.19151106.FMA.01.xml https://sturm-edition.de/api/files/Q.01.19151106.FMA.01.xml \ncurl -o Q.01.19150918.FMA.01.xml https://sturm-edition.de/api/files/Q.01.19150918.FMA.01.xml \ncurl -o Q.01.19150827.FMA.01.xml https://sturm-edition.de/api/files/Q.01.19150827.FMA.01.xml \ncurl -o Q.01.19150303.FMA.01.xml https://sturm-edition.de/api/files/Q.01.19150303.FMA.01.xml \n...\ncat briefe_marc.xml gibt den Inhalt der Datei ins Terminal; grep -o 'Q.*FMA.01.xml\\b' findet in diesem Inhalt alle Zeichenketten zwischen “Q” und “FMA.01.xml”, wobei nach “xml” durch das Hinzufügen von \\b das Zeichenende angezeigt ist; die 45 gefundenen Zeichenketten werden in je eine neue Zeile geschrieben, wobei mit curl -o $_ der Befehl curl -o und mit $_ als Platzhalter die Zeichenkette (also der Dateiname) geschrieben wird, gefolgt von https://sturm-edition.de/api/files/$_ und mit $_ wieder die Zeichenkette (also wieder der Dateiname). Mit einem dritten Kommando, bash, führen wir die erstellte Datei aus, d.h. die in ihr stehenden Kommandos werden ausgeführt – also via curl (Client URL) die Briefe heruntergeladen.\nbash dateinamen_briefe_marc.txt\n\n\n\nEgal, wie Sie die Dateien heruntergeladen haben, sollten Sie 45 Briefe im xml-Format vorfinden. Öffnen Sie dann das Terminal (Mac/Linux) bzw. die PowerShell (Windows) und bewegen sich mit cd, also change directory, in den Ordner (directory), in dem Ihre Textdateien liegen.\nIn meinem Fall ist das unter Documents/GitHub/digital_history_intro/docs/letters_Der_Sturm.\n(base) serina00@dg-19-mac-02 ~ % cd Documents/GitHub/digital_history_intro/docs/letters_Der_Sturm`\nBei den meisten von Ihnen ist das vermutlich unter “Downloads” – probieren Sie es aus.\n(Um zu prüfen, was in einem Ordner liegt, können Sie im Terminal ls (für list) eingeben, bzw. in der PowerShell dir (für directory)):\nls\nQ.01.19140115.FMA.01.xml    Q.01.19150315.FMA.02.xml\nQ.01.19140119.FMA.01.xml    Q.01.19150327.FMA.01.xml\nQ.01.19140121.FMA.01.xml    Q.01.19150417.FMA.01.xml\nQ.01.19140124.FMA.01.xml    Q.01.19150501.FMA.01.xml\nQ.01.19140125.FMA.01.xml    Q.01.19150615.FMA.01.xml\nQ.01.19140125.FMA.02.xml    Q.01.19150703.FMA.01.xml\nQ.01.19140409.FMA.01.xml    Q.01.19150710.FMA.01.xml\nQ.01.19140414.FMA.01.xml  Q.01.19140421.FMA.01.xml\nQ.01.19150827.FMA.01.xml  Q.01.19140507.FMA.01.xml\n...",
    "crumbs": [
      "Praxis",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Durch die Hintertür</span>"
    ]
  },
  {
    "objectID": "08_Hintertuer.html#erste-schritte",
    "href": "08_Hintertuer.html#erste-schritte",
    "title": "8  Durch die Hintertür",
    "section": "8.3 Erste Schritte",
    "text": "8.3 Erste Schritte\nWenn Sie in den Ordner navigiert sind, in dem die Briefdateien liegen, können Sie mit einem einzeiligen Kommando die Suchvorgänge nach einer Grußformel, die Sie in einem Texteditor nacheinander mit Strg-F mit jeder einzelnen Datei ausführen würden, im Terminal/in der PowerShell mit dem Programm grep (Global Regular Expression Print, Mac/Linux) bzw. Select.String (Windows) für alle Briefe in diesem Ordner auf einmal vornehmen, indem Sie alle Dateien, die auf “.xml” enden, in die Suche aufnehmen. Die Ergebnisse – bei dieser Suche nach den Grußformeln “Mit herzlichem Gruß” oder “Mit herzlichen Grüßen” ein Treffer in einem Brief – können Sie sich im Terminal/in der PowerShell anschauen:\nMac/Linux:\ngrep -E -i '(Mit herzlichem Gruß|Mit herzlichen Grüßen)' *.xml \nWindows:\nSelect-String -Path *.xml -Pattern \"(Mit herzlichem Gruß|Mit herzlichen Grüßen)\"\nOutput:\nQ.01.19160115.FMA.01.xml:               &lt;salute&gt;Mit herzlichen Grüßen für Sie beide&lt;/salute&gt; &lt;signed&gt;Ihr &lt;persName key=\"P.0000003\" ref=\"http://d-nb.info/gnd/11857745X\"&gt;F.Marc&lt;/persName&gt;\nDie Formulierung “Mit herzlichen Grüßen” kommt also einmal im Korpus vor, und zwar im Dokument Q.01.19160115.FMA.01.xml.\nSie können auch mit wc -l (Mac/Linux) den Wordcount, die Anzahl der gefundenen Treffer auf Zeilenebene, -l zählen, bzw. mit Matches.Count (Windows), und mit &gt; in eine neue Datei schreiben (die während der Ausführung des Kommandos erstellt wird):\nMac/Linux:\ngrep -E -i '(Mit herzlichem Gruß|Mit herzlichen Grüßen)' *.xml | wc -l &gt; count_greetings.txt\nWindows:\n(Select-String -Path *.xml -Pattern \"(Mit herzlichem Gruß|Mit herzlichen Grüßen)\").Matches.Count &gt; count_greetings.txt\nWenn Sie die neu erstellte Datei count_greetings.txt öffnen, die sich im selben Ordner wie die Briefe befindet, sollte dort “1” stehen, weil unsere Suche einen Treffer ergeben hat.\nDas Kommando grep (Mac/Linux) hat im obigen Befehl den Zusatzparameter E bekommen, das Kommando Select.String (Windows) den Parameter -Pattern, d.h. wir suchen nicht eine exakte Zeichenkette, sondern nutzen Möglichkeiten zur Mustersuche, zur Suche nach Patterns. Diese werden formuliert als sog. Extended Regular Expressions (von hier kommt das E), als reguläre Ausdrücke. Wir haben in unserer Suchabfrage nämlich nicht nur nach “Mit herzlichem Gruß” gesucht, sondern auch nach “Mit herzlichen Grüßen”, formuliert mit dem Zeichen “|”, hier als “oder” zu lesen. Mithilfe Regular Expressions können wir unsere Suche weiter ausbauen und nach verschiedenen Varianten/Schreibweisen auf einmal suchen.\n\n\n\n\n\n\nHinweis\n\n\n\nRegular Expressions haben verschiedene flavours – je nach Programmiersprache werden Dinge etwas anders formuliert, und manche Defaulteinstellungen unterscheiden sich. In unserem Fall benötigt grep noch den Parameter -i, um Groß- und Kleinschreibung zu ignorieren. Select.String ignoriert dies by default und braucht keinen zusätzlichen Parameter. Solche Feinheiten sind bei der Arbeit mit Regular Expressions wichtig zu wissen, aber das lernt man on the go.\n\n\nMac/Linux:\ngrep -E -i '(Mit herzlichem Gru(ß|ss)|Mit herzlichen Grü(ß|ss)en|H(e|.?)rzl. Gru(ß|ss))' *.xml | wc -l\nWindows:\n(Select-String -Path *.xml -Pattern \"(Mit herzlichem Gru(ß|ss)|Mit herzlichen Grü(ß|ss)en|H(e|.?)rzl. Gru(ß|ss))\").Matches.Count &gt; count_greetings.txt\nSo formuliert finden wir 17 Treffer für eine Grußformel, mit den möglichen Schreibweisen “Mit herzlichem Gruß”, “Mit herzlichem Gruss”, “Mit herzlichen Grüßen”, “Mit herzlichen Grüssen”, “Herzl. Gruß”, “Herzl. Gruss”, “Hrzl. Gruß”, “Hrzl. Gruss”.\nWenn wir herausfinden möchten, ob Grüße mal herzlich, mal hrzl. oder freundlich verschickt wurden, können wir die Suche und die Art der Ausgabe modifizieren:\nMac/Linux:\ngrep -E -i 'Gr(u|ü)(ß|ss)' *.xml\nWindows:\nSelect-String -Path *.xml -Pattern \"Gr(u|ü)(ß|ss)\"\nOutput:\nQ.01.19140115.FMA.01.xml:                    stets sofort antworte; es muß verloren gegangen sein. Grüßen Sie bitte D&lt;hi rend=\"super\"&gt;\nQ.01.19140119.FMA.01.xml:                    &lt;salute&gt;Hrzl. Gruß&lt;/salute&gt; &lt;signed&gt;Ihr &lt;persName key=\"P.0000003\" ref=\"http://d-nb.info/gnd/11857745X\"&gt;F. Marc&lt;/persName&gt;\nQ.01.19140125.FMA.02.xml:                    &lt;salute&gt;Hrzl. Gruß&lt;/salute&gt;\nQ.01.19140421.FMA.01.xml:                        &lt;closer&gt;Gute Besserung &lt;persName key=\"P.0000002\" ref=\"http://d-nb.info/gnd/118891456\"&gt;Ihrer Frau&lt;/persName&gt; u. &lt;salute&gt;viel Grüße von mir&lt;/salute&gt; &lt;signed&gt;Ihr &lt;persName key=\"P.0000003\" ref=\"http://d-nb.info/gnd/11857745X\"&gt;Fz Marc&lt;/persName&gt;\nQ.01.19140507.FMA.01.xml:                    &lt;salute&gt;besten Gruß&lt;/salute&gt;\nQ.01.19140831.FMA.01.xml:                    &lt;salute&gt;Hrzl. Gruß von Eurem Freund in Waffen&lt;/salute&gt; &lt;signed&gt;\nQ.01.19141113.FMA.01.xml:                    &lt;salute&gt;Hrzl. Gruß 1 x 2&lt;/salute&gt; &lt;signed&gt;Ihr &lt;persName key=\"P.0000003\" ref=\"http://d-nb.info/gnd/11857745X\"&gt;Fz. Marc&lt;/persName&gt;\nQ.01.19150112.FMA.01.xml:                    &lt;salute&gt;Hrzl. Gruß Ihnen beiden&lt;/salute&gt;\nQ.01.19150116.FMA.01.xml:                    &lt;salute&gt;Mit herzl. Gruß Ihnen beiden&lt;/salute&gt; &lt;signed&gt;Ihr &lt;persName key=\"P.0000003\" ref=\"http://d-nb.info/gnd/11857745X\"&gt;FrM&lt;/persName&gt;.&lt;/signed&gt;\nQ.01.19150121.FMA.01.xml:                    &lt;salute&gt;Herzl. Gruß&lt;/salute&gt; &lt;signed&gt;Ihr &lt;persName key=\"P.0000003\" ref=\"http://d-nb.info/gnd/11857745X\"&gt;Fz. Marc&lt;/persName&gt;\n...\nMit diesem Kommando durchsuchen wir also den Text nach dem Muster Gr(u|ü)(ß|ss), also Beginn mit Gr oder gr, dann folgt entweder ein u oder ein ü, dann entweder ein ß oder ss. Weil wir kein Wortende markiert haben (das ginge mit \\b), werden auch “Grüße” oder “Grüssen” gefunden.\nWenn Sie sich während der Lektüre des vorangegangenen Kapitels auf der Webseite durch die Briefe geklickt haben, werden Sie festgestellt haben, dass ein Brief nicht immer mit “Gruß” oder “Grüßen” endet. Beim Output der Suchanfragen im Terminal sehen Sie, dass alle Grußformeln von einem Tag-Paar umgeben sind: &lt;salute&gt; kennzeichnet den Beginn des Grußes, &lt;/salute&gt; das Ende. Öffnen Sie eine der Briefdateien und suchen Sie nach “salute”. (Wenn Sie keinen XML-fähigen Editor auf dem Computer haben, öffnen Sie die Datei einfach mit einem Browser.)\n\n\n\nAusschnitt aus Brief Nr. 1 von Franz Marc an Herwarth Walden\n\n\nWie Sie sehen, gibt es das Tag-Paar &lt;salute&gt;-&lt;/salute&gt; zweimal, einmal umrahmt vom Tag-Paar &lt;opener&gt;-&lt;/opener&gt;, einmal von &lt;closer&gt;-&lt;/closer&gt;. Die Anrede ist mit dem ersten, die Grußformel mit dem zweiten Tag-Paar markiert. Wir können also, wenn wir mit Dokumenten arbeiten, die nach festgelegten Richtlinien ausgezeichnet wurden, nach dem Element Grußformel suchen, ohne erst einen Blick in die Texte werfen zu müssen, um verschiedene Suchabfragen zu formulieren. Wir formulieren unsere Suchabfrage um und suchen nun nach einer Abfolge von Zeichen mit dem Beginn &lt;closer&gt;, gefolgt von keinem bis zu beliebig vielen (.*) Zeichen der Klasse cntrl, also nicht sichtbare Zeichen wie Tabs, Seiten- oder Zeilenumbruch. Danach folgt &lt;salute&gt;, wiederum gefolgt von keinem bis zu beliebig vielen (.*) Zeichen, keinem bis zu beliebig vielen (.*) Zeichen der Klasse cntrl und nochmal keinem bis zu beliebig vielen (.*) Zeichen, bis der Beginn des Schlusstags zu &lt;/salute&gt; kommt. Damit werden die verschiedenen Fälle in den Briefen abgedeckt, dass zwischen &lt;closer&gt; und &lt;salute&gt; Text oder ein Zeilenumbruch stehen kann oder auch nicht, und dass zwischen &lt;salute&gt; und &lt;/salute&gt; Text, kein Text oder ein Zeilenumbruch kommen kann.\nMac/Linux:\ngrep -E -zo '&lt;closer&gt;[[:cntrl:]].*&lt;salute&gt;.*[[:cntrl:]].*&lt;'  *.xml\nOutput:\nQ.01.19140115.FMA.01.xml:&lt;closer&gt;\n                    &lt;salute&gt;Hrzl.&lt;\nQ.01.19140119.FMA.01.xml:&lt;closer&gt;\n                    &lt;salute&gt;Hrzl. Gruß&lt;/salute&gt; &lt;signed&gt;Ihr &lt;persName key=\"P.0000003\" ref=\"http://d-nb.info/gnd/11857745X\"&gt;F. Marc&lt;\nQ.01.19140121.FMA.01.xml:&lt;closer&gt;\n                    &lt;salute&gt;Hrzl.&lt;/salute&gt; &lt;signed&gt;Ihr &lt;persName key=\"P.0000003\" ref=\"http://d-nb.info/gnd/11857745X\"&gt;FzMarc&lt;\nQ.01.19140125.FMA.02.xml:&lt;closer&gt;\n                    &lt;salute&gt;Hrzl. Gruß&lt;\nQ.01.19140409.FMA.01.xml:&lt;closer&gt;\n                    &lt;salute&gt;Herzl.&lt;\nQ.01.19140414.FMA.01.xml:&lt;closer&gt;\n                    &lt;salute&gt;Hrzl.&lt;/salute&gt; &lt;signed&gt;Ihr &lt;persName key=\"P.0000003\" ref=\"http://d-nb.info/gnd/11857745X\"&gt;FMarc&lt;/persName&gt;.&lt;\nQ.01.19140507.FMA.01.xml:&lt;closer&gt;\n                    &lt;salute&gt;besten Gruß&lt;\nQ.01.19140512.FMA.01.xml:&lt;closer&gt;\n                    &lt;salute&gt;Hrzl.&lt;/salute&gt; &lt;signed&gt;Ihr &lt;persName key=\"P.0000003\" ref=\"http://d-nb.info/gnd/11857745X\"&gt;Fr Marc&lt;\nQ.01.19140606.FMA.01.xml:&lt;closer&gt;\n                    &lt;salute&gt;Hrzl.&lt;/salute&gt; &lt;signed&gt;Ihr &lt;persName key=\"P.0000003\" ref=\"http://d-nb.info/gnd/11857745X\"&gt;Fz. Marc&lt;\nQ.01.19140608.FMA.01.xml:&lt;closer&gt;\n                    &lt;salute&gt;hrzl.&lt;/salute&gt; &lt;signed&gt;Ihr &lt;persName key=\"P.0000003\" ref=\"http://d-nb.info/gnd/11857745X\"&gt;F. Marc&lt;\n...\nWenn wir die Ergebnisse direkt in eine Datei schreiben wollen, können wir das natürlich auch tun:\nMac/Linux:\ngrep -E -zo '&lt;closer&gt;[[:cntrl:]].*&lt;salute&gt;.*[[:cntrl:]].*&lt;'  *.xml &gt; Grussformeln.txt\nSpätestens jetzt wäre es aber an der Zeit, das Instrumentarium zu wechseln: Mit dem Terminal bzw. der Shell kann man verschiedenste Operationen durchführen, und es gibt zahlreiche kleine Programme, die man zusätzlich installieren kann – zum Parsen, also Zerlegen von XML-Dateien, zur Bearbeitung von Bilddateien oder zum Download von YouTube-Videos. Die Übersichtlichkeit ist allerdings recht begrenzt, und gerade für die Analyse von Struktur- und Textdaten gibt es weitaus geeignetere Programmiersprachen wie R oder Python, wie in Kapitel 2.2 bereits erwähnt.\nUm beispielsweise mit dem Volltext der Briefe weiterzuarbeiten, sei es für ein close reading oder für quantitative Textanalysen, können für eine bessere Lesbarkeit und einfachere Auswertungen die Auszeichnungen des TEI-XML entfernt werden.",
    "crumbs": [
      "Praxis",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Durch die Hintertür</span>"
    ]
  },
  {
    "objectID": "08_Hintertuer.html#footnotes",
    "href": "08_Hintertuer.html#footnotes",
    "title": "8  Durch die Hintertür",
    "section": "",
    "text": "Kommandozeile/Command Line, Bash, Shell oder Prompt finden sich oft als synonym genutzte Begriffe für Command Line Interfaces. Auf UNIX-basierten Betriebssystemen wie Mac OS und Linux ist das Terminal als Interface weit verbreitet; für Details: https://en.wikipedia.org/wiki/Command-line_interface#History. Windowsnutzer:innen kommen mit der PowerShell ganz gut zurecht, es empfiehlt sich eventuell die Installation von Cygwin oder MinGW, um mit einem UNIX-basierten Interface arbeiten zu können.↩︎",
    "crumbs": [
      "Praxis",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Durch die Hintertür</span>"
    ]
  },
  {
    "objectID": "09_Ausblick.html#footnotes",
    "href": "09_Ausblick.html#footnotes",
    "title": "9  Ausblick",
    "section": "",
    "text": "In diesem Fall wurde die mithilfe des verlinkten Skripts (in der Programmiersprache R) erstellte csv-Datei mit OpenRefine weiter bearbeitet, um den Adressen Geokoordinaten hinzuzufügen. Die so angereicherte Tabelle wurde in das Onlinetool Palladio geladen, das Koordinaten auf einer Karte darstellen kann.↩︎",
    "crumbs": [
      "Praxis",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Ausblick</span>"
    ]
  },
  {
    "objectID": "terms.html",
    "href": "terms.html",
    "title": "Appendix A — Glossar",
    "section": "",
    "text": "API\nApplication Programming Interface: a facility offered by a web resource which allows search queries independent of a GUI, often performed using scripts\n\n\nbash\ndefault program that runs in the command line\n\n\nbias\nsystematic error that results from an unbalanced sample\n\n\nbig data\nhuge amount of data, identifiable through repeated freezing of your standard program when opening a file\n\n\nborn digital data\ndata which originated in a digital form\n\n\nCLI\nCommand Line Interface, text interface that allows interaction with the computer; see also bash\n\n\nclose reading\ncareful and attentive interpretation of a text\n\n\nCMS\nContent Management System\n\n\nConsole\nSee CLI\n\n\nCrowdsourcing\nprojects that include the active participation of the public to generate content, transcribe sources etc.\n\n\ncsv\ncomma separated values, a structured text format, using commas as separators between columns\n\n\ndistant reading\nquantitative approach to huge amounts of texts, using computational methods to search for interpretable patterns\n\n\nGUI\nGraphical User Interface\n\n\nHTML\nHypertext Markup Language, a structured text format, like the format this guide is written in, to render documents in a browser\n\n\nJupyter notebook\nweb application/interactive coding environment that runs in a browser; let’s you create and share code (https://jupyter.org)\n\n\nmachine learning\numbrella term for different methods that use data to do a task in a specific way, using data to learn and to improve the results\n\n\nmachine readable\ntransformation of, for example, text into a data format that is processable by a computer\n\n\nOCR\nOptical Character Recognition, process of transforming text on an image into a data format\n\n\nOS\nOperating System\n\n\nopen source\nfreely available source code that can be used, modified and redistributed without limitations\n\n\nOSS\nOpen Source Software\n\n\nRegular Expression\nsyntax for search and replace text using patterns (instead of exact matches)\n\n\nterminal\nSee CLI\n\n\nweb scraping\nextracting data from websites",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Glossar</span>"
    ]
  },
  {
    "objectID": "further_resources.html",
    "href": "further_resources.html",
    "title": "Appendix B — Literatur, Tools, Tutorials",
    "section": "",
    "text": "B.1 Was ist digital history?",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Literatur, Tools, Tutorials</span>"
    ]
  },
  {
    "objectID": "further_resources.html#sec-digitalhistory-paper",
    "href": "further_resources.html#sec-digitalhistory-paper",
    "title": "Appendix B — Literatur, Tools, Tutorials",
    "section": "",
    "text": "Antenhofer, Christina, Kühberger, Christoph, Strohmeyer, Arno (Hrsg.): Digital Humanities in den Geschichtswissenschaften. Wien 2023.\nBrennan, Sheila A.: Digital History, in: The Inclusive Historian’s Handbook, https://inclusivehistorian.com/digital-history/, 04.06.2019.\nHohls, Rüdiger: Digital Humanities und digitale Geschichtswissenschaften, in: Busse, Laura u. a. (Hg.): Clio-Guide. Ein Handbuch zu digitalen Ressourcen für die Geschichtswissenschaften, Berlin 2018, S. A.1-1–B.1-34. Online: https://doi.org/10.18452/19244.\nRomein, C. Annemieke u. a.: State of the Field: Digital History, in: History 105 (365), 04.2020, S. 291—312. Online: https://doi.org/10.1111/1468-229X.12969.\nWinters, Jane: Digital History, in: Tamm, Marek; Burke, Peter (Hg.): Debating New Approaches to History, London 2019, S. 277–300.\nArt. “Digital history”, in: Wikipedia, 07.09.2022. Online: https://en.wikipedia.org/w/index.php?title=Digital_history&oldid=1220662831, Stand: 25.04.2024.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Literatur, Tools, Tutorials</span>"
    ]
  },
  {
    "objectID": "further_resources.html#einführungen-und-guides",
    "href": "further_resources.html#einführungen-und-guides",
    "title": "Appendix B — Literatur, Tools, Tutorials",
    "section": "B.2 Einführungen und Guides",
    "text": "B.2 Einführungen und Guides\n\nBattershill, Claire; Ross, Shawna: Using Digital Humanities in the Classroom. A Practical Introduction for Teachers, Lecturers, and Students, London u.a. 2022.\nBlaney, Jonathan u. a.: Doing Digital History. A Beginner’s Guide to Working with Text as Data, Manchester 2021.\nCohen, Daniel J.; Rosenzweig, Roy: Digital History. A Guide to Gathering, Preserving, and Presenting the Past on the Web, Philadelphia 2006. Online: https://chnm.gmu.edu/digitalhistory/.\nDöring, Karoline u. a. (Hg.): Digital History. Konzepte, Methoden und Kritiken Digitaler Geschichtswissenschaft, Boston 2022, Online: https://doi.org/10.1515/9783110757101.\nDougherty, Jack; Nawrotzki, Kristen (Hg.): Writing History in the Digital Age, 2013. Online: https://doi.org/10.3998/dh.12230987.0001.001.\nGraham, Shawn u. a.: Exploring Big Historical Data. The Historian’s Macroscope, 2022. Online: https://doi.org/10.1142/12435.\nLemercier, Claire; Zalc, Claire: Quantitative Methods in the Humanities. An Introduction, Charlottesville 2019.\nPeroni, Silvio: The CTP Book. A book for teaching Computational Thinking and Programming skills to people with a background in the Humanities.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Literatur, Tools, Tutorials</span>"
    ]
  },
  {
    "objectID": "further_resources.html#sec-digitaltools",
    "href": "further_resources.html#sec-digitaltools",
    "title": "Appendix B — Literatur, Tools, Tutorials",
    "section": "B.3 Digital history tools (free/open source)",
    "text": "B.3 Digital history tools (free/open source)\n\nB.3.1 Allgemein\n\nProgramming Historian: Tutorials zu verschiedenen Tools und Methoden für historische Forschung und Lehre\n\n\n\nB.3.2 Datenanalyse\n\nDatasette: interaktives Tool zur Datenexploration und -visualisierung\nHeurist: webbasierte Forschungsumgebung zur Datenerhebung, -exploration und -visualisierung\nNodegoat: webbasierte Forschungsumgebung zur Datenerhebung, -exploration und -visualisierung\n\n\n\nB.3.3 Karten\n\nDariah GeoBrowser: webbasiertes Tool zur Darstellung raumzeitlicher Daten\nQGis: Software zum Erstellen, Bearbeiten, Anzeigen und Analysieren räumlicher Information\nMap in Seconds: webbasiertes Tool zur Erstellung von Karten (optimiert für Chrome Browser)\nPalladio: webbasiertes Tool zur Visualisierung strukturierter Daten, u.a. Geodaten\n\n\n\nB.3.4 Netzwerkanalyse\n\nGephi: Programm zur Analyse und Visualisierung von Graphen und Netzwerken\nNodegoat: webbasierte Forschungsumgebung zur Datenerhebung, -exploration und -visualisierung, u.a. auch Netzwerkanalyse\nPalladio: webbasiertes Tool zur Visualisierung strukturierter Daten, u.a. auch Netzwerkanalyse\n\n\n\nB.3.5 Text-/Korpusanalyse\n\nAntConc: Korpusanalyse-Toolkit\nLemmatisierung: Sammlung der FID Romanistik\nNatural Language Toolkit, Package für Python zur Tokenisierung, Lemmatisierung usw.: NLTK\nTokenisierung: Tutorial von fortext zu NLTK\nVoyant-Tools: Sammlung von Tools zur Textanalyse, browserbasiert oder standalone\n\n\n\nB.3.6 Storytelling\n\nOmeka: webbasierte Plattform zur Kuration digitaler Sammlungen, für die Veröffentlichung und Ausstellung von Bildern/Objekten\nStoryLine JS: Tool zur Erstellung von interaktiven Diagrammen\nStoryMap JS: Tool zur Erstellung von interaktiven Karten/Bildern mit verschiedenen Elementen\nTimeline JS: Tool zur Erstellung von interaktiven Timelines\n\n\n\nB.3.7 Visualisierung\n\nBostock, Michael; Heer, Jeffrey; Ogievetsky, Vadim: A Tour through the Visualization Zoo. A Survey of Powerful Visualization Techniques, from the Obvious to the Obscure, in: Queue 8, Nr. 5 (2010). Online: https://queue.acm.org/detail.cfm?id=1805128\nData Visualisation Catalogue: Guide zur Auswahl von Visualisierungsformen\nDatasette: interaktives Tool zur Datenexploration und -visualisierung\nFID Romanistik: Sammlung von Tools zur Datenvisualisierung\nPalladio: webbasiertes Tool zur Visualisierung strukturierter Daten\nRAWGraphs: Tool zur Datenvisualisierung von tabularen Daten (.tsv-, .csv-, .dsv- oder .json-Dateien)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Literatur, Tools, Tutorials</span>"
    ]
  },
  {
    "objectID": "further_resources.html#digital-literacy-digital-criticism",
    "href": "further_resources.html#digital-literacy-digital-criticism",
    "title": "Appendix B — Literatur, Tools, Tutorials",
    "section": "B.4 Digital Literacy, Digital Criticism",
    "text": "B.4 Digital Literacy, Digital Criticism\n\nEkström, Andreas: The Moral Bias behind your Search Results, TED talk 7.12.2015 (9:18), Online: https://www.youtube.com/watch?v=_vBggxCNNno.\nGibbs, Frederick W.: New Forms of History: Critiquing Data and Its Representations, in: The American Historian, February 2016. Online: http://tah.oah.org/february-2016/new-forms-of-history-critiquing-data-and-its-representations/.\nTavani, Herman; Zimmer, Michael: Search Engines and Ethics, in: Edward N. Zalta (Hg.): The Stanford Encyclopedia of Philosophy (Fall 2020 Edition), Online: https://plato.stanford.edu/archives/fall2020/entries/ethics-search/, Kap. 3.1.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Literatur, Tools, Tutorials</span>"
    ]
  },
  {
    "objectID": "further_resources.html#programmiersprachen-editoren",
    "href": "further_resources.html#programmiersprachen-editoren",
    "title": "Appendix B — Literatur, Tools, Tutorials",
    "section": "B.5 Programmiersprachen & Editoren",
    "text": "B.5 Programmiersprachen & Editoren\n\nPython:\n\nAlthage, Melanie u. a.: Python für Historiker:innen. Ein anwendungsorientierter und interaktiver Einstieg, 20.07.2022, Jupyter Book, (v1.0)\nKarsdorp, Folgert: Python Programming for the Humanities. Interactive Tutorial and Introduction to Programming with Python for the Humanities.\nMattingly, William : PythonHumanities.com. A Free Course for the Digital Humanities.\nEditor z.B. PyCharm\n\nR:\n\nMullen, Lincoln A.: Computational Historical Thinking With Applications in R\nPhillips, Nathaniel D.: YaRrr! The Pirate’s Guide to R.\nWickham, Hadley; Grolemund, Garrett: R for Data Science.\nEditor z.B. RStudio\n\nEditoren: Anaconda; Sublime Text; Visual Studio Code",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Literatur, Tools, Tutorials</span>"
    ]
  },
  {
    "objectID": "further_resources.html#regular-expressions",
    "href": "further_resources.html#regular-expressions",
    "title": "Appendix B — Literatur, Tools, Tutorials",
    "section": "B.6 Regular Expressions",
    "text": "B.6 Regular Expressions\n\nKnox, Doug: Understanding Regular Expressions, Programming Historian 2 (2013), https://doi.org/10.46430/phen0033. (self-learning lesson)\nRegexOne: Learn Regular Expressions with simple, interactive exercises. (interactive self-learning tutorial)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Literatur, Tools, Tutorials</span>"
    ]
  },
  {
    "objectID": "further_resources.html#terminalcommand-lineshell",
    "href": "further_resources.html#terminalcommand-lineshell",
    "title": "Appendix B — Literatur, Tools, Tutorials",
    "section": "B.7 Terminal/Command Line/Shell",
    "text": "B.7 Terminal/Command Line/Shell\n\nDawson, Ted: Introduction to the Windows Command Line with PowerShell, Programming Historian 5 (2016), https://doi.org/10.46430/phen0054. (self-learning lesson)\nMIT Computer Science Department: 1-hour-lecture on the Shell (video)\nMilligan, Ian; Baker, James: Introduction to the Bash Command Line, Programming Historian 3 (2014), https://doi.org/10.46430/phen0037. (self-learning lesson)\ndatacamp course:Introduction to Shell (interactive self-learning lesson)\nJeroen Janssens: Data Science at the command line (book)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Literatur, Tools, Tutorials</span>"
    ]
  },
  {
    "objectID": "further_resources.html#xml",
    "href": "further_resources.html#xml",
    "title": "Appendix B — Literatur, Tools, Tutorials",
    "section": "B.8 XML",
    "text": "B.8 XML\n\nLatex Ninja Blog: A shamelessly short intro to XML for DH beginners (includes TEI) (blog post)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Literatur, Tools, Tutorials</span>"
    ]
  }
]