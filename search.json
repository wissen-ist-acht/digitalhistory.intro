[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Digital History",
    "section": "",
    "text": "Welcome\nDer vorliegende Guide begleitet die Einführungskurse im Fach Geschichte an der Universität Basel und soll einen ersten Einblick in den Bereich digital history geben. Die epochen- und areaspezifischen Inhalte der verschiedenen Einführungskurse sollen dabei berücksichtigt werden, die Verweise auf verschiedene digital-history-Projekte aus unterschiedlichen Epochen mit der Zeit also anwachsen.\nDer Guide wird für die Teilnehmer:innen der Einführungskurse von einer Präsenzsitzung begleitet, bietet aber hoffentlich auch unabhängig davon einen Mehrwert. Für Kommentare, Anregungen oder Beschwerden freue ich mich über eine Nachricht.\nDer Guide ist in zwei Teile gegliedert: Die Kapitel 1–5 sollen eine erste Übersicht über “digital history” bieten und den Blick auf Neuerungen und Veränderungen richten, die sich in den Geschichtswissenschaften aus der Nutzung digitaler Methoden ergeben. Der anschließende Praxisteil zeigt an einem konkreten Beispiel die Anwendung verschiedener Techniken auf, die sich (nicht nur) für Historiker:innen bei der Arbeit mit Quellenmaterial anbieten. Der Praxisteil verfolgt dabei zwei Ziele: Zum einen sollen Hemmungen bei der Arbeit mit dem Computer, die über die Nutzung als elektronische Schreibmaschine hinausgeht, abgebaut werden. Zum zweiten soll ein grundlegendes Verständnis dafür hergestellt werden, welche Möglichkeiten computergestützte Analysen bieten und wie diese in der historischen Arbeit eingesetzt werden können.\nDie Übersicht soll möglichst knapp gehalten werden – es gibt zahlreiche ausführliche Grundlagenwerke, weswegen viele Themen nur kurz angeschnitten, dafür aber mit weiterführenden Verweisen versehen werden. Dasselbe gilt für den Praxisteil: Weiterreichende Anleitungen, Tutorials oder Onlinekurse werden an entsprechender Stelle verlinkt. Vollständigkeit wird an keiner Stelle beansprucht; Hinweise auf weitere Online-Angebote nehme ich gerne auf."
  },
  {
    "objectID": "01_intro.html",
    "href": "01_intro.html",
    "title": "1  Was ist digital history?",
    "section": "",
    "text": "Über die Antwort zur Frage, was digital history ist oder umfasst, kann man ausgiebig diskutieren; als Teilgebiet der digital humanities, der digitalen Geisteswissenschaften, kann die folgende aktuelle und pragmatische Definition von Blaney et al. hilfreich sein:\n\nDigital humanities, in our view, is a question of approach: if you are actively and critically using digital tools to aid your work in researching, teaching or learning, you are probably doing digital humanities. We would encourage anyone to learn to program if they are interested in doing so, but we do not see it as a defining characteristic of work in digital humanities.1\n\nDabei umfassen “digital tools” eine große Bandbreite – und es wird sich kaum eine:r finden, der:die Studium, Forschung oder Lehre völlig ohne die Nutzung digitaler Techniken betreibt. Wir sind alle Historiker:innen im digitalen Zeitalter, und als solche müssen wir ohnehin neue Kompetenzen entwickeln. Wir können uns aber zudem dafür entscheiden, für ein Forschungsprojekt Methoden und Techniken einzusetzen, die über die traditionellen Werkzeuge der Geschichtswissenschaften hinausgehen – Analyse und Interpretation von Quellen durch deren genaue Lektüre, sogenanntes close reading –, und uns durch den Computer unterstützen lassen. Ob wir hierbei auf vorhandene Software zurückgreifen oder selbst Programme schreiben, um uns nicht nur als Historiker:innen im digitalen Zeitalter, sondern auch als digitale Historiker:innen zu verstehen, mögen manche als Glaubensfrage auffassen; eine inkludierende Haltung zu dieser Frage scheint mir dabei nur Vorteile zu haben.2\nFür eine erste Idee dafür, wie man historische Fragestellungen mithilfe digitaler Methoden beantworten kann und wie unterschiedlich digital unterstützte Forschungsprojekte aussehen können, bietet sich unter anderem der Übersichtsartikel “State of the Field: Digital History” von Romein et al. (2020) an.3 Eine anwachsende Liste an Beispielprojekten aus unterschiedlichen Epochen bzw. Themenbereichen findet sich unter Projekte und Ressourcen in Kapitel 2.\nUm eine Annäherung an die aktive, kritische und reflektierte Nutzung digitaler Methoden in Forschung und Lehre mit einem Fokus auf deren Anwendung in den Geschichtswissenschaften geht es im vorliegenden Guide. Weiterführende Texte zur Frage, was digital history ist bzw. umfasst, finden sich unter Further Ressources\n\n\n\n\n\nBlaney, Jonathan; Winters, Jane; Milligan, Sarah u. a.: Doing digital history: a beginner’s guide to working with text as data, Manchester 2021 (IHR research guides), S. 6.↩︎\nEntgegen einer häufig zitierten Aussage von Emmanuel Le Roy Ladurie (*1929), der Historiker von morgen werde Programmierer sein, oder er werde nicht sein: “L’historien de demain sera programmeur ou il ne sera pas.” Le Roy Ladurie, Emmanuel: La fin des érudits, in: Le Nouvel Observateur, 08.1968.↩︎\nRomein, C. Annemieke; Kemman, Max; Birkholz, Julie M. u. a.: State of the Field: Digital History, in: History 105 (365), 04.2020, S. 291–312. Online: <https://doi.org/10.1111/1468-229X.12969>, Stand: 15.09.2022.↩︎"
  },
  {
    "objectID": "02_forschung_lehre.html#digitalisierte-quellen-digitale-quellen",
    "href": "02_forschung_lehre.html#digitalisierte-quellen-digitale-quellen",
    "title": "2  Forschung und Lehre",
    "section": "2.1 Digitalisierte Quellen, digitale Quellen",
    "text": "2.1 Digitalisierte Quellen, digitale Quellen\nAls Historiker:innen steht die Arbeit mit Quellen im Mittelpunkt unserer Analysen. Das bedeutet gleichzeitig, dass der Zugang bzw. die Verfügbarkeit von Dokumenten einen Einfluss darauf hat, welche Fragen wir beantworten oder welche Analysen wir vornehmen können. Zugangsbeschränkungen, die die Größe und Zusammensetzung unseres Untersuchungskorpus beeinflussen, können dabei von Gedächtnisinstitutionen ausgehen, beispielsweise wenn bei zeitgenössischen Akten eine Schutzfrist festgesetzt wird oder wenn ein Objekt zu fragil für die Benutzung ist. Auch kann es aus finanziellen und/oder organisatorischen Gründen schwierig sein, bestimmte Archive an weiter entfernten Orten aufzusuchen, um weitere Dokumente für die Untersuchung zu berücksichtigen. Groß angelegte Digitalisierungsprojekte in Bibliotheken und Archiven bergen damit die Möglichkeit, zusätzliche Quellen nicht nur über einen Eintrag im Bibliothekskatalog zu finden, sondern die entsprechenden Dokumente in digitaler Form auf den eigenen Rechner zu laden. Gerade auch für wertvolle historische Bestände – antike Papyri, Handschriften aus dem Frühmittelalter, einzelüberlieferte Frühdrucke usw. – entsteht hier die Möglichkeit, diese einem größeren Kreis verfügbar zu machen, ohne das Objekt zu großer Belastung durch häufige Benutzung auszusetzen, und ohne dass die Benutzer:innen lange Reisen auf sich nehmen müssten. Für mittelalterliche und frühneuzeitliche Handschriften und Drucke beispielsweise existieren mittlerweile mehrere (meist nationale) Portale, die eine zentrale Suche über alle Bestände ermöglichen; eine Auswahl findet sich unter Projekte und Ressourcen.\nNeben der Digitalisierung vorhandener Quellen (Retrodigitalisierung) steht die unaufhörliche Entstehung neuer Quellen in rein digitaler Form (born digital data). Der relativen Knappheit von Quellen – und damit Daten –, die Vormodernehistoriker:innen oftmals zu beklagen haben, steht eine Überfülle an zeitgenössischem Material gegenüber, und beide Situationen – zu wenig/zu unvollständige und zu viele/zu unübersichtliche Datenmengen – bergen methodische Probleme: Wie stellt man ein Korpus zusammen, das ausreichend Quellen beinhaltet, um Fragestellungen zu beantworten, Thesen zu stützen, neue Erkenntnisse zu erhalten, das aber gleichzeitig in einem Forscher:innenleben bewältigbar bleibt? Historiker:innen müssen neue Kompetenzen erwerben, um mit solchen Fragen reflektiert umzugehen. Zur klassischen Quellenkritik kommt die digitale Quellenkritik, zur Fähigkeit, analoge Quellen zu lesen und zu verstehen, ein Äquivalent für den digitalen Bereich. Etwas ausführlicher geht es in Kapitel 3 um Digital Literacy und Digital Criticism."
  },
  {
    "objectID": "02_forschung_lehre.html#sec-digitaletools",
    "href": "02_forschung_lehre.html#sec-digitaletools",
    "title": "2  Forschung und Lehre",
    "section": "2.2 Digitale Tools zur Analyse",
    "text": "2.2 Digitale Tools zur Analyse\nDie hier bereits zitierte Definition, die aktive und kritische Nutzung digitaler Werkzeuge in Forschung, Lehre oder Studium sei es, was digital humanities ausmachten, wirft die Frage auf, was genau unter “digital tools” zu verstehen ist, und zu welchem Zweck man sie einsetzt. Allein schon das Lesen dieses Guides ist ohne digitale Hilfsmittel nicht möglich – es existiert kein gedrucktes Exemplar davon. Lesen am Bildschirm allein macht noch keinen digital humanist, aber man muss nicht erst eine Programmiersprache lernen, um den Computer für die eigene Arbeit zu nutzen und zu Ergebnissen zu kommen, die mit klassischen Methoden – im Bereich der Geschichtswissenschaften etwa papierbasiertes close reading von Quellen und Forschungsliteratur – nicht im selben Ausmaß erzielt werden könnten.\nUntersuchungen, die digitale Methoden einsetzen, sind im Normalfall skalierbar – wenn man eine Software benutzt, die die Häufigkeit von Begriffen in einem Dokument zählt, sollte es keinen Unterschied in der Anwendung machen, ob man eines oder einhundert Dokumente auswerten will. Würde man dasselbe per Hand tun, wäre man analog zum Anwachsen des Untersuchungskorpus mit der Auszählung beschäftigt. Digitale Werkzeuge ermöglichen es also unter anderem, Untersuchungen auf größere Mengen von Daten auszuweiten. Sie ermöglichen es auch, an einen solchen erweiterten Datensatz andere Fragen zu stellen, als dies mit einer kleineren Quellen-/Datengrundlage möglich wäre. Die vorherrschende Überlieferung historischer Quellen besteht aus Text, handgeschrieben, gemeißelt oder gedruckt – und durch die Möglichkeit, diesen mittels Texterkennung in computerlesbare Daten umzuwandeln, ergeben sich neue Perspektiven für die Arbeit von Historiker:innen: Wenn Texte als Daten verstanden werden, lassen sich aus Textquellen Datenbestände erstellen, die mithilfe quantitativer Methoden untersucht und ausgewertet werden können.1\nFür die Literaturwissenschaften beispielsweise ist ein wichtiges Anwendungsfeld die Überprüfung von Autor:innenschaft: Ob ein anonym überliefertes Werk einem:r namentlich bekannten Autor:in zugeschrieben werden kann, lässt sich entweder durch close reading von Literaturwissenschaftler:innen überprüfen, oder durch die Suche nach Mustern, nach quantifizierbare Eigenschaften eines Textes, wie beispielsweise die Häufigkeit von Funktionswörtern, Partikeln, Satzzeichen usw. Der unter dem Pseudonym Robert Galbraith veröffentlichte Kriminalroman The Cuckoo’s Calling konnte mit entsprechender Software Joanne K. Rowling zugeschrieben werden – mit dieser dauerte das dreißig Minuten, was etwa dem Lesen von zwanzig Romanseiten entspricht. Zu einem Artikel, der diesen Fall thematisiert und in das Feld der linguistischen Forensik einbettet, die Straftäter:innen mithilfe quantitativer Textanalyse ermittelt, geht es hier. Ein Video zur Entwicklung und Anwendung von Software zur Zuschreibung von Autor:innenschaft finden Sie hier. Die genutzte Software, JGGAP,2 lässt sich offensichtlich auch für historische Analysen nutzen – man denke nur an Herrschaftssysteme, in denen Zensurpolitik herrscht(e) und viele Autor:innen daher nicht unter ihrem Klarnamen publizier(t)en. Durch eine Identifikation anonymer Schreiber:innen lassen sich weitere Aspekte rund um die Thematik Zensur untersuchen – welche Akteur:innen waren öffentlich bekannt, wer publizierten gleichzeitig anonym und unter Klarnamen, welche Autor:innen schrieben aus dem Exil, welche Netzwerke lassen sich rekonstruieren, usw. Dadurch, dass ein Programm durch quantitative Auswertungen die Kärrnerarbeit der Identifikation abnehmen kann – um einen reflektierten Umgang mit Daten und Algorithmen geht es in Kapitel 3 –, bleibt mehr Zeit für die qualitative Arbeit; gleichzeitig fußt die Analyse auf einem aussagekräftigen Datensatz, anstatt nur Einzelbeispiele beleuchten zu können.\nQuantitative und qualitative Methoden sollen hier keinesfalls gegeneinander ausgespielt werden; vielmehr soll verdeutlicht werden, dass beide Herangehensweisen Vor- und Nachteile haben, und dass sie im besten Fall gewinnbringend miteinander kombiniert werden können – quantitative Auswertungen nur um ihrer selbst willen und ohne eine spezifische historische Fragestellung generieren kaum je einen Mehrwert.\nJe nach Datengrundlage, Analysezweck und Forschungsfrage bieten sich unterschiedliche Tools zur Nutzung an; für die meisten Forschungsvorhaben bis zum Ende des Studiums dürfte existierende Software ausreichen, sei es für die Akquise und Aufbereitung von Daten(-sätzen), für verschiedene Arten von Textanalyse, statistische Auswertungen, Netzwerkanalysen, Geomapping oder Visualisierungen. Eine Auswahl an Tools – alle free/open source – für spezifische Analysen findet sich unter Further Ressources. Für gewisse Analysen bietet es sich an, Programmierkenntnisse zu erwerben – das Erstellen eigener Skripts beinhaltet die umfassende Kontrolle darüber, wie Daten eingelesen, aufbereitet, angereichert, analysiert und visualisiert werden; bei wiederkehrenden Prozessen, die händisch einige Arbeitszeit in Anspruch nehmen würden, lässt sich so zusätzlich Zeit sparen. Für geisteswissenschafliche Projekte werden zurzeit vor allem zwei Programmiersprachen genutzt, R und Python. Da sich beide großer Beliebtheit in den Humanities erfreuen, existieren mittlerweile zahlreiche Packages, die Data und Text Mining sehr einfach machen. Solche Packages für Programmiersprachen kann man sich wie Plug-Ins für Programme vorstellen, beispielsweise ein AdBlocker für den Browser. So etwas ist nativ von den Entwickler:innen nichht vorgesehen, aber jemand hatte Bedarf, Anzeigen zu blockieren, hat hierzu ein Programm geschrieben und es der Community zur Verfügung gestellt. Der Unterschied zu einem Package ist, dass dieses verschiedenen Funktionen zur Verfügung stellt - auswählen und ausführen müssen die Anwender:innen. Wer in Schule und Studium keine Berührungspunkte mit Programmieren hatte, wird zu Beginn vielleicht größere Berührungsängste haben – und nochmal: Sie müssen nicht programmieren können, um quantitativ zu arbeiten. Speziell für Historiker:innen ohne Programmier-Vorkenntnisse ist das Projekt The Programming Historian, das seit 2008 zahlreiche Tutorials veröffentlicht, um verschiedene Tools, Techniken und Workflows für Forschung und Lehre vorzustellen."
  },
  {
    "objectID": "02_forschung_lehre.html#digitale-tools-zur-kommunikation",
    "href": "02_forschung_lehre.html#digitale-tools-zur-kommunikation",
    "title": "2  Forschung und Lehre",
    "section": "2.3 Digitale Tools zur Kommunikation",
    "text": "2.3 Digitale Tools zur Kommunikation\ntbd"
  },
  {
    "objectID": "02_forschung_lehre.html#digitale-elemente-in-der-hochschullehre",
    "href": "02_forschung_lehre.html#digitale-elemente-in-der-hochschullehre",
    "title": "2  Forschung und Lehre",
    "section": "2.4 Digitale Elemente in der Hochschullehre",
    "text": "2.4 Digitale Elemente in der Hochschullehre\ntbd"
  },
  {
    "objectID": "02_forschung_lehre.html#sec-projects",
    "href": "02_forschung_lehre.html#sec-projects",
    "title": "2  Forschung und Lehre",
    "section": "2.5 Projekte und Ressourcen",
    "text": "2.5 Projekte und Ressourcen\n\n2.5.1 Alte Geschichte\nProjekte:\nRessourcen/Portale:\n\n\n2.5.2 Mittelalter und Frühe Neuzeit\nProjekte:\nRessourcen/Portale:\n\ndMGH: Monumenta Germaniae Historica online (Beta-Version)\ne-codices: Virtuelle Handschriftenbibliothek der Schweiz\nFragmentarium: Laboratory for Medieval Manuscript Fragments\nHandschriftenportal: Zentraler nationaler Nachweis für Buchhandschriften in deutschen Bibliotheken und in deutscher Sprache (Entwicklungsstadium)\ne-manuscripta: Digitalisierte handschriftliche Quellen aus Schweizer Bibliotheken und Archiven\ne-rara: Plattform für digitalisierte Drucke aus Schweizer Institutionen\nGallica: Digitalisierte Quellen aus französischen Biblioteken\n\n\n\n2.5.3 Moderne und Zeitgeschichte\nProjekte:\n\nRefugee History: Wissenschaftliches Blog und interaktives Netzwerk zu aktuellen Debatten um das Thema “Flüchtlinge”\n\nRessourcen/Portale:\n\nDatenbank Bild + Ton zur Geschichte (Schweizer) sozialer Bewegungen\nDodis: Wissenschaftliche Edition von Dokumenten zur Schweizer Außenpolitik\ne-newspaperarchives.ch: Schweizer Zeitungen online\ne-periodica: Schweizer Zeitschriften online\nHistorische Statistik der Schweiz (HSSO)\nhistat: Zeitreihen zur Historischen Statistik\n\n\n\n2.5.4 Jüdische Geschichte\nProjekte:\n\nDigital Jewish Studies Online, Stroum Center for Jewish Studies, University of Washington\n\nRessourcen/Portale:\n\nMenny, Anna; Rürup, Miriam; Siegel, Björn: Jüdische Geschichte im deutschsprachigen Raum, in: Busse, Laura u. a. (Hg.): Clio-Guide. Ein Handbuch zu digitalen Ressourcen für die Geschichtswissenschaften, Berlin 2018, S. E.2-1–E.2-56. Online: https://doi.org/10.18452/19244.\n\n\n\n2.5.5 Geschichte Afrikas\nProjekte:\n\nEmandulo: A digital archive wich convenes and re-assembles archival/museal collections and presentations on southern African history before colonialism (the last 500 years)\n\nRessourcen/Portale:\n\nFHYA: Experimental digital research platform on southern African history before colonialism (the last 500 years)\n\n\n\n2.5.6 Osteuropäische Geschichte\nProjekte:\nRessourcen/Portale:\n\n\n2.5.7 Epochen-/Areaübergreifend:\nProjekte:\nRessourcen/Portale:\n\nAround DH in 80 days"
  },
  {
    "objectID": "03_digital_literacy_criticism.html#digital-literacy-data-literacy",
    "href": "03_digital_literacy_criticism.html#digital-literacy-data-literacy",
    "title": "3  Digital Literacy, Digital Critisicm",
    "section": "3.1 Digital Literacy, Data Literacy",
    "text": "3.1 Digital Literacy, Data Literacy\nUnter Data Literacy wird die Kompetenz verstanden, Daten zu sammeln, zu managen, zu evaluieren und zu nutzen,1 eine Kompetenz, die jede:r für den mittlerweile unvermeidlichen Umgang mit Daten verschiedenster Art im eigenen Alltag entwickeln sollte. Je nach Forschungsdisziplin ergeben sich weiter gewisse Spezifika, wobei Studierenden der Geisteswissenschaften ein Thema wie Algorithmenkritik nicht als erstes in den Sinn kommt, wenn es um die im Studium zu erwerbenden Kompetenzen geht.2 Aber auch ohne den Quellcode vonmachine-learning-Software im Detail zu verstehen, ermöglicht ein grundlegendes Verständnis von und ein Wissen über die Funktionsweisen solcher Anwendungen einen reflektierten Umgang mit diesen; eine solche Art von digital bzw. data literay ist vor allem dann relevant, wenn es um die Interpretation von Ergebnissen geht, die scheinbar objektiv sind, bzw. objektiv entstanden. Ein gutes Beispiel hierfür sind die Ergebnislisten bei Suchanfragen in einer Suchmaschine. Je nachdem, welchen Anbieter Sie nutzen, spielen verschiedene Umstände in die Generierung von Trefferlisten hinein, beispielsweise Ihre Suchhistorie, sodass search neutrality nicht mehr gewährleistet ist.3\nGehen Sie auf die Bilder-Suche von Google und suchen Sie nach “historian”. Was sehen Sie?\nWüsste ich nichts über Geschichtswissenschaftler:innen, würde ich aufgrund der Ergebnisse meiner Suche davon ausgehen, “a historian” wäre meist ein alter, weißer Mann mit Brille, Bart und einem großen Bücherregal; wenn Sie sich am Departement Geschichte der Uni Basel umsehen, dürfte ein etwas anderer Eindruck entstehen. Die Ergebnisse von Suchmaschinen, die für ihr Funktionieren Algorithmen anwenden, sind biased: Sie beruhen auf vorangegangenen Suchen, Vorlieben, geographischem Standort – und auf von Menschen eingegebenen Metadaten. Ein Bewusstsein hierfür und das Hinterfragen von Datensätzen gehören also mit zur Arbeit in einer digitalisierten Welt."
  },
  {
    "objectID": "03_digital_literacy_criticism.html#digital-criticism-data-criticism",
    "href": "03_digital_literacy_criticism.html#digital-criticism-data-criticism",
    "title": "3  Digital Literacy, Digital Critisicm",
    "section": "3.2 Digital Criticism, Data Criticism",
    "text": "3.2 Digital Criticism, Data Criticism\nDigitalisierte Quellen ebenso wie rein digitale erfordern eine erweiterte Art von Quellenkritik – im Einführungskurs an der Universität Basel lernen Sie die Grundlagen:\nWoher kommt eine Quelle, wer hat sie unter welchen Umständen und zu welchem Zweck erstellt? Welche Absichten können darin verborgen sein, und welche Verzerrungen können sich durch sie ergeben?\nWelche Tendenzen könnten sich in hochmittelalterlichen Herrscherchroniken verstecken, wenn der Verfasser in direkter Abhängigkeit des Auftraggebers stand? Wie sind Zeug:innenaussagen in Hexenprozessen zu bewerten, die unter Anwendung körperlicher Strafe entstanden sind? Mit wie viel Vorsicht sind die Inhalte eines Tagebuchs zu berwerten, das allem Anschein nach mit Blick auf eine spätere Veröffentlichung verfasst wurde?\nNeben der inneren Kritik geht es bei der Arbeit mit Quellen immer auch um Fragen der Korpusbildung: Wie kann eine Quellengrundlage erstellt werden, die für Beantwortung einer spezifischen historischen Fragestellung belastbar und aussagekräftig genug ist und gleichzeitig in angemessener Zeit bearbeitet werden kann? Hinzu kommen Spezifika bei der Arbeit mit unterschiedlichen Quellenformen bzw. -formaten: Bei analogen Quellen, die auch in digitaler Form zur Verfügung stehen, besteht die Gefahr, dass ein Thema, ein Bereich, ein Aspekt vernachlässigt wird, wenn nur die unmittelbar verfügbaren, digitalisierten Bestände zur Korpusbidlung genutzt werden. Wenn Sie sich beispielsweise für die Schweizer Historikerin und Frauenrechtlerin Meta von Salis (1855–1929) und deren briefliche Korrespondenz – Friedrich Nietzsche war einer ihrer Brieffreunde – interessieren und über die Suchplattform für historische Schweizer Bestände, swisscollections, in nationalen Bibliotheken und Archiven nach entsprechenden Dokumenten suchen, erhalten Sie 361 Treffer:\n\n\n\n\n\nErweiterte Suchmaske von swisscollections\n\n\n\n \n\n\n\n\n\nSuchergebnisse für “Meta von Salis” + “Brief”\n\n\n\n\n\nDigital verfügbar waren hiervon im Oktober 2022 lediglich 3 Einträge, wobei der erste ein Brief von Nietzsche an Meta von Salis ist, der zweite Eintrag umfasst sieben Briefe von Caroline Farner, und der dritte Eintrag ist weder an noch von Meta von Salis, sondern hat sie nur zum Thema:\n\n\n\nSuchergebnisse für “Meta von Salis” + “Brief” + “Digitalisat verfügbar”\n\n\nIhnen würde bei einer Korpuserstellung vom Schreibtisch aus also der Großteil der Überlieferung fehlen, und Ihre Untersuchungsergebnisse wären wohl sehr verzerrt, würden Sie statistische Aussagen treffen wollen: Meta von Salis unterhielt brieflichen Kontakt zu einem Mann und einer Frau, das Geschlechterverhältnis wäre also ausgeglichen; und Frauen schreiben im Schnitt mehr Briefe an Meta von Salis als Männer. Beim Blick auf alle Suchergebnisse würden sich Ihre Aussagen aber sehr ändern, und es würde sich lohnen, diese Verzerrung, diesen Bias aus Ihrer Datengrundlage zu entfernen.\n\nHinzu kommt natürlich immer das grundlegende Problem bei der Suche nach Quellen: swisscollections und ähnliche Portale können nur anzeigen, was die Kooperationspartner:innen zur Verfügung stellen. Hat eine Bibliothek Briefe von Meta von Salis in ihrem Bestand, diese aber noch nicht als Datensatz erfasst, wissen Sie im Gegensatz zum obigen Beispiel nicht einmal, dass Ihnen etwas entgehen würde, dass in Ihrem Korpus überhaupt ein Bias vorhanden ist.\n\nÄhnliche Vorsicht zur Vermeidung von Verzerrungen in der Datengrundlage gilt bei der Arbeit mit rein digitalen Daten, beispielweise bei der Auswertung von Datensätzen aus Befragungen. Wenn Sie sich am 27.10.2022 vor die Universitätsbibliothek in Basel stellen und einen Tag lang mithilfe eines kurzen Fragebogens und einer Tabellendatei erfassen, wie zufrieden die befragten Personen mit dem Essen in der Unimensa sind, werden Sie am Ende einen Datensatz erhalten, in dem sich vermutlich über 80% der Befragten für besseres und nahezu 100% für günstigeres Essen in der Mensa aussprechen – eine gute Schlagzeile für die BZ, die sich auf die neuesten Ergebnisse einer wissenschaftlichen Studie berufen kann. Führen Sie die gleiche Umfrage eine Woche später, mitten während der Herbstmesse durch, werden die Ergebnisse wohl erheblich anders aussehen. Die Wahrscheinlichkeit, dass die Mensa infolge der BZ-Schlagzeile innerhalb weniger Tage den Menüplan überarbeitet und die Preise herabgesetzt hat, ist dabei wohl geringer als diejenige, dass sich Ihr Sample, die Auswahl an Datenpunkten, also befragten Personen, durch die Messe stark verändert hat: Im Umkreis der Bibliothek treffen Sie nun nicht mehr vor allem Studierende und andere Uni-Angehörige an, sondern auch Messebesucher:innen vom Petersplatz. Auch hier sind Verzerrungen entstanden, ähnlich wie beim vorherigen Beispiel mit den Briefen: Wenn aus einer Gesamtheit nur eine spezifische Untermenge beobachtet wird, die sich durch ein gemeinsames Merkmal von der Gesamtheit unterscheidet – digitalisierte Quelle oder Besucher:in der Universitätsbibliothek –, ist die Datengrundlage und damit die Untersuchungsergebnisse biased. Um bei Daten, die Sie nachnutzen, eventuell vorhandene Verzerrungen nicht weiterzutransportieren, ist das Üben von Datenkritik eine essentielle Kompetenz.\nZur Tatsache, dass Daten eben nicht gegeben sind (lat. dare, datum: gebene, gegeben), sondern gemacht und daher entsprechend interpretiert werden müssen, finden Sie ein gutes Interview von Roopika Risam;4 zur Zementierung von Klischees durch Übersetzungsalgorithmen gibt es einen Artikel in der Republik von Marie-José Kolly und Simon Schmid;5 und über die Macht von Data Science und dem Änderungspotential von Data Feminism haben Catherine D’Ignazio und Lauren F. Klein ein ganzes Buch veröffentlich.6\nZur Frage, wie sich die digitale Wende, der digital turn, auf die Quellenkritik auswirkt, sehen Sie sich dieses kurze Video des Projekts Ranke.2 – Quellenkritik im digitalen Zeitalter an:7\n\n\nEine Handreichung zum Umgang mit digitalisierten und digitalen Daten, das im selben Projekt erarbeitet wurde, finden Sie hier."
  },
  {
    "objectID": "04_datenerhebung_analyse.html#datenerhebung",
    "href": "04_datenerhebung_analyse.html#datenerhebung",
    "title": "4  Datenerhebung und -aufbereitung",
    "section": "4.1 Datenerhebung",
    "text": "4.1 Datenerhebung\nEs gibt verschiedene Möglichkeiten, Daten für die historische Forschung zu erheben bzw. zu erstellen, von denen einige im Folgenden kurz angesprochen werden.\nFür Zeiträume, in denen Quellen vergleichsweise knapp sind und keine seriellen Daten existieren, bietet sich die Digitalisierung von Texten und deren anschließende Analyse an. Digitalisierung beinhaltet dabei nicht nur die Transformation von einer physischen Quelle in ein digitales Bild, sondern auch die Anreicherung des Bilds mit Layout und Text: Erst durch eine Markierung von Bereichen, in denen Text vorkommt, ist es in einem zweiten Schritt möglich, diesen als solchen zu erkennen und damit maschinenlesbar und auswertbar zu machen. Eine solche Umwandlung vom Bild zum Text ist dabei sowohl für moderne Texte, die als Typoskript vorliegen, als auch für vormoderne Handschriften und Drucke möglich, in lateinischer ebenso wie in arabischer, chinesischer oder japanischer Schrift. Es gibt kostenpflichtige Programme wie den Abbyy FineReader, aber auch Open-Source-Tools mit und ohne Graphical User Interface (GUI). Weit verbreitet ist Transkribus, das viele Funktionalitäten bündelt; die Texterkennung ist ab einer gewissen Menge Seiten allerdings kostenpflichtig, wobei studentische Projekte auf Anfrage unterstützt werden können. Programme, die über die Kommandozeile laufenm gänzlich kostenfrei sind und ebenfalls zahlreiche Funktionalitäten bieten, sind beispielsweise Kraken, OCR4all, OCRopus oder Calamari.\nZur Extraktion von Daten aus digitalen/digitalisierten Texten existieren verschiedene Möglichkeiten mithilfe kleiner Kommandozeilenprogramme (eher mühsam und schwierig zu lesen) oder mit Packages für Programmiersprachen, für die Geisteswissenschaften vor allem R oder Python (siehe dazu auch Kapitel 2.2). So können besipielsweise aus digitalisierten Telefonbüchern Entitäten wie Personen, Straßennamen oder Berufe oder aus alten Theaterprogrammheften gespielte Stücke, beteiligte Schauspieler:innen und verantwortliche Regisseurinnen extrahiert und als Datensätze weitergenutzt werden.1\nDer anfängliche Aufwand, der einer automatisierten Datenextraktion vorangeht und die steile Lernkurve mancher Programme können abschreckend wirken. Und wenn Sie nur ein Theaterprogramm detaillierter auswerten wollen, sind Sie sicher schneller, wenn Sie die entsprechenden Daten in eine Tabellensoftware abtippen. Wenn Sie aber einen größeren Quellenbestand zur Verfügung haben, der in sich ähnlich strukturiert ist, wie das bei Telefonbüchern oder Theaterprogrammheften der Fall sein dürfte, macht es kaum einen Unterschied mehr, ob Sie 10 oder 1000 Theaterprogramme analysieren möchten. Zudem können Sie Ihr erstelltes Skript, Ihr kleines Computerprogramm, anderen zur Verfügung stellen oder für ähnlich strukturierte Quellen in einem anderen Projekt nachnutzen.\nWenn Sie mit bereits digitalisierten Beständen aus öffentlichen Institutionen wie Galerien, Bibliotheken, Museen oder Archiven arbeiten wollen (sog. GLAMs: Galleries, Libraries, Archives, Museums), besteht oft die Möglichkeit, Daten über Schnittstellen herunterzuladen.2 Solche APIs (Application Programming Interface) ermöglichen eine Kommunikation zwischen zwei Computern, ohne dass hierfür der Umweg über eine graphische Oberfläche nötig ist. Anstatt also beispielsweise über die Suchmaske der Staatlichen Museen zu Berlin nach Objekten oder Dokumenten mithilfe verschiedener Schlagwörter zu suchen und die Ergebnisse dann einzeln herunterzuladen, kann Ihr Computer mit der Schnittstelle des Museums direkt kommunizieren und mit einfachen Befehlen ganze Ergebnislisten zur Weiterarbeit herunterladen. Für solche Abfragen können ein Kommandozeilenprogramm oder Programmiersprachen genutzt werden, die Abfrage besteht dabei im Wesentlichen aus einer Zeile, wie hier in der Programmiersprache R:\nlibrary(jsonlite)\ncats <- fromJSON(\"https://smb.museum-digital.de/json/objects?&s=katze\")\n\nSie können R hier herunterladen. Wenn Sie das Programm öffnen, müssen Sie dort zuerst das Paket jsonlite installieren: install.packages(\"jsonlite\") Mit “Enter” wird das Paket installiert. Dann können Sie die zwei Zeilen oben eintippen und ebenfalls mit “Enter” ausführen. Die Ergebnisse Ihrer Suche können Sie sich mit cats + “Enter” anzeigen lassen\n\nDas Ergebnis der Suchanfrage nach “katze” wird in der Variable cats gespeichert, und diese kann zur Weiterarbeit in ein Tabellenformat exportiert werden:\nwrite.csv(cats, \"docs/cats_smb.csv\")\nDie Funktion write.csv speichert den Inhalt der Variable cats als csv-Datei3 unter dem Dateipfad “docs/cats_smb.csv” auf der Festplatte.\n\n\n\nBeginn der Trefferliste für “katze” ¨über die API der Staatlichen Museen zu Berlin\n\n\nWenn Webseiten keine Schnittstellen zur Verfügung stellen, besteht die Möglichkeit, mit Web Scraping an gewünschte Daten zu kommen. Je nach Webseite bzw. Inhalten ist die Rechtslage allerdings nicht ganz klar. Zum Download von Webseiten mit der Programmiersprache Python von William J. Turkel und Adam Crymble gibt es eine Lektion im Programming Historian. Ein weiteres Tutorial zur Datenakquise, von Zach Coble, Liz Rodrigues, Erin Pappas, Chelcie Rowell, und Yasmeen Shorish, findet sich hier."
  },
  {
    "objectID": "04_datenerhebung_analyse.html#datenaufbereitung4",
    "href": "04_datenerhebung_analyse.html#datenaufbereitung4",
    "title": "4  Datenerhebung und -aufbereitung",
    "section": "4.2 Datenaufbereitung4",
    "text": "4.2 Datenaufbereitung4\nBei der Arbeit mit Datensätzen, seien sie selbst erhoben oder von Dritten übernommen, ist es häufig der Fall, dass Informationen fehlen oder uneinheitlich erhoben wurden, was eine spätere Analyse erschwert.\nWenn in einer Umfrage unter Studierenden das Studienfach mit aufgenommen wurde, ohne zuvor Werte für diese Kategorie zu definieren, finden sich für “Geschichte” und “Deutsch” vielleicht auch folgende Varianten: “Gesch.”, “Geschichtswissenschaft”, “Geschichtswissenschaften”, “Geschihcte”, “Germanistik”, “Dt.”, “Germ.”. Anstatt zwei Werten für zwei Studienfächer gibt es neun – ohne, dass sich das Fächerspektrum erweitert hätte. Im besten Fall werden solche Varianten schon bei der Erhebung der Daten vermieden, indem eine feste Liste an Werten erstellt wird. Erhält man jedoch einen Datensatz mit verschiedenen Varianten für ein und denselben Wert, muss man diese zusammenführen, um eine saubere Datengrundlage zu erhalten. Sie können entweder mit Strg-R versuchen, verschiedene Schreibweisen zu finden und zu ersetzen; in Tabellenprogrammen wie Excel, Open Office oder Google Sheets können Sie sich einzigartige Werte einzelner Spalten anzeigen lassen und zusammengehörende Varianten zu einem Grundwert zusammenführen; am hilfreichsten und recht voraussetzungslos zu bedienen und dabei auch für große Datensätze nutzbar ist die Software OpenRefine, mit der Sie Daten extrahieren,5 säubern/vereinheitlichen6 und anreichern7 können, um eine für Ihre Forschungsfrage und dafür notwendige Analysen sinnvolle Datengrundlage zu erhalten.\nFür Textdaten sind verschiedene Schritte zur Aufbereitung notwendig, je nachdem, welche Methode bzw. Software Sie nutzen möchten. Für die meisten Analysen ist es sinnvoll, mit sogenannten Stopword-Listen zu arbeiten. Stopwords sind Wörter, die vor einer Analyse aus einem Korpus entfernt werden, um aussagekräftigere Ergebnisse zu erhalten, gerade, wenn es um rein quantitative Methoden zur inhaltlichen Erschließung geht. Stopwords sind Wörter mit grammatikalischen Funktionen, die in großer Zahl in Dokumenten vorkommen, jedoch wenig Bedeutung tragen. Wenn man den unbearbeiteten Text dieses Guides nach Worthäufigkeiten auswertet, hier mit Voyant-Tools lässt sich nur schwerlich erahnen, worum es geht – “digital” steht auf Platz 12, viel häufiger sind Artikel und Präpositionen. Mit Hilfe einer Stopword-Liste, die die häufigsten nicht-sinntragenden Wörter aus dem Text entfernt, wird der Inhalt klarer:\n\n\n\n\n\n\nWorthäufigkeiten roher Text\n\n\n\n\n \n\n\n\n\n\nWorthäufigkeiten ohne Stopwords\n\n\n\n\n\nWeitere Schritte beinhalten oft eine Tokenisierung, also die Segmentierung in Einheiten der Wortebene, und eine Lemmatisierung, also die Rückführung von verschiedenene Formen eines Worts auf eine Grundform – aus “ist”, “war” und “sind” wird “sein. Wie bei den Schreibvarianten der Studienfächer haben die verschiedenen Flexionsformen für die meisten Forschungsfragen keinen Mehrwert und können zur weiteren Analyse zusammengeführt werden. Für solche vorbereitenden Schritte gibt es existierende Software und Packages für Programmiersprachen, sodass hier das Rad nicht neu erfunden werden muss, vor allem für moderne, weit verbreitete Sprachen, siehe auch Kapitel B.3. Schwieriger wird es für nicht-standardisierte Sprachen bzw. Sprachformen, also dialektal geprägte oder vormoderne Texte. Zwar gibt es auch hier Programme, die tatsächlich erreichte Präzision muss dabei je nach Quelle beurteilt werden."
  },
  {
    "objectID": "04_datenerhebung_analyse.html#datenanalyse",
    "href": "04_datenerhebung_analyse.html#datenanalyse",
    "title": "4  Datenerhebung und -aufbereitung",
    "section": "4.3 Datenanalyse",
    "text": "4.3 Datenanalyse\nWenn Sie einen Datensatz zur Analyse zur Verfügung haben, aus selbst erhobenen Daten oder durch Nachnutzung eines vorhandenen, und für Ihre Zwecke aufbereitet haben, folgt (endlich) auch die Analyse. Welche Software oder Methoden Sie verwenden, hängt dabei nicht nur von der Art und Menge der Daten, sondern auch dem Datenformat und vor allem auch Ihrer Forschungsfrage ab. Wenn Sie eine Personendatenbank haben, in der Briefschreiber:innen und Empfänger:innen aufgenommen sind und der Wohnort der Personen bekannt ist, Sie es jedoch versäumt haben, die Daten der Einzelbriefe zu verzeichnen, können Sie nur eine räumliche Verteilung, keine raum-zeitliche Entwicklung eines Briefschreiber:innennetzwerks darstellen.8 Wenn Sie aber nur an der örtlichen Verteilung weiblicher und männlicher Verfasser:innen interessiert sind und die zeitliche Komponente für Sie keine Rolle spielt, erübrigt sich auch ein raum-zeitliche Analyse. Bevor Sie sich also für eine Methode entscheiden, sollten Sie sich fragen, zu welchem Zweck Sie Ihren Datensatz nutzen wollen und welche Frage(n) er beantworten soll.\nIn einem nächsten Schritt sollte über die konkrete Art der Analyse nachgedacht werden, die mit den vorhandenen Daten möglich ist. Unter den zahlreichen Möglichkeiten für die Arbeit mit strukturellen Daten sind für die Geschichtswissenschaften u.a. die Netzwerkanalyse oder die Regressionsanalyse häufig genutzte Methoden. Für textuelle Daten bieten sich ebenfalls verschiedene Arten der Analyse an, darunter beispielsweise Auszählungen von Worthäufigkeiten als Teil der Stylometrie/Zuschreibung von Autor:innenschaft (siehe Kapitel 2.2), Topic Modelling als statistische Methode zur Identifizierung wiederkehrender Themen in größeren Textbeständen, oder Sentiment Analyse, um Stimmungen, Gefühle, Bewertungen aus Textpassagen zu extrahieren. Wenn Sie über georeferenzierte Daten verfügen, können Sie verschiedene Analysen mithilfe von GIS (Geographic Information System) durchführen und visualisieren.\nOb Sie für Topic Modelling ein eigenes Skript schreiben oder vorhandene Software nutzen, ob Sie Regressionsanalysen selbst durchführen oder auf Webseiten durchführen lassen, ist dabei Ihre Entscheidung; oftmals ist das Nutzen vorhandener Webangebote für erste kurze Analysen sinnvoll, um zu überlegen, ob die vorgesehene Methode sinnvolle Ergebnisse zu liefern verspricht. Für größere Projekte, in denen komplexere Analysen über einen längeren Zeitraum durchgeführt werden sollen, bietet sich die Arbeit mit Programmiersprachen schon allein deswegen an, weil so ein sehr hohes Maß an Anpassungen von vorhandenen Funktionen für die eigenen Zwecke und die völlige Kontrolle über die eigenen Daten ermöglicht wird. Eine Auflistung häufig genutzter Tools für die historische Arbeit findet sich in Kapitel B.3."
  },
  {
    "objectID": "04_datenerhebung_analyse.html#datensicherung",
    "href": "04_datenerhebung_analyse.html#datensicherung",
    "title": "4  Datenerhebung und -aufbereitung",
    "section": "4.4 Datensicherung",
    "text": "4.4 Datensicherung\nIn Kapitel 5 wird es um Fragen zur nachhaltigen Speicherung von Forschungsdaten gehen; an dieser Stelle sei darauf hingewiesen, dass die Sicherung von Daten am besten auch mit einer Versionierung und mit einer Dokumentation einhergeht. Datenversionierung hat den Vorteil, dass Schritte wieder rückgängig gemacht werden können, Datensätze in unterschiedlichen Stadien gespeichert und für eine spätere Weiterarbeit genutzt werden können und einzelne Schritte einzelnen Projektmitarbeiter:innen zugeschrieben werden können. Zusätzliche Versionierung geht dabei über die Funktionalitäten von Backup-Programmen oder Cloudspeichern wie Dropbox oder Switchdrive hinaus, und für Einzelprojekte wie auch für kollaboratives Arbeiten hat sich git etabliert, häufig in Kombination mit Daten-/Coderepositorien auf GitHub. Die meisten von Ihnen werden vermutlich keine eigenen GitHub-Repositorien anlegen, aber das System dennoch irgendwann nutzen, am ehesten durch den Download von dort zur Verfügung gestellten Daten – die Textdaten für diesen Guide liegen auch in einem GitHub-Repositorium. Die Dokumentation von gespeicherten Daten schließlichh beinhaltet Informationen zur Entstehung des Datensatzes: Wie und von wem wurden die Daten erhoben? Wie wurden sie annotiert? In welchem Format sind die Daten vorhanden? Welche Software wurde an welcher Stelle benutzt? Was stellen die Daten dar? Die Sicherung von Daten an mehreren Orten, bspw. auf der lokalen Festplatte, in einem Cloudspeicher und einem USB-Stick, hilft sicher vor Datenverlust. Eine Dokumentation und die Sicherung in einem Repositorium, einem Langzeitspeicher für Daten, sorgt zusätzlich für Sichtbarkeit und die Möglichkeit zur Nachnutzung von Ergebnissen. Als Fachrepositorien für die Geisteswissenschaften existieren beispielsweise DARIAH-DE oder das DaSCH, es gibt spezialisiertere Repositorien wie AMAD (Mittelalter), oder für alle Disziplinen offene wie Zenodo (fächerübergreifend, betrieben durch das CERN) – Sie können Ihre Forschungsdaten dort kostenfrei ablegen, Ihre Urheberschaft nachweisen und die Daten/Publikation mit einem DOI (Digital Object Identifier) zitierbar machen."
  },
  {
    "objectID": "05_FAIR_CARE.html#faire-daten",
    "href": "05_FAIR_CARE.html#faire-daten",
    "title": "5  FAIR und CARE",
    "section": "5.1 FAIRe Daten",
    "text": "5.1 FAIRe Daten\nDie Prinzipien FAIRer Daten wurden 2016 von einem Konsortium aus Wissenschaftler:innen und Organisationen wie folgt definiert:1 Findability, Accessibility, Interoperability, Reuse of digital assets."
  },
  {
    "objectID": "Shell.html",
    "href": "Shell.html",
    "title": "6  shell",
    "section": "",
    "text": "Shell 101\nEs gibt zwei Arten, um mit dem Computer zu interagieren bzw. ihn zu nutzen: über ein Graphical User Interface (GUI) oder, etwas direkter, über die Kommandozeile1. Um eine Datei “Bild1.jpg” im Ordner “Bilder” zu löschen, öffnet man den Explorer (Windows) oder den Finder (Mac), klickt sich zum Ordner “Bilder”, macht einen Rechtsklick auf die zu löschende Datei “Bild1.jpg”, klickt “In den Papierkorb legen” oder zieht die Datei mit der Maus direkt dorthin. Man kann dieselbe Aktion als Kommando eintippen: Man öffnet eine Powershell (Windows) oder das Terminal (Mac), navigiert mit Texteingabe zum entsprechenden Ordner, bspw. cd Dokumente/Bilder und gibt dort das Kommando rm \"Datei.jpg\" ein, das mit der Entertaste ausgeführt wird.\n(base) serina00@dg-19-mac-02 Bilder % rm \"Datei.jpg\"\nDie beiden Vorgehensweisen unterscheiden sich dabei in drei Punkten:\n\nDas Kommando rm ist endgültig, die Datei ist ohne Übergangszeit im Papierkorb gelöscht.\nDas Kommando lässt sich relativ simpel auf eine Vielzahl von Dokumenten anwenden, wobei ganz unterschiedliche Bedingungen beachtet werden können, und es lässt sich mit anderen Kommandos verbinden.\nTerminal sieht k3wl aus.\n\nBevor wir den zweiten – und für unsere Arbeit hilfreichsten – Unterschied genauer anschauen, kurz zur Kommandozeile:\nIn einem Terminal/Shell können Befehle/Programme ausgeführt werden, die auf der Datenstrukturebene stattfinden – wie beispielsweise das Löschen einer Datei, rm Dateiname.xyz, oder das Erstellen eines Ordners, mkdir NeuerOrdner. Oder aber Operationen auf Dateninhaltsebene – wie beispielsweise das Suchen eines bestimmten Begriffs in einer Textdatei, grep Begriff Textdatei.txt, oder das Auszählen mehrerer Begriffe und das Speichern des Ergebnisses in einer neuen Datei, grep -Ec \"Begriff1|Begriff2\" Textdatei.txt > Ergebnisse.txt.\nWoher weiss Ihre Shell aber, was sie ausführen soll, wenn Sie rm oder grep eintippen? Es gibt zahlreiche Shell-Programme, die bereits auf Ihrem System vorinstalliert sind, und mit denen Sie vieles tun können – öffnen Sie Ihre Shell und tippen Sie date ein: Das aktuelle Datum mit Uhrzeit erscheint. Ihre Shell sucht nach dem ersten Argument, dem Befehl, im Filesystem des Computers, und wenn sie fündig wird, führt sie eine Aktion mit den entsprechenden Parametern aus.\n\ntmi: Wenn Sie echo $PATH im Terminal eingeben, sehen Sie eine Auflistung der Orte, an denen nach Befehlen gesucht wird. Tippen Sie which date ein und drücken Sie ‘Enter’, um zu sehen, wo das Programm “date” in Ihrem Computer liegt.\n\nWenn Sie einen Befehl eintippen, den es nicht gibt bzw. für den es noch kein installiertes Programm auf Ihrem Computer gibt, bekommen Sie eine simple Fehlermeldung:\n(base) serina00@dg-19-mac-02 EK-dh % nonsense\ncommand not found: nonsense\nDas aktuelle Datum wird Ihnen wahrscheinlich auch in Ihrer Toolbar angezeigt, und einen neuen Ordner können Sie per Rechtsklick erstellen, dazu brauchen Sie das Terminal nicht unbedingt. Um einen Begriff in einem Textdokument zu finden und alle Vorkommen zu zählen, können Sie das Dokument öffnen, Strg-F drücken, den Begriff eingeben und das Ergebnis sehen. Wenn Sie nach mehreren Begriffen suchen wollen, müssen Sie dieselbe Aktion zweimal ausführen: Strg-F, Begriff 2.\nWenn Sie wissen möchten, wie häufig Arthur Dent, Zaphod Beeblebrox, Slartibartfast und Mrs Enid Kapelsen in den sechs Bänden von “The Hitchhiker’s Guide to the Galaxy” genannt werden, müssen Sie, wenn Sie den Volltext heruntergeladen haben, für jeden Namen eine Suche ausführen, mit Strg-F. Bei der Suche nach Personen mit Vor- und Nachnamen wie “Arthur Dent” suchen Sie nach “Arthur”, nach “Dent” und nach “Arthur Dent” und ziehen alle “Arthur Dent”-Treffer von den übrigen Treffern ab, um nichts doppelt zu zählen und ihre Suchergebnisse zu verfälschen. Die Trefferzahl all Ihrer Suchanfragen schreiben Sie in ein neues Dokument.\nSie können dasselbe auch mit dem Terminal machen und einige der Built-in-Programme nutzen: Sie bewegen sich mit cd, change directory, in den Ordner (directory), in dem Ihre Textdatei liegt:\n(base) serina00@dg-19-mac-02 serina00 % cd Documents/progr/EK_dh/Hitchhiker\n(Um zu prüfen, was dort liegt, können Sie im Terminal ls (für list) eingeben, bzw. in der Powershell dir (für directory)\n(base) serina00@dg-19-mac-02 Hitchhiker % ls hitchhiker_fulltext\nMit einer Zeile können Sie die in einem Texteditor ausgeführten Suchvorgänge mit grep (Global Regular Expression Print) ausführen und die Ergebnisse mit > in eine neue Datei schreiben:\n\n\n\n\n\nKommandozeile/command line, bash, shell, prompt finden sich oft als synonym genutzte Begriffe für Command Line Interfaces. Auf UNIX-basierten Betriebssystemen wie Mac OS und Linux ist das Terminal als Interface weit verbreitet; für Details: https://en.wikipedia.org/wiki/Command-line_interface#History. Windowsnutzer:innen kommen mit der Powershell ganz gut zurecht, es empfiehlt sich eventuell die Installation von Cygwin oder MinGW, um mit einem UNIX-basierten Interface arbeiten zu können.↩︎"
  },
  {
    "objectID": "terms.html#glossary",
    "href": "terms.html#glossary",
    "title": "Appendix A — Appendix",
    "section": "Glossary",
    "text": "Glossary\n\n\n\nAPI\nApplication Programming Interface: a facility offered by a web resource which allows search queries independent of a GUI, often performed using scripts\n\n\nbash\ndefault program that runs in the command line\n\n\nbig data\nhuge amount of data, identifiable through repeated freezing of your standard program when opening a file\n\n\nborn digital data\ndata which originated in a digital form\n\n\nCLI\nCommand Line Interface, text interface that allows interaction with the computer; see also bash\n\n\nCMS\nContent Management System\n\n\nConsole\nSee CLI\n\n\nCrowdsourcing\nprojects that include the active participation of the public to generate content, transcribe sources etc.\n\n\ncsv\ncomma separated values, a structured text format, using commas as separators between columns\n\n\ndistant reading\nquantitative approach to huge amounts of texts, using computational methods to search for interpretable patterns\n\n\nGUI\nGraphical User Interface\n\n\nHTML\nHypertext Markup Language, a structured text format, like the format this guide is written in, to render documents in a browser\n\n\nJupyter notebook\nweb application/interactive coding environment that runs in a browser; let’s you create and share code (https://jupyter.org)\n\n\nmachine readable\ntransformation of, for example, text into a data format that is processable by a computer\n\n\nOCR\nOptical Character Recognition, process of transforming text on an image into a data format\n\n\nOS\nOperating System\n\n\nOSS\nOpen Source Software\n\n\nRegular Expression\nsyntax for search and replace text using patterns (instead of exact matches)\n\n\nterminal\nSee CLI"
  },
  {
    "objectID": "further_ressources.html#sec-digitalhistory-paper",
    "href": "further_ressources.html#sec-digitalhistory-paper",
    "title": "Appendix B — Further Ressources",
    "section": "B.1 Was ist digital history?",
    "text": "B.1 Was ist digital history?\n\nBrennan, Sheila A.: Digital History, in: The Inclusive Historian’s Handbook, https://inclusivehistorian.com/digital-history/, 04.06.2019.\nCohen, Daniel J.; Rosenzweig, Roy: Digital History. A Guide to Gathering, Preserving, and Presenting the Past on the Web, Philadelphia 2006. Online: https://chnm.gmu.edu/digitalhistory/.\nHohls, Rüdiger: Digital Humanities und digitale Geschichtswissenschaften, in: Busse, Laura u. a. (Hg.): Clio-Guide. Ein Handbuch zu digitalen Ressourcen für die Geschichtswissenschaften, Berlin 2018, S. A.1-1–B.1-34. Online: https://doi.org/10.18452/19244.\nWinters, Jane: Digital history, in: Tamm, Marek; Burke, Peter (Hg.): Debating New Approaches to History, London 2019, S. 277–300.\nDigital history, in: Wikipedia, 07.09.2022. Online: https://en.wikipedia.org/w/index.php?title=Digital_history&oldid=1109027465, Stand: 02.11.2022."
  },
  {
    "objectID": "further_ressources.html#digital-literacy-digital-criticism",
    "href": "further_ressources.html#digital-literacy-digital-criticism",
    "title": "Appendix B — Further Ressources",
    "section": "B.2 Digital Literacy, Digital Criticism",
    "text": "B.2 Digital Literacy, Digital Criticism\n\nEkström, Andreas: The Moral Bias behind your Search Results, TED talk 7.12.2015 (9:18), Online: https://www.youtube.com/watch?v=_vBggxCNNno.\nGibbs, Frederick W.: New Forms of History: Critiquing Data and Its Representations, in: The American Historian, February 2016. Online: http://tah.oah.org/february-2016/new-forms-of-history-critiquing-data-and-its-representations/.\nTavani, Herman; Zimmer, Michael Zimmer: Search Engines and Ethics, in: Edward N. Zalta (Hg.): The Stanford Encyclopedia of Philosophy (Fall 2020 Edition), Online: https://plato.stanford.edu/archives/fall2020/entries/ethics-search/, Kap. 3.1."
  },
  {
    "objectID": "further_ressources.html#sec-digitaltools",
    "href": "further_ressources.html#sec-digitaltools",
    "title": "Appendix B — Further Ressources",
    "section": "B.3 Tools für digital history (n.b.: free/open source)",
    "text": "B.3 Tools für digital history (n.b.: free/open source)\n\nB.3.1 Allgemein\n\nProgramming Historian: Tutorials zu verschiedenen Tools und Methoden für historische Forschung und Lehre\n\n\n\nB.3.2 Text-/Korpusanalyse\n\nAntConc: Korpusanalyse-Toolkit\nLemmatisierung: Sammlung der FID Romanistik\nNatural Language Toolkit, Package für Python zur Tokenisierung, Lemmatisierung usw.: NLTK\nTokenisierung: Tutorial von fortext zu NLTK\nVoyant-Tools: Sammlung von Tools zur Textanalyse, browserbasiert oder standalone\n\n\n\nB.3.3 Visualisierung\n\nFID Romanistik: Sammlung von Tools zur Datenvisualisierung"
  },
  {
    "objectID": "further_ressources.html#terminalcommand-line",
    "href": "further_ressources.html#terminalcommand-line",
    "title": "Appendix B — Further Ressources",
    "section": "B.4 Terminal/Command Line",
    "text": "B.4 Terminal/Command Line\n\nDawson, Ted: Introduction to the Windows Command Line with PowerShell, Programming Historian 5 (2016), https://doi.org/10.46430/phen0054. (self-learning lesson)\nMIT Computer Science Department: 1-hour-lecture on the Shell (video)\nMilligan, Ian; Baker, James: Introduction to the Bash Command Line, Programming Historian 3 (2014), https://doi.org/10.46430/phen0037. (self-learning lesson)\ndatacamp course:Introduction to Shell (interactive self-learning lesson)\nJeroen Janssens: Data Science at the command line (book)"
  },
  {
    "objectID": "further_ressources.html#regular-expressions",
    "href": "further_ressources.html#regular-expressions",
    "title": "Appendix B — Further Ressources",
    "section": "B.5 Regular Expressions",
    "text": "B.5 Regular Expressions\n\nKnox, Doug: Understanding Regular Expressions, Programming Historian 2 (2013), https://doi.org/10.46430/phen0033. (self-learning lesson)\nRegexOne: Learn Regular Expressions with simple, interactive exercises. (interactive self-learning tutorial)"
  }
]