[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Digital History",
    "section": "",
    "text": "Welcome\nDer vorliegende Guide, erstellt im Herbstsemester 2022, begleitet die Einführungskurse im Fach Geschichte an der Universität Basel und soll einen ersten Einblick in den Bereich Digital History geben. Es ist ein living document, das regelmäßig aktualisiert wird und dabei auch die epochen- und areaspezifischen Inhalte der verschiedenen Einführungskurse berücksichtigt, indem Verweise auf verschiedene Digital-History-Projekte aus unterschiedlichen Bereichen mit der Zeit in den Guide einfließen. Für die Teilnehmer:innen der Einführungskurse wird der Guide von einer Präsenzsitzung begleitet, bietet aber hoffentlich auch unabhängig davon einen Mehrwert. Für Kommentare, Anregungen oder Beschwerden freue ich mich über eine Nachricht.\nDer Guide ist in zwei Teile gegliedert: Die Kapitel 1–5 sollen eine erste Übersicht über Digital History bieten und den Blick auf Neuerungen und Veränderungen richten, die sich in den Geschichtswissenschaften aus der Nutzung digitaler Methoden ergeben. Der anschließende praktische Teil zeigt an einem konkreten Beispiel die Anwendung verschiedener Techniken auf, die sich (nicht nur) für Historiker:innen bei der Arbeit mit Quellenmaterial anbieten. Der Praxisteil verfolgt dabei zwei Ziele: Zum einen sollen Hemmungen bei der Arbeit mit dem Computer, die über die Nutzung als elektronische Schreibmaschine hinausgeht, abgebaut werden. Zum anderen soll ein grundlegendes Verständnis dafür hergestellt werden, welche Möglichkeiten computergestützte Analysen bieten und wie diese in der historischen Arbeit eingesetzt werden können.\nDer Praxisteil wird in Kalenderwoche 47 online geschaltet.\nDie Übersicht soll möglichst knapp gehalten werden – es gibt zahlreiche ausführliche Grundlagenwerke, weswegen viele Themen nur kurz angeschnitten, dafür aber mit weiterführenden Verweisen versehen werden. Dasselbe gilt für den Praxisteil: Weiterreichende Anleitungen, Tutorials oder Onlinekurse werden an entsprechender Stelle verlinkt. Vollständigkeit wird an keiner Stelle beansprucht; Hinweise auf weitere Online-Angebote nehme ich gerne auf."
  },
  {
    "objectID": "01_intro.html",
    "href": "01_intro.html",
    "title": "1  Was ist Digital History?",
    "section": "",
    "text": "Über die Antwort zur Frage, was Digital History ist oder umfasst, kann man ausgiebig diskutieren. Als Teilgebiet der Digital Humanities, der digitalen Geisteswissenschaften, kann die folgende aktuelle und pragmatische Definition von Blaney et al. (2021) hilfreich sein:\n\nDigital humanities, in our view, is a question of approach: if you are actively and critically using digital tools to aid your work in researching, teaching or learning, you are probably doing digital humanities. We would encourage anyone to learn to program if they are interested in doing so, but we do not see it as a defining characteristic of work in digital humanities.1\n\nDabei umfassen “digital tools” eine große Bandbreite – und es wird sich kaum eine:r finden, der:die Studium, Forschung oder Lehre völlig ohne die Nutzung digitaler Techniken betreibt. Wir sind alle Historiker:innen im digitalen Zeitalter, und als solche müssen wir ohnehin neue Kompetenzen entwickeln. Wir können uns aber zudem dafür entscheiden, für ein Forschungsprojekt Methoden und Techniken einzusetzen, die über die traditionellen Werkzeuge der Geschichtswissenschaften hinausgehen – Analyse und Interpretation von Quellen durch deren genaue Lektüre, sogenanntes close reading –, und uns durch den Computer unterstützen lassen. Ob wir hierbei auf vorhandene Software zurückgreifen oder selbst Programme schreiben, um uns nicht nur als Historiker:innen im digitalen Zeitalter, sondern auch als digitale Historiker:innen zu verstehen, mögen manche als Glaubensfrage auffassen; eine inkludierende Haltung zu dieser Frage scheint mir dabei nur Vorteile zu haben.2\nFür eine erste Idee dafür, wie man historische Fragestellungen mithilfe digitaler Methoden beantworten kann und wie unterschiedlich digital unterstützte Forschungsprojekte aussehen können, bietet sich unter anderem der Übersichtsartikel “State of the Field: Digital History” von Romein et al. (2020) an.3 Eine anwachsende Liste an Beispielprojekten aus unterschiedlichen Epochen bzw. Themenbereichen findet sich unter Projekte und Ressourcen in Kapitel 2.\nUm eine Annäherung an die aktive, kritische und reflektierte Nutzung digitaler Methoden in Forschung und Lehre mit einem Fokus auf deren Anwendung in den Geschichtswissenschaften geht es im vorliegenden Guide. Weiterführende Texte zur Frage, was Digital History ist bzw. umfasst, finden sich unter Literatur, Tools, Tutorials\n\n\n\n\n\nBlaney, Jonathan; Winters, Jane; Milligan, Sarah u. a.: Doing digital history: a beginner’s guide to working with text as data, Manchester 2021 (IHR research guides), S. 6.↩︎\nEntgegen einer häufig zitierten Aussage von Emmanuel Le Roy Ladurie (*1929), der Historiker von morgen werde Programmierer sein, oder er werde nicht sein: “L’historien de demain sera programmeur ou il ne sera pas.” Le Roy Ladurie, Emmanuel: La fin des érudits, in: Le Nouvel Observateur, 08.1968.↩︎\nRomein, C. Annemieke; Kemman, Max; Birkholz, Julie M. u. a.: State of the Field: Digital History, in: History 105 (365), 04.2020, S. 291–312. Online: <https://doi.org/10.1111/1468-229X.12969>, Stand: 15.09.2022.↩︎"
  },
  {
    "objectID": "02_forschung_lehre.html#digitalisierte-quellen-digitale-quellen",
    "href": "02_forschung_lehre.html#digitalisierte-quellen-digitale-quellen",
    "title": "2  Forschung und Lehre",
    "section": "2.1 Digitalisierte Quellen, digitale Quellen",
    "text": "2.1 Digitalisierte Quellen, digitale Quellen\nAls Historiker:innen steht die Arbeit mit Quellen im Mittelpunkt unserer Analysen. Das bedeutet gleichzeitig, dass der Zugang bzw. die Verfügbarkeit von Dokumenten einen Einfluss darauf hat, welche Fragen wir beantworten oder welche Analysen wir vornehmen können. Zugangsbeschränkungen, die die Größe und Zusammensetzung unseres Untersuchungskorpus beeinflussen, können dabei von Gedächtnisinstitutionen – also Museen, Archiven, Bibliotheken – ausgehen, beispielsweise wenn bei zeitgenössischen Akten eine Schutzfrist festgesetzt wird oder wenn ein Objekt zu fragil für die Benutzung ist. Auch kann es aus finanziellen und/oder organisatorischen Gründen schwierig sein, bestimmte Archive an weiter entfernten Orten aufzusuchen, um weitere Dokumente für die Untersuchung zu berücksichtigen. Groß angelegte Digitalisierungsprojekte in Bibliotheken und Archiven bergen damit die Möglichkeit, zusätzliche Quellen nicht nur über einen Eintrag im Bibliothekskatalog zu finden, sondern die entsprechenden Dokumente in digitaler Form auf den eigenen Rechner zu laden. Gerade auch für wertvolle historische Bestände – antike Papyri, Handschriften aus dem Frühmittelalter, einzelüberlieferte Frühdrucke usw. – entsteht hier die Möglichkeit, diese einem größeren Kreis verfügbar zu machen, ohne das Objekt zu großer Belastung durch häufige Benutzung auszusetzen, und ohne dass die Benutzer:innen lange Reisen auf sich nehmen müssten. Für mittelalterliche und frühneuzeitliche Handschriften und Drucke beispielsweise existieren mittlerweile mehrere (meist nationale) Portale, die eine zentrale Suche über alle Bestände ermöglichen; eine Auswahl findet sich unter Projekte und Ressourcen.\nNeben der Digitalisierung vorhandener Quellen (Retrodigitalisierung) steht die unaufhörliche Entstehung neuer Quellen in rein digitaler Form (born digital data). Der relativen Knappheit von Quellen – und damit Daten –, die Vormodernehistoriker:innen oftmals zu beklagen haben, steht eine Überfülle an zeitgenössischem Material gegenüber, und beide Situationen – zu wenig/zu unvollständige und zu viele/zu unübersichtliche Datenmengen – bergen methodische Probleme: Wie stellt man ein Korpus, also eine Sammlung von Quellen zusammen, das ausreichend Dokumente beinhaltet, um Fragestellungen zu beantworten, Thesen zu stützen, neue Erkenntnisse zu erhalten, das aber gleichzeitig in einem Forscher:innenleben bewältigbar bleibt? Historiker:innen müssen neue Kompetenzen erwerben, um mit solchen Fragen reflektiert umzugehen. Zur klassischen Quellenkritik kommt die digitale Quellenkritik, zur Fähigkeit, analoge Quellen zu lesen und zu verstehen, ein Äquivalent für den digitalen Bereich. Etwas ausführlicher geht es in Kapitel 3 um Digital Literacy und Digital Criticism."
  },
  {
    "objectID": "02_forschung_lehre.html#sec-digitaletools",
    "href": "02_forschung_lehre.html#sec-digitaletools",
    "title": "2  Forschung und Lehre",
    "section": "2.2 Digitale Tools zur Analyse",
    "text": "2.2 Digitale Tools zur Analyse\nDie hier bereits zitierte Definition, die aktive und kritische Nutzung digitaler Werkzeuge in Forschung, Lehre oder Studium sei es, was Digital Humanities ausmachten, wirft die Frage auf, was genau unter digitalen Werkzeugen, unter digital tools zu verstehen ist, und zu welchem Zweck man sie einsetzt. Allein schon das Lesen dieses Guides ist ohne digitale Hilfsmittel nicht möglich – es existiert kein gedrucktes Exemplar davon. Lesen am Bildschirm allein macht noch keinen digital humanist, aber man muss nicht erst eine Programmiersprache lernen, um den Computer für die eigene Arbeit zu nutzen und zu Ergebnissen zu kommen, die mit klassischen Methoden – im Bereich der Geschichtswissenschaften etwa papierbasiertes close reading von Quellen und Forschungsliteratur – nicht im selben Ausmaß erzielt werden könnten.\nUntersuchungen, die digitale Methoden einsetzen, sind im Normalfall skalierbar – wenn man eine Software benutzt, die die Häufigkeit von Begriffen in einem Dokument zählt, sollte es keinen Unterschied in der Anwendung machen, ob man eines oder einhundert Dokumente auswerten will. Würde man dasselbe per Hand tun, wäre man analog zum Anwachsen der Dokumente mit der Auszählung beschäftigt. Digitale Werkzeuge ermöglichen es also unter anderem, Untersuchungen auf größere Mengen von Dokumenten auszuweiten. Sie ermöglichen es auch, an ein so erweitertes Korpus andere Fragen zu stellen, als dies mit einer kleineren Quellen-/Datengrundlage möglich wäre. Die vorherrschende Überlieferung historischer Quellen besteht aus Text, handgeschrieben, gemeißelt oder gedruckt – und durch die Möglichkeit, diesen mittels Texterkennung in computerlesbare Daten umzuwandeln, ergeben sich neue Perspektiven für die Arbeit von Historiker:innen: Wenn Texte als Daten verstanden werden, lassen sich aus Textquellen Datenbestände erstellen, die mithilfe quantitativer Methoden untersucht und ausgewertet werden können.1\nFür die Literaturwissenschaften beispielsweise ist ein wichtiges Anwendungsfeld die Überprüfung von Autor:innenschaft: Ob ein anonym überliefertes Werk einem:r namentlich bekannten Autor:in zugeschrieben werden kann, lässt sich entweder durch close reading von Literaturwissenschaftler:innen überprüfen, oder durch die Suche nach patterns, Mustern, nach quantifizierbare Eigenschaften eines Textes, wie beispielsweise die Häufigkeit von Funktionswörtern, Partikeln, Satzzeichen usw. Der unter dem Pseudonym Robert Galbraith veröffentlichte Kriminalroman The Cuckoo’s Calling konnte mit entsprechender Software Joanne K. Rowling zugeschrieben werden – damit dauerte die Untersuchung dreißig Minuten, was etwa dem Lesen von zwanzig Romanseiten entspricht. Zu einem Artikel, der diesen Fall thematisiert und in das Feld der linguistischen Forensik einbettet, die Straftäter:innen mithilfe quantitativer Textanalyse ermittelt, geht es hier. Ein Video zur Entwicklung und Anwendung von Software zur Zuschreibung von Autor:innenschaft finden Sie hier. Die genutzte Software, JGGAP,2 lässt sich offensichtlich auch für historische Analysen nutzen – man denke nur an Herrschaftssysteme, in denen strenge Zensur geübt wird/wurde und viele Autor:innen daher nicht unter ihrem Klarnamen publizier(t)en. Durch eine Identifikation anonymer Schreiber:innen lassen sich weitere Aspekte rund um die Thematik Zensur untersuchen – welche Akteur:innen waren öffentlich bekannt, wer publizierte gleichzeitig anonym und unter Klarnamen, welche Autor:innen schrieben aus dem Exil, welche Netzwerke lassen sich rekonstruieren usw. Dadurch, dass ein Programm durch quantitative Auswertungen die Kärrnerarbeit der Identifikation abnehmen kann – um einen reflektierten Umgang mit Daten und Algorithmen geht es in Kapitel 3 –, bleibt mehr Zeit für die qualitative Arbeit; gleichzeitig fußt die Analyse auf einem aussagekräftigen Datensatz, anstatt nur Einzelbeispiele beleuchten zu können.\nQuantitative und qualitative Methoden sollen hier keinesfalls gegeneinander ausgespielt werden; vielmehr soll verdeutlicht werden, dass beide Herangehensweisen Vor- und Nachteile haben, und dass sie im besten Fall gewinnbringend miteinander kombiniert werden können – quantitative Auswertungen nur um ihrer selbst willen und ohne eine spezifische historische Fragestellung generieren kaum je einen Mehrwert.\nJe nach Datengrundlage, Analysezweck und Forschungsfrage bieten sich unterschiedliche Tools zur Nutzung an; für die meisten Forschungsvorhaben bis zum Ende des Studiums dürfte existierende Software ausreichen, sei es für die Akquise und Aufbereitung von Daten(-sätzen), für verschiedene Arten von Textanalysen, statistische Auswertungen, Netzwerkanalysen, Geomapping oder Visualisierungen. Eine Auswahl an Tools – alle kostenfrei/open source – für spezifische Analysen findet sich unter Literatur, Tools, Tutorials. Für gewisse Analysen bietet es sich an, Programmierkenntnisse zu erwerben – das Erstellen eigener Skripts, also kleiner Programme, beinhaltet die umfassende Kontrolle darüber, wie Daten eingelesen, aufbereitet, angereichert, analysiert und visualisiert werden; bei wiederkehrenden Prozessen, die händisch einige Arbeitszeit in Anspruch nehmen würden, lässt sich so zusätzlich Zeit sparen.\nFür geisteswissenschafliche Projekte werden zurzeit vor allem zwei Programmiersprachen genutzt, R und Python. Da sich beide großer Beliebtheit in den Humanities erfreuen, existieren mittlerweile zahlreiche Packages, die Data und Text Mining, also groß angelegte Daten- und Textanalysen, sehr einfach machen. Solche Packages für Programmiersprachen kann man sich wie Plug-Ins für Programme vorstellen, beispielsweise ein AdBlocker für den Browser. So etwas war von den Entwickler:innen ursprünglich nicht vorgesehen, aber jemand hatte Bedarf, Werbeanzeigen zu blockieren, hat hierzu ein Programm geschrieben und es der Allgemeinheit zur Verfügung gestellt. Der Unterschied zu einem Package ist, dass dieses verschiedene Funktionen zur Verfügung stellt – auswählen und ausführen müssen die Anwender:innen. Wer in Schule und Studium keine Berührungspunkte mit Programmieren hatte, wird zu Beginn vielleicht größere Berührungsängste haben – aber noch einmal: Sie müssen nicht programmieren können, um quantitativ zu arbeiten. Speziell an Historiker:innen ohne Programmier-Vorkenntnisse richtet sich das Projekt “The Programming Historian”, das seit 2008 zahlreiche Tutorials veröffentlicht, um verschiedene Tools, Techniken und Workflows für die geschichtswissenschaftliche Forschung und Lehre vorzustellen."
  },
  {
    "objectID": "02_forschung_lehre.html#digitale-tools-zur-kommunikation",
    "href": "02_forschung_lehre.html#digitale-tools-zur-kommunikation",
    "title": "2  Forschung und Lehre",
    "section": "2.3 Digitale Tools zur Kommunikation",
    "text": "2.3 Digitale Tools zur Kommunikation\ntbd"
  },
  {
    "objectID": "02_forschung_lehre.html#digitale-tools-in-der-hochschullehre",
    "href": "02_forschung_lehre.html#digitale-tools-in-der-hochschullehre",
    "title": "2  Forschung und Lehre",
    "section": "2.4 Digitale Tools in der Hochschullehre",
    "text": "2.4 Digitale Tools in der Hochschullehre\ntbd"
  },
  {
    "objectID": "02_forschung_lehre.html#sec-projects",
    "href": "02_forschung_lehre.html#sec-projects",
    "title": "2  Forschung und Lehre",
    "section": "2.5 Projekte und Ressourcen",
    "text": "2.5 Projekte und Ressourcen\n\n2.5.1 Alte Geschichte\nProjekte:\nRessourcen/Portale:\n\n\n2.5.2 Mittelalter und Frühe Neuzeit\nProjekte:\n\nRepertorium Academicum: Projekt zur Erfassung europäischer Gelehrter zwischen 1250 und 1550\n\nRessourcen/Portale:\n\ndMGH: Monumenta Germaniae Historica online (Beta-Version)\ne-codices: Virtuelle Handschriftenbibliothek der Schweiz\nFragmentarium: Laboratory for Medieval Manuscript Fragments\nHandschriftenportal: Zentraler nationaler Nachweis für Buchhandschriften in deutschen Bibliotheken und in deutscher Sprache (Entwicklungsstadium)\ne-manuscripta: Digitalisierte handschriftliche Quellen aus Schweizer Bibliotheken und Archiven\ne-rara: Plattform für digitalisierte Drucke aus Schweizer Institutionen\nGallica: Digitalisierte Quellen aus französischen Biblioteken\nswisscollections: Suchplattform für historische Schweizer Bestände\ntranscriptiones: Plattform zum Erstellen, Teilen und Nutzen von Transkriptionen historischer Manuskripte\n\n\n\n2.5.3 Moderne und Zeitgeschichte\nProjekte:\n\nRefugee History: Wissenschaftliches Blog und interaktives Netzwerk zu aktuellen Debatten um das Thema “Flüchtlinge”\n\nRessourcen/Portale:\n\nDatenbank Bild + Ton zur Geschichte (Schweizer) sozialer Bewegungen\nDodis: Wissenschaftliche Edition von Dokumenten zur Schweizer Außenpolitik\ne-newspaperarchives.ch: Schweizer Zeitungen online\ne-periodica: Schweizer Zeitschriften online\nHistorische Statistik der Schweiz (HSSO)\nhistat: Zeitreihen zur Historischen Statistik\n\n\n\n2.5.4 Jüdische Geschichte\nProjekte:\n\nDigital Jewish Studies Online, Stroum Center for Jewish Studies, University of Washington\n\nRessourcen/Portale:\n\nBlavatnik Archive: Archiv zur Erhaltung und Verfügbarmachung von Material zur (jüdischen) Geschichte des 20. Jahrhunderts mit Fokus auf die zwei Weltkriege und Sowietrussland.\nMenny, Anna; Rürup, Miriam; Siegel, Björn: Jüdische Geschichte im deutschsprachigen Raum, in: Busse, Laura u. a. (Hg.): Clio-Guide. Ein Handbuch zu digitalen Ressourcen für die Geschichtswissenschaften, Berlin 2018, S. E.2-1–E.2-56. Online: https://doi.org/10.18452/19244.\n\n\n\n2.5.5 Geschichte Afrikas\nProjekte:\n\nEmandulo: Digitales Archiv, das archivalische/museale Sammlungen und Präsentationen über präkoloniale südafrikanische Geschichte zusammenführt und neu zusammenstellt.\nLegacies of British Slavery: Forschungsprojekt zum britischen Sklavenhandel und -besitz\n\nRessourcen/Portale:\n\nFHYA: Experimentelle digitale Forschungsplattform über präkoloniale südafrikanische Geschichte\nLegacies of British Slavery: Datenbank zum britischen Sklavenhandel und -besitz\nSlave Voyages: Datenbanken zum transatlantischen und interamerikanischen Sklavenhandel und Personendatenbank\n\n\n\n2.5.6 Osteuropäische Geschichte\nProjekte:\n\nGulag: Many Days, Many Lives: Archiv und Präsentationsplattform zu den sowjetischen Gulags\nGulag Online: Virtuelles Museum mit Präsentationen und Quellen zum Leben im Gulag, zu Personen und Objekten\nSeventeen Moments in Soviet History: Multimediales Online-Archiv mit ausgewählten Quellen zu Ereignissen in der sowjetischen Geschichte anhand 17 verschiedener Jahre zwischen 1917 und 1991\n\nRessourcen/Portale:\n\nBlavatnik Archive: Archiv zur Erhaltung und Verfügbarmachung von Material zur (jüdischen) Geschichte des 20. Jahrhunderts mit Fokus auf die zwei Weltkriege und Sowietrussland.\nThe Other Side: Webarchiv von Interviews ehemaliger Ostarbeiter:innenr, Kriegsgefangener und Insassen deutscher Lager; Publikationsplattform\n\n\n\n2.5.7 Epochen-/Areaübergreifend:\nProjekte:\n\nLord of the Rings Project: Interaktive Analyse der Werke J. R. R. Tolkiens\n\nRessourcen/Portale:\n\nAround DH in 80 days: Portal zur Vorstellung achtzig verschiedener Digital-Humanities-Projekte weltweit und aus verschiedenen Disziplinen\nInternet Archive: digitale Bibliothek zur Archivierung von Büchern, Bildern, Filmen, Software, Musik und Webseiten"
  },
  {
    "objectID": "03_digital_literacy_criticism.html#digital-literacy-data-literacy",
    "href": "03_digital_literacy_criticism.html#digital-literacy-data-literacy",
    "title": "3  Digital Literacy, Digital Criticism",
    "section": "3.1 Digital Literacy, Data Literacy",
    "text": "3.1 Digital Literacy, Data Literacy\nUnter Data Literacy wird die Kompetenz verstanden, Daten zu sammeln, zu managen, zu evaluieren und zu nutzen,1 eine Kompetenz, die jede:r für den mittlerweile unvermeidlichen Umgang mit Daten verschiedenster Art im eigenen Alltag entwickeln sollte. Je nach Forschungsdisziplin ergeben sich weiter gewisse Spezifika, wobei Studierenden der Geisteswissenschaften ein Thema wie Algorithmenkritik nicht als erstes in den Sinn kommt, wenn es um die im Studium zu erwerbenden Kompetenzen geht.2 Aber auch ohne den Quellcode von machine-learning-Software im Detail zu verstehen, ermöglicht ein grundlegendes Verständnis von und ein Wissen über die Funktionsweisen solcher Anwendungen einen reflektierten Umgang mit diesen. Eine solche Art von Digital bzw. Data Literacy ist vor allem dann relevant, wenn es um die Interpretation von Ergebnissen geht, die scheinbar objektiv sind, bzw. scheinbar objektiv entstanden. Ein gutes Beispiel hierfür sind die Ergebnislisten bei Suchanfragen in einer Suchmaschine. Je nachdem, welchen Anbieter Sie nutzen, spielen verschiedene Umstände in die Generierung von Trefferlisten hinein, beispielsweise Ihre Suchhistorie, sodass search neutrality nicht mehr gewährleistet ist.3\nGehen Sie auf die Bilder-Suche von Google und suchen Sie nach “historian”. Was sehen Sie?\nWüssten ich nichts über Geschichtswissenschaftler:innen, würde ich aufgrund der Ergebnisse meiner Suche davon ausgehen, “a historian” wäre meist ein alter, weißer Mann mit Brille, Bart und einem großen Bücherregal; wenn Sie sich am Departement Geschichte der Uni Basel umsehen, dürfte ein etwas anderer Eindruck entstehen. Die Ergebnisse von Suchmaschinen, die für ihr Funktionieren Algorithmen anwenden, sind biased, verzerrt: Sie beruhen auf vorangegangenen Suchen, Vorlieben, geographischem Standort – und auf von Menschen eingegebenen Metadaten, also Daten mit Informationen über andere Daten. Ein Bewusstsein hierfür und das Hinterfragen von Datensätzen gehören also mit zur Arbeit in einer digitalisierten Welt."
  },
  {
    "objectID": "03_digital_literacy_criticism.html#digital-criticism-data-criticism",
    "href": "03_digital_literacy_criticism.html#digital-criticism-data-criticism",
    "title": "3  Digital Literacy, Digital Criticism",
    "section": "3.2 Digital Criticism, Data Criticism",
    "text": "3.2 Digital Criticism, Data Criticism\nDigitalisierte Quellen ebenso wie rein digitale erfordern eine erweiterte Art von Quellenkritik – im Einführungskurs an der Universität Basel lernen Sie die Grundlagen klassischer Quellenkritik:\nWoher kommt eine Quelle, wer hat sie unter welchen Umständen und zu welchem Zweck erstellt? Welche Absichten können darin verborgen sein, und welche Verzerrungen können sich durch sie ergeben?\nWelche Tendenzen könnten sich in hochmittelalterlichen Herrscherchroniken verstecken, wenn der Verfasser in direkter Abhängigkeit des Auftraggebers stand? Wie sind Zeug:innenaussagen in Hexenprozessen zu bewerten, die unter Anwendung körperlicher Strafe entstanden sind? Mit wie viel Vorsicht sind die Inhalte eines Tagebuchs zu bewerten, das allem Anschein nach mit Blick auf eine spätere Veröffentlichung verfasst wurde?\nNeben der inneren Kritik geht es bei der Arbeit mit Quellen immer auch um Fragen der Korpusbildung: Wie kann eine Quellengrundlage erstellt werden, die für Beantwortung einer spezifischen historischen Fragestellung belastbar und aussagekräftig genug ist und gleichzeitig in angemessener Zeit bearbeitet werden kann? Hinzu kommen Spezifika bei der Arbeit mit unterschiedlichen Quellenformen bzw. -formaten: Bei analogen Quellen, die auch in digitaler Form zur Verfügung stehen, besteht die Gefahr, dass ein Thema, ein Bereich, ein Aspekt vernachlässigt wird, wenn nur die unmittelbar verfügbaren, digitalisierten Bestände zur Korpusbidlung genutzt werden. Wenn Sie sich beispielsweise für die Schweizer Historikerin und Frauenrechtlerin Meta von Salis (1855–1929) und deren briefliche Korrespondenz – Friedrich Nietzsche war einer ihrer Brieffreunde – interessieren und über die Suchplattform für historische Schweizer Bestände, swisscollections, in nationalen Bibliotheken und Archiven nach entsprechenden Dokumenten suchen, erhalten Sie 361 Treffer:\n\n\n\n\n\nErweiterte Suchmaske von swisscollections\n\n\n\n \n\n\n\n\n\nSuchergebnisse für “Meta von Salis” + “Brief”\n\n\n\n\n\nDigital verfügbar waren hiervon im Oktober 2022 lediglich drei Einträge, wobei der erste ein Brief von Nietzsche an Meta von Salis ist, der zweite Eintrag umfasst sieben Briefe von Caroline Farner, und der dritte Eintrag ist weder an noch von Meta von Salis, sondern hat sie nur zum Thema:\n\n\n\nSuchergebnisse für “Meta von Salis” + “Brief” + “Digitalisat verfügbar”\n\n\nIhnen würde bei einer Korpuserstellung vom Schreibtisch aus, also nur mit den angezeigten Digitalisaten, also der Großteil der Überlieferung fehlen, und Ihre Untersuchungsergebnisse wären wohl sehr verzerrt, würden Sie statistische Aussagen treffen wollen: Meta von Salis unterhielt brieflichen Kontakt zu einem Mann und einer Frau, das Geschlechterverhältnis wäre also ausgeglichen; und Frauen schreiben im Schnitt mehr Briefe an Meta von Salis als Männer. Beim Blick auf alle Suchergebnisse würden sich Ihre Aussagen aber sehr ändern, und es würde sich lohnen, diese Verzerrung, diesen Bias aus Ihrer Datengrundlage zu entfernen.\n\nHinzu kommt natürlich immer das grundlegende Problem bei der Suche nach Quellen: swisscollections und ähnliche Portale können nur anzeigen, was die Kooperationspartner:innen zur Verfügung stellen. Hat eine Bibliothek Briefe von Meta von Salis in ihrem Bestand, diese aber noch nicht als Datensatz erfasst, wissen Sie im Gegensatz zum obigen Beispiel nicht einmal, dass Ihnen etwas entgehen würde, dass in Ihrem Korpus überhaupt ein Bias vorhanden ist.\n\nÄhnliche Vorsicht zur Vermeidung von Verzerrungen in der Datengrundlage gilt bei der Arbeit mit rein digitalen Daten, beispielweise bei der Auswertung von Datensätzen aus Befragungen. Wenn Sie sich am 27.10.2022 vor die Universitätsbibliothek in Basel stellen und einen Tag lang mithilfe eines kurzen Fragebogens und einer Tabellendatei erfassen, wie zufrieden die befragten Personen mit dem Essen in der Unimensa sind, werden Sie am Ende einen Datensatz erhalten, in dem sich vermutlich über 80% der Befragten für besseres und nahezu 100% für günstigeres Essen in der Mensa aussprechen – eine gute Schlagzeile für die BZ, die sich auf die neuesten Ergebnisse einer wissenschaftlichen Studie berufen kann. Führen Sie die gleiche Umfrage eine Woche später, mitten während der Herbstmesse durch, werden die Ergebnisse wohl erheblich anders aussehen. Die Wahrscheinlichkeit, dass die Mensa infolge der BZ-Schlagzeile innerhalb weniger Tage den Menüplan überarbeitet und die Preise herabgesetzt hat, ist dabei wohl geringer als diejenige, dass sich Ihr Sample, die Auswahl an Datenpunkten, also befragten Personen, durch die Messe stark verändert hat: Im Umkreis der Bibliothek treffen Sie nun nicht mehr vor allem Studierende und andere Uni-Angehörige an, sondern auch Messebesucher:innen vom Petersplatz. Auch hier sind Verzerrungen entstanden, ähnlich wie beim vorherigen Beispiel mit den Briefen: Wenn aus einer Gesamtheit nur eine spezifische Untermenge beobachtet wird, die sich durch ein gemeinsames Merkmal von der Gesamtheit unterscheidet – digitalisierte Quelle oder Besucher:in der Universitätsbibliothek –, ist die Datengrundlage und damit die Untersuchungsergebnisse biased. Um bei Daten, die Sie nachnutzen, eventuell vorhandene Verzerrungen nicht weiterzutransportieren, ist das Üben von Datenkritik eine essentielle Kompetenz.\nZur Tatsache, dass Daten eben nicht “gegeben” sind (lat. dare, datum: geben, gegeben), sondern gemacht, und daher entsprechend interpretiert werden müssen, finden Sie ein gutes Interview von Roopika Risam (2020);4 zur Zementierung von Klischees durch Übersetzungsalgorithmen gibt es einen Artikel in der Republik von Marie-José Kolly und Simon Schmid (2021);5 und über die Macht von Data Science und dem Änderungspotential von Data Feminism haben Catherine D’Ignazio und Lauren F. Klein 2020 ein ganzes Buch veröffentlich.6\nZur Frage, wie sich die digitale Wende, der digital turn, auf die Quellenkritik auswirkt, sehen Sie sich dieses kurze Video des Projekts Ranke.2 – Quellenkritik im digitalen Zeitalter an:7\n\n\nEine Handreichung zum Umgang mit digitalisierten und digitalen Daten, das im selben Projekt erarbeitet wurde, finden Sie hier."
  },
  {
    "objectID": "04_datenerhebung_analyse.html#datenerhebung",
    "href": "04_datenerhebung_analyse.html#datenerhebung",
    "title": "4  Datenerhebung, -aufbereitung und -analyse",
    "section": "4.1 Datenerhebung",
    "text": "4.1 Datenerhebung\nEs gibt verschiedene Möglichkeiten, Daten für die historische Forschung zu erheben bzw. zu erstellen, von denen einige im Folgenden kurz angesprochen werden.\nFür Zeiträume, in denen Quellen vergleichsweise knapp sind und keine seriellen Daten existieren, bietet sich die Digitalisierung von Texten und deren anschließende Analyse an. Digitalisierung beinhaltet dabei nicht nur die Transformation von einer physischen Quelle in ein digitales Bild, sondern auch die Anreicherung des Bilds mit Layout und Text: Erst durch eine Markierung von Bereichen, in denen Text vorkommt, ist es in einem zweiten Schritt möglich, diesen als solchen zu erkennen und damit maschinenlesbar und auswertbar zu machen. Eine solche Umwandlung vom Bild zum Text ist dabei sowohl für moderne Texte, die als Typoskript vorliegen, als auch für vormoderne Handschriften und Drucke möglich, in lateinischer ebenso wie in arabischer, chinesischer oder japanischer Schrift. Es gibt kostenpflichtige Programme wie den Abbyy FineReader, aber auch Open-Source-Tools mit und ohne Graphical User Interface (GUI). Weit verbreitet ist Transkribus, das viele Funktionalitäten bündelt; die Texterkennung ist ab einer gewissen Menge Seiten allerdings kostenpflichtig, wobei studentische Projekte auf Anfrage unterstützt werden können. Programme, die über die Kommandozeile laufen, gänzlich kostenfrei sind und ebenfalls zahlreiche Funktionalitäten bieten, sind beispielsweise Kraken, OCR4all, OCRopus oder Calamari.\nZur Extraktion von Daten aus digitalen/digitalisierten Texten existieren verschiedene Möglichkeiten mithilfe kleiner Kommandozeilenprogramme (eher mühsam und schwierig zu lesen) oder mit Packages für Programmiersprachen, für die Geisteswissenschaften vor allem R oder Python (siehe dazu auch Kapitel 2.2). So können besipielsweise aus digitalisierten Telefonbüchern Entitäten, also Einheiten, wie Personen, Straßennamen oder Berufe oder aus alten Theaterprogrammheften gespielte Stücke, beteiligte Schauspieler:innen und verantwortliche Regisseurinnen extrahiert und als Datensätze weitergenutzt werden.1\nDer anfängliche Aufwand, der einer automatisierten Datenextraktion vorangeht und die steile Lernkurve bei der Bedienung mancher Programme können abschreckend wirken. Wenn Sie nur ein Theaterprogramm detaillierter auswerten wollen, sind Sie sicher schneller, wenn Sie die entsprechenden Daten in eine Tabellensoftware abtippen. Wenn Sie aber einen größeren Quellenbestand zur Verfügung haben, der in sich ähnlich strukturiert ist, wie das bei Telefonbüchern oder einer Serie von Theaterprogrammheften der Fall sein dürfte, macht es kaum einen Unterschied mehr, ob Sie zehn oder hundert Theaterprogramme analysieren möchten. Zudem können Sie Ihr erstelltes Skript, Ihr kleines Computerprogramm, anderen zur Verfügung stellen oder für ähnlich strukturierte Quellen in einem anderen Projekt nachnutzen.\nWenn Sie mit bereits digitalisierten Beständen aus öffentlichen Institutionen wie Galerien, Bibliotheken, Museen oder Archiven arbeiten wollen (sog. GLAMs: Galleries, Libraries, Archives, Museums), besteht oft die Möglichkeit, Daten über Schnittstellen herunterzuladen.2 Solche Schnittstellen, engl. API (Application Programming Interface), ermöglichen eine Kommunikation zwischen zwei Computern, ohne dass hierfür der Umweg über eine graphische Oberfläche nötig ist. Anstatt also beispielsweise über die Suchmaske der Staatlichen Museen zu Berlin nach Objekten oder Dokumenten mithilfe verschiedener Schlagwörter zu suchen und die Ergebnisse dann einzeln herunterzuladen, kann Ihr Computer mit der Schnittstelle des Museums direkt kommunizieren und mit einfachen Befehlen ganze Ergebnislisten zur Weiterarbeit herunterladen. Für solche Abfragen können ein Kommandozeilenprogramm oder Programmiersprachen genutzt werden, die Abfrage besteht dabei im Wesentlichen aus einer Zeile, wie hier in der Programmiersprache R:\nlibrary(jsonlite)\ncats <- fromJSON(\"https://smb.museum-digital.de/json/objects?&s=katze\")\n\nWenn Sie die Schritte nachvollziehen möchten, können Sie R hier herunterladen. Wenn Sie das Programm öffnen, müssen Sie zuerst das Paket jsonlite installieren: install.packages(\"jsonlite\")\nMit “Enter” wird das Paket installiert.  Dann können Sie die zwei Zeilen oben eintippen und ebenfalls mit “Enter” ausführen. Die Ergebnisse Ihrer Suche können Sie sich mit\ncats + “Enter”\nanzeigen lassen.\n\nDas Ergebnis der Suchanfrage nach “katze” wird in der Variable cats gespeichert, und diese kann zur Weiterarbeit in ein Tabellenformat exportiert werden:\nwrite.csv(cats, \"docs/cats_smb.csv\")\nDie Funktion write.csv speichert den Inhalt der Variable cats als csv-Datei3 unter dem Dateipfad “docs/cats_smb.csv” auf der Festplatte.\n\n\n\nBeginn der Trefferliste für “katze” über die API der Staatlichen Museen zu Berlin\n\n\n\n\n\nWenn Webseiten keine Schnittstellen zur Verfügung stellen, besteht die Möglichkeit, mit Web Scraping an gewünschte Daten zu kommen. Je nach Webseite bzw. Inhalten ist die Rechtslage allerdings nicht ganz klar. Zum Download von Webseiten mit der Programmiersprache Python gibt es eine Lektion im Programming Historian von William J. Turkel und Adam Crymble. Ein weiteres Tutorial zur Datenakquise, von Zach Coble, Liz Rodrigues, Erin Pappas, Chelcie Rowell, und Yasmeen Shorish, findet sich hier."
  },
  {
    "objectID": "04_datenerhebung_analyse.html#datenaufbereitung4",
    "href": "04_datenerhebung_analyse.html#datenaufbereitung4",
    "title": "4  Datenerhebung, -aufbereitung und -analyse",
    "section": "4.2 Datenaufbereitung4",
    "text": "4.2 Datenaufbereitung4\nBei der Arbeit mit Datensätzen, seien sie selbst erhoben oder von Dritten übernommen, ist es häufig der Fall, dass Informationen fehlen oder uneinheitlich erhoben wurden, was eine spätere Analyse erschwert.\nWenn in einer Umfrage unter Studierenden das Studienfach mit in eine Tabelle aufgenommen wurde, ohne zuvor Werte für diese Kategorie zu definieren, finden sich für “Geschichte” und “Deutsch” vielleicht auch folgende Varianten: “Gesch.”, “Geschichtswissenschaft”, “Geschichtswissenschaften”, “Geschihcte”, “Germanistik”, “Dt.”, “Germ.”. Anstatt zwei Werten für zwei Studienfächer gibt es neun – ohne, dass sich das Fächerspektrum erweitert hätte. Im besten Fall werden solche Varianten schon bei der Erhebung der Daten vermieden, indem eine feste Liste an Werten erstellt wird. Erhält man jedoch einen Datensatz mit verschiedenen Varianten für ein und denselben Wert, muss man diese zusammenführen, um eine saubere Datengrundlage zu erhalten. Sie können entweder mit Strg-R versuchen, verschiedene Schreibweisen zu finden und zu ersetzen; in Tabellenprogrammen wie Excel, Open Office oder Google Sheets können Sie sich einzigartige Werte einzelner Spalten anzeigen lassen und zusammengehörende Varianten zu einem Grundwert zusammenführen; am hilfreichsten, recht voraussetzungslos zu bedienen und dabei auch für große Datensätze nutzbar ist die Software OpenRefine, mit der Sie Daten extrahieren,5 säubern/vereinheitlichen6 und anreichern7 können, um eine für Ihre Forschungsfrage und dafür notwendige Analysen sinnvolle Datengrundlage zu erhalten.\nFür Textdaten sind verschiedene Schritte zur Aufbereitung notwendig, je nachdem, welche Methode bzw. Software Sie nutzen möchten. Für die meisten Analysen ist es sinnvoll, mit sogenannten Stopword-Listen zu arbeiten. Stopwords sind Wörter, die vor einer Analyse aus einem Korpus entfernt werden, um aussagekräftigere Ergebnisse zu erhalten, gerade, wenn es um rein quantitative Methoden zur inhaltlichen Erschließung geht. Stopwords sind Wörter mit grammatikalischen Funktionen, die in großer Zahl in Dokumenten vorkommen, jedoch wenig Bedeutung tragen. Wenn man den unbearbeiteten Text dieses Guides nach Worthäufigkeiten auswertet, hier mit Voyant-Tools lässt sich nur schwerlich erahnen, worum es geht – “digital” steht auf Platz 12, viel häufiger sind Artikel und Präpositionen. Mit Hilfe einer Stopword-Liste, die die häufigsten nicht-sinntragenden Wörter aus dem Text entfernt, wird der Inhalt klarer:\n\n\n\n\n\n\nWorthäufigkeiten roher Text\n\n\n\n\n \n\n\n\n\n\nWorthäufigkeiten ohne Stopwords\n\n\n\n\n\nWeitere Schritte beinhalten oft eine Tokenisierung, also die Segmentierung in Einheiten der Wortebene, und eine Lemmatisierung, also die Rückführung von verschiedenene Formen eines Worts auf eine Grundform – aus “ist”, “war” und “sind” wird “sein”. Wie bei den Schreibvarianten der Studienfächer haben die verschiedenen Flexionsformen für die meisten Forschungsfragen keinen Mehrwert und können zur weiteren Analyse zusammengeführt werden. Für solche vorbereitenden Schritte gibt es existierende Software und Packages für Programmiersprachen, sodass hier das Rad nicht neu erfunden werden muss, vor allem für moderne, weit verbreitete Sprachen, siehe auch Kapitel B.3. Schwieriger wird es für nicht-standardisierte Sprachen bzw. Sprachformen, also dialektal geprägte oder vormoderne Texte. Zwar gibt es auch hierfür Programme, die tatsächlich erreichte Präzision muss dabei jedoch je nach Quelle beurteilt werden."
  },
  {
    "objectID": "04_datenerhebung_analyse.html#datenanalyse",
    "href": "04_datenerhebung_analyse.html#datenanalyse",
    "title": "4  Datenerhebung, -aufbereitung und -analyse",
    "section": "4.3 Datenanalyse",
    "text": "4.3 Datenanalyse\nWenn Sie einen Datensatz zur Analyse zur Verfügung haben, aus selbst erhobenen Daten oder durch Nachnutzung eines vorhandenen, und für Ihre Zwecke aufbereitet haben, folgt (endlich) auch die Analyse. Welche Software oder Methoden Sie verwenden, hängt dabei nicht nur von der Art und Menge der Daten, sondern auch dem Datenformat und vor allem auch Ihrer Forschungsfrage ab. Wenn Sie eine Personendatenbank haben, in der Briefschreiber:innen und Empfänger:innen aufgenommen sind und der Wohnort der Personen bekannt ist, Sie es jedoch versäumt haben, die Datierungen der Einzelbriefe zu verzeichnen, können Sie nur eine räumliche Verteilung, keine raum-zeitliche Entwicklung eines Briefschreiber:innennetzwerks darstellen.8 Wenn Sie aber nur an der örtlichen Verteilung weiblicher und männlicher Verfasser:innen interessiert sind und die zeitliche Komponente für Sie keine Rolle spielt, erübrigt sich auch ein raum-zeitliche Analyse. Bevor Sie sich also für eine Methode entscheiden, sollten Sie sich fragen, zu welchem Zweck Sie Ihren Datensatz nutzen wollen und welche Frage(n) er beantworten soll.\nIn einem nächsten Schritt sollte über die konkrete Art der Analyse nachgedacht werden, die mit den vorhandenen Daten möglich ist. Unter den zahlreichen Möglichkeiten für die Arbeit mit strukturellen Daten sind für die Geschichtswissenschaften u.a. die Netzwerkanalyse oder die Regressionsanalyse häufig genutzte Methoden. Für textuelle Daten bieten sich ebenfalls verschiedene Arten der Analyse an, darunter beispielsweise Auszählungen von Worthäufigkeiten als Teil der Stylometrie/Zuschreibung von Autor:innenschaft (siehe Kapitel 2.2), Topic Modelling als statistische Methode zur Identifizierung wiederkehrender Themen in größeren Textbeständen, oder Sentimentanalyse, um Stimmungen, Gefühle, Bewertungen aus Textpassagen zu extrahieren. Wenn Sie über georeferenzierte Daten verfügen, können Sie verschiedene Analysen mithilfe von GIS (Geographic Information System) durchführen und visualisieren.\nOb Sie für Topic Modelling ein eigenes Skript schreiben oder vorhandene Software nutzen, ob Sie Regressionsanalysen selbst durchführen oder auf Webseiten durchführen lassen, ist dabei Ihre Entscheidung; oftmals ist das Nutzen vorhandener Webangebote für erste kurze Analysen sinnvoll, um zu überlegen, ob die vorgesehene Methode überhaupt sinnvolle Ergebnisse liefern kann. Für größere Projekte, in denen komplexere Analysen über einen längeren Zeitraum durchgeführt werden sollen, bietet sich die Arbeit mit Programmiersprachen schon allein deswegen an, weil so ein sehr hohes Maß an Anpassungen von vorhandenen Funktionen für die eigenen Zwecke und die völlige Kontrolle über die eigenen Daten ermöglicht wird. Eine Auflistung häufig genutzter Tools für die historische Arbeit findet sich in Kapitel B.3."
  },
  {
    "objectID": "04_datenerhebung_analyse.html#datensicherung",
    "href": "04_datenerhebung_analyse.html#datensicherung",
    "title": "4  Datenerhebung, -aufbereitung und -analyse",
    "section": "4.4 Datensicherung",
    "text": "4.4 Datensicherung\nIn Kapitel 5 wird es um Fragen zur nachhaltigen Speicherung von Forschungsdaten gehen; an dieser Stelle sei darauf hingewiesen, dass die Sicherung von Daten am besten auch mit einer Versionierung und mit einer Dokumentation einhergeht. Datenversionierung hat den Vorteil, dass Schritte wieder rückgängig gemacht werden können, Datensätze in unterschiedlichen Stadien gespeichert und für eine spätere Weiterarbeit genutzt werden können und einzelne Schritte einzelnen Projektmitarbeiter:innen zugeschrieben werden können. Zusätzliche Versionierung geht dabei über die Funktionalitäten von Backup-Programmen oder Cloudspeichern wie Dropbox oder Switchdrive hinaus, und für Einzelprojekte wie auch für kollaboratives Arbeiten hat sich in der Wissenschaft wie in der Wirtschaft git etabliert, häufig in Kombination mit Daten-/Coderepositorien auf GitHub. Die meisten von Ihnen werden vermutlich keine eigenen GitHub-Repositorien anlegen, aber das System dennoch irgendwann nutzen, am ehesten durch den Download von dort zur Verfügung gestellten Daten – die Textdaten für diesen Guide liegen auch in einem GitHub-Repositorium. Die Dokumentation von gespeicherten Daten schließlich beinhaltet Informationen zur Entstehung des Datensatzes: Wie und von wem wurden die Daten erhoben? Wie wurden sie annotiert? In welchem Format sind die Daten vorhanden? Welche Software wurde an welcher Stelle benutzt? Was stellen die Daten dar? Die Sicherung von Daten an mehreren Orten, bspw. auf der lokalen Festplatte, in einem Cloudspeicher und auf einem USB-Stick, schützt sicher vor Datenverlust. Eine Dokumentation und die Sicherung in einem Repositorium, einem Langzeitspeicher für Daten, sorgt zusätzlich für Sichtbarkeit und die Möglichkeit zur Nachnutzung von Ergebnissen. Als Fachrepositorien für die Geisteswissenschaften existieren beispielsweise DARIAH-DE oder das DaSCH, es gibt spezialisiertere Repositorien wie AMAD (Mittelalter), oder für alle Disziplinen offene wie Zenodo (fächerübergreifend, betrieben durch das CERN). Sie können Ihre Forschungsdaten dort kostenfrei ablegen, Ihre Urheberschaft nachweisen und die Daten/Publikation mit einem Digital Object Identifier (DOI), also einem eindeutigen und dauerhaften digitalen Identifikator, nachhaltig zitierbar machen."
  },
  {
    "objectID": "05_FAIR_CARE.html#faire-daten",
    "href": "05_FAIR_CARE.html#faire-daten",
    "title": "5  FAIR und CARE",
    "section": "5.1 FAIRe Daten",
    "text": "5.1 FAIRe Daten\nDie Prinzipien FAIRer Daten wurden 2016 von einem Konsortium aus Wissenschaftler:innen und Organisationen wie folgt definiert:1 Findability, Accessibility, Interoperability, Reuse of digital assets.\nDaten sollen also auffindbar und zugänglich sein, zudem interoperabel, also mit verschiedenen Systemen nutzbar, und wiederverwendbar. Wenn Sie für eine Proseminararbeit zehn Testamente aus dem 18. Jahrhundert im Staatsarchiv Basel fotografieren, anschließend transkribieren, die vererbten Gegenstände identifizieren, zwischen den Erblasser:innen vergleichen und Ihre Ergebnisse ausgedruckt bei dem:r Dozierenden einreichen, sind Ihre Daten das genaue Gegenteil: Niemand weiß, dass Sie die Daten erhoben haben; und wenn Ihr:e Dozent:in Ihre Ergebnisse anderen Studierenden zur Verfügung stellen will, um weitere Forschung anzuregen, geht dies nur in Form von Kopien Ihrer gedruckten Arbeit. Wenn Sie Ihre transkribierten Texte und die identifzierten Objekte in Standardformaten und mit offener Lizenz auf einem Repositorium veröffentlichen, machen Sie nicht nur wichtige Teile Ihrer eigenen Arbeit sichtbar, sondern erleichtern so auch anschließende Forschungen. Zudem kann so vermieden werden, dass geleistete Arbeit wie beispielsweise Transkriptionen nicht doppelt gemacht wird."
  },
  {
    "objectID": "terms.html",
    "href": "terms.html",
    "title": "Appendix A — Glossar",
    "section": "",
    "text": "API\nApplication Programming Interface: a facility offered by a web resource which allows search queries independent of a GUI, often performed using scripts\n\n\nbash\ndefault program that runs in the command line\n\n\nbias\nsystematic error that results from an unbalanced sample\n\n\nbig data\nhuge amount of data, identifiable through repeated freezing of your standard program when opening a file\n\n\nborn digital data\ndata which originated in a digital form\n\n\nCLI\nCommand Line Interface, text interface that allows interaction with the computer; see also bash\n\n\nclose reading\ncareful and attentive interpretation of a text\n\n\n\n                                              |\nCMS | Content Management System |\nConsole | See CLI |\nCrowdsourcing | projects that include the active participation of the public to generate content, transcribe sources etc. |\ncsv | comma separated values, a structured text format, using commas as separators between columns |\ndistant reading | quantitative approach to huge amounts of texts, using computational methods to search for interpretable patterns |\nGUI | Graphical User Interface |\nHTML | Hypertext Markup Language, a structured text format, like the format this guide is written in, to render documents in a browser |\nJupyter notebook | web application/interactive coding environment that runs in a browser; let’s you create and share code (https://jupyter.org) |\nmachine learning | umbrella term for different methods that use data to do a task in a specific way, using data to learn and to improve the results\nmachine readable | transformation of, for example, text into a data format that is processable by a computer |\nOCR | Optical Character Recognition, process of transforming text on an image into a data format |\nOS | Operating System |\nopen source | freely available source code that can be used, modified and redistributed without limitations\nOSS | Open Source Software |\nRegular Expression | syntax for search and replace text using patterns (instead of exact matches) |\nterminal | See CLI |\nweb scraping | extracting data from websites"
  },
  {
    "objectID": "further_ressources.html#sec-digitalhistory-paper",
    "href": "further_ressources.html#sec-digitalhistory-paper",
    "title": "Appendix B — Literatur, Tools, Tutorials",
    "section": "B.1 Was ist digital history?",
    "text": "B.1 Was ist digital history?\n\nBrennan, Sheila A.: Digital History, in: The Inclusive Historian’s Handbook, https://inclusivehistorian.com/digital-history/, 04.06.2019.\nHohls, Rüdiger: Digital Humanities und digitale Geschichtswissenschaften, in: Busse, Laura u. a. (Hg.): Clio-Guide. Ein Handbuch zu digitalen Ressourcen für die Geschichtswissenschaften, Berlin 2018, S. A.1-1–B.1-34. Online: https://doi.org/10.18452/19244.\nRomein, C. Annemieke u. a.: State of the Field: Digital History, in: History 105 (365), 04.2020, S. 291–-312. Online: https://doi.org/10.1111/1468-229X.12969.\nWinters, Jane: Digital History, in: Tamm, Marek; Burke, Peter (Hg.): Debating New Approaches to History, London 2019, S. 277–300.\nArt. “Digital history”, in: Wikipedia, 07.09.2022. Online: https://en.wikipedia.org/w/index.php?title=Digital_history&oldid=1109027465, Stand: 02.11.2022."
  },
  {
    "objectID": "further_ressources.html#einführungen-und-guides",
    "href": "further_ressources.html#einführungen-und-guides",
    "title": "Appendix B — Literatur, Tools, Tutorials",
    "section": "B.2 Einführungen und Guides",
    "text": "B.2 Einführungen und Guides\n\nBattershill, Claire; Ross, Shawna: Using Digital Humanities in the Classroom. A Practical Introduction for Teachers, Lecturers, and Students, London u.a. 2022.\nBlaney, Jonathan u. a.: Doing Digital History. A Beginner’s Guide to Working with Text as Data, Manchester 2021.\nCohen, Daniel J.; Rosenzweig, Roy: Digital History. A Guide to Gathering, Preserving, and Presenting the Past on the Web, Philadelphia 2006. Online: https://chnm.gmu.edu/digitalhistory/.\nDöring, Karoline u. a. (Hg.): Digital History. Konzepte, Methoden und Kritiken Digitaler Geschichtswissenschaft, Boston 2022, Online: https://doi.org/10.1515/9783110757101.\nDougherty, Jack; Nawrotzki, Kristen (Hg.): Writing History in the Digital Age, 2013. Online: https://doi.org/10.3998/dh.12230987.0001.001.\nGraham, Shawn u. a.: Exploring Big Historical Data. The Historian’s Macroscope, 2022. Online: https://doi.org/10.1142/12435.\nLemercier, Claire; Zalc, Claire: Quantitative Methods in the Humanities. An Introduction, Charlottesville 2019."
  },
  {
    "objectID": "further_ressources.html#sec-digitaltools",
    "href": "further_ressources.html#sec-digitaltools",
    "title": "Appendix B — Literatur, Tools, Tutorials",
    "section": "B.3 Tools für digital history (free/open source)",
    "text": "B.3 Tools für digital history (free/open source)\n\nB.3.1 Allgemein\n\nProgramming Historian: Tutorials zu verschiedenen Tools und Methoden für historische Forschung und Lehre\n\n\n\nB.3.2 Text-/Korpusanalyse\n\nAntConc: Korpusanalyse-Toolkit\nLemmatisierung: Sammlung der FID Romanistik\nNatural Language Toolkit, Package für Python zur Tokenisierung, Lemmatisierung usw.: NLTK\nTokenisierung: Tutorial von fortext zu NLTK\nVoyant-Tools: Sammlung von Tools zur Textanalyse, browserbasiert oder standalone\n\n\n\nB.3.3 Visualisierung\n\nBostock, Michael; Heer, Jeffrey; Ogievetsky, Vadim: A Tour through the Visualization Zoo. A Survey of Powerful Visualization Techniques, from the Obvious to the Obscure, in: Queue 8, Nr. 5 (2010). Online: https://queue.acm.org/detail.cfm?id=1805128\nData Visualisation Catalogue: Guide zur Auswahl von Visualisierungsformen\nFID Romanistik: Sammlung von Tools zur Datenvisualisierung\nRAWGraphs: Tool zur Datenvisualisierung von tabularen Daten (.tsv-, .csv-, .dsv- oder .json-Dateien)"
  },
  {
    "objectID": "further_ressources.html#digital-literacy-digital-criticism",
    "href": "further_ressources.html#digital-literacy-digital-criticism",
    "title": "Appendix B — Literatur, Tools, Tutorials",
    "section": "B.4 Digital Literacy, Digital Criticism",
    "text": "B.4 Digital Literacy, Digital Criticism\n\nEkström, Andreas: The Moral Bias behind your Search Results, TED talk 7.12.2015 (9:18), Online: https://www.youtube.com/watch?v=_vBggxCNNno.\nGibbs, Frederick W.: New Forms of History: Critiquing Data and Its Representations, in: The American Historian, February 2016. Online: http://tah.oah.org/february-2016/new-forms-of-history-critiquing-data-and-its-representations/.\nTavani, Herman; Zimmer, Michael Zimmer: Search Engines and Ethics, in: Edward N. Zalta (Hg.): The Stanford Encyclopedia of Philosophy (Fall 2020 Edition), Online: https://plato.stanford.edu/archives/fall2020/entries/ethics-search/, Kap. 3.1."
  },
  {
    "objectID": "further_ressources.html#terminalcommand-lineshell",
    "href": "further_ressources.html#terminalcommand-lineshell",
    "title": "Appendix B — Literatur, Tools, Tutorials",
    "section": "B.5 Terminal/Command Line/Shell",
    "text": "B.5 Terminal/Command Line/Shell\n\nDawson, Ted: Introduction to the Windows Command Line with PowerShell, Programming Historian 5 (2016), https://doi.org/10.46430/phen0054. (self-learning lesson)\nMIT Computer Science Department: 1-hour-lecture on the Shell (video)\nMilligan, Ian; Baker, James: Introduction to the Bash Command Line, Programming Historian 3 (2014), https://doi.org/10.46430/phen0037. (self-learning lesson)\ndatacamp course:Introduction to Shell (interactive self-learning lesson)\nJeroen Janssens: Data Science at the command line (book)"
  },
  {
    "objectID": "further_ressources.html#regular-expressions",
    "href": "further_ressources.html#regular-expressions",
    "title": "Appendix B — Literatur, Tools, Tutorials",
    "section": "B.6 Regular Expressions",
    "text": "B.6 Regular Expressions\n\nKnox, Doug: Understanding Regular Expressions, Programming Historian 2 (2013), https://doi.org/10.46430/phen0033. (self-learning lesson)\nRegexOne: Learn Regular Expressions with simple, interactive exercises. (interactive self-learning tutorial)"
  }
]